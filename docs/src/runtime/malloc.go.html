<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Source: malloc.go in package runtime</title>
<link href="../../css/light-v0.3.2.css" rel="stylesheet">
<script src="../../jvs/golds-v0.3.2.js"></script>
<body onload="onPageLoad()"><div>

<pre id="header"><code><span class="title">Source File</span>
	malloc.go

<span class="title">Belonging Package</span>
	<a href="../../pkg/runtime.html">runtime</a>
</code></pre>

<pre class="line-numbers">
<span class="codeline" id="line-1"><code>// Copyright 2014 The Go Authors. All rights reserved.</code></span>
<span class="codeline" id="line-2"><code>// Use of this source code is governed by a BSD-style</code></span>
<span class="codeline" id="line-3"><code>// license that can be found in the LICENSE file.</code></span>
<span class="codeline" id="line-4"><code></code></span>
<span class="codeline" id="line-5"><code>// Memory allocator.</code></span>
<span class="codeline" id="line-6"><code>//</code></span>
<span class="codeline" id="line-7"><code>// This was originally based on tcmalloc, but has diverged quite a bit.</code></span>
<span class="codeline" id="line-8"><code>// http://goog-perftools.sourceforge.net/doc/tcmalloc.html</code></span>
<span class="codeline" id="line-9"><code></code></span>
<span class="codeline" id="line-10"><code>// The main allocator works in runs of pages.</code></span>
<span class="codeline" id="line-11"><code>// Small allocation sizes (up to and including 32 kB) are</code></span>
<span class="codeline" id="line-12"><code>// rounded to one of about 70 size classes, each of which</code></span>
<span class="codeline" id="line-13"><code>// has its own free set of objects of exactly that size.</code></span>
<span class="codeline" id="line-14"><code>// Any free page of memory can be split into a set of objects</code></span>
<span class="codeline" id="line-15"><code>// of one size class, which are then managed using a free bitmap.</code></span>
<span class="codeline" id="line-16"><code>//</code></span>
<span class="codeline" id="line-17"><code>// The allocator's data structures are:</code></span>
<span class="codeline" id="line-18"><code>//</code></span>
<span class="codeline" id="line-19"><code>//	fixalloc: a free-list allocator for fixed-size off-heap objects,</code></span>
<span class="codeline" id="line-20"><code>//		used to manage storage used by the allocator.</code></span>
<span class="codeline" id="line-21"><code>//	mheap: the malloc heap, managed at page (8192-byte) granularity.</code></span>
<span class="codeline" id="line-22"><code>//	mspan: a run of in-use pages managed by the mheap.</code></span>
<span class="codeline" id="line-23"><code>//	mcentral: collects all spans of a given size class.</code></span>
<span class="codeline" id="line-24"><code>//	mcache: a per-P cache of mspans with free space.</code></span>
<span class="codeline" id="line-25"><code>//	mstats: allocation statistics.</code></span>
<span class="codeline" id="line-26"><code>//</code></span>
<span class="codeline" id="line-27"><code>// Allocating a small object proceeds up a hierarchy of caches:</code></span>
<span class="codeline" id="line-28"><code>//</code></span>
<span class="codeline" id="line-29"><code>//	1. Round the size up to one of the small size classes</code></span>
<span class="codeline" id="line-30"><code>//	   and look in the corresponding mspan in this P's mcache.</code></span>
<span class="codeline" id="line-31"><code>//	   Scan the mspan's free bitmap to find a free slot.</code></span>
<span class="codeline" id="line-32"><code>//	   If there is a free slot, allocate it.</code></span>
<span class="codeline" id="line-33"><code>//	   This can all be done without acquiring a lock.</code></span>
<span class="codeline" id="line-34"><code>//</code></span>
<span class="codeline" id="line-35"><code>//	2. If the mspan has no free slots, obtain a new mspan</code></span>
<span class="codeline" id="line-36"><code>//	   from the mcentral's list of mspans of the required size</code></span>
<span class="codeline" id="line-37"><code>//	   class that have free space.</code></span>
<span class="codeline" id="line-38"><code>//	   Obtaining a whole span amortizes the cost of locking</code></span>
<span class="codeline" id="line-39"><code>//	   the mcentral.</code></span>
<span class="codeline" id="line-40"><code>//</code></span>
<span class="codeline" id="line-41"><code>//	3. If the mcentral's mspan list is empty, obtain a run</code></span>
<span class="codeline" id="line-42"><code>//	   of pages from the mheap to use for the mspan.</code></span>
<span class="codeline" id="line-43"><code>//</code></span>
<span class="codeline" id="line-44"><code>//	4. If the mheap is empty or has no page runs large enough,</code></span>
<span class="codeline" id="line-45"><code>//	   allocate a new group of pages (at least 1MB) from the</code></span>
<span class="codeline" id="line-46"><code>//	   operating system. Allocating a large run of pages</code></span>
<span class="codeline" id="line-47"><code>//	   amortizes the cost of talking to the operating system.</code></span>
<span class="codeline" id="line-48"><code>//</code></span>
<span class="codeline" id="line-49"><code>// Sweeping an mspan and freeing objects on it proceeds up a similar</code></span>
<span class="codeline" id="line-50"><code>// hierarchy:</code></span>
<span class="codeline" id="line-51"><code>//</code></span>
<span class="codeline" id="line-52"><code>//	1. If the mspan is being swept in response to allocation, it</code></span>
<span class="codeline" id="line-53"><code>//	   is returned to the mcache to satisfy the allocation.</code></span>
<span class="codeline" id="line-54"><code>//</code></span>
<span class="codeline" id="line-55"><code>//	2. Otherwise, if the mspan still has allocated objects in it,</code></span>
<span class="codeline" id="line-56"><code>//	   it is placed on the mcentral free list for the mspan's size</code></span>
<span class="codeline" id="line-57"><code>//	   class.</code></span>
<span class="codeline" id="line-58"><code>//</code></span>
<span class="codeline" id="line-59"><code>//	3. Otherwise, if all objects in the mspan are free, the mspan's</code></span>
<span class="codeline" id="line-60"><code>//	   pages are returned to the mheap and the mspan is now dead.</code></span>
<span class="codeline" id="line-61"><code>//</code></span>
<span class="codeline" id="line-62"><code>// Allocating and freeing a large object uses the mheap</code></span>
<span class="codeline" id="line-63"><code>// directly, bypassing the mcache and mcentral.</code></span>
<span class="codeline" id="line-64"><code>//</code></span>
<span class="codeline" id="line-65"><code>// If mspan.needzero is false, then free object slots in the mspan are</code></span>
<span class="codeline" id="line-66"><code>// already zeroed. Otherwise if needzero is true, objects are zeroed as</code></span>
<span class="codeline" id="line-67"><code>// they are allocated. There are various benefits to delaying zeroing</code></span>
<span class="codeline" id="line-68"><code>// this way:</code></span>
<span class="codeline" id="line-69"><code>//</code></span>
<span class="codeline" id="line-70"><code>//	1. Stack frame allocation can avoid zeroing altogether.</code></span>
<span class="codeline" id="line-71"><code>//</code></span>
<span class="codeline" id="line-72"><code>//	2. It exhibits better temporal locality, since the program is</code></span>
<span class="codeline" id="line-73"><code>//	   probably about to write to the memory.</code></span>
<span class="codeline" id="line-74"><code>//</code></span>
<span class="codeline" id="line-75"><code>//	3. We don't zero pages that never get reused.</code></span>
<span class="codeline" id="line-76"><code></code></span>
<span class="codeline" id="line-77"><code>// Virtual memory layout</code></span>
<span class="codeline" id="line-78"><code>//</code></span>
<span class="codeline" id="line-79"><code>// The heap consists of a set of arenas, which are 64MB on 64-bit and</code></span>
<span class="codeline" id="line-80"><code>// 4MB on 32-bit (heapArenaBytes). Each arena's start address is also</code></span>
<span class="codeline" id="line-81"><code>// aligned to the arena size.</code></span>
<span class="codeline" id="line-82"><code>//</code></span>
<span class="codeline" id="line-83"><code>// Each arena has an associated heapArena object that stores the</code></span>
<span class="codeline" id="line-84"><code>// metadata for that arena: the heap bitmap for all words in the arena</code></span>
<span class="codeline" id="line-85"><code>// and the span map for all pages in the arena. heapArena objects are</code></span>
<span class="codeline" id="line-86"><code>// themselves allocated off-heap.</code></span>
<span class="codeline" id="line-87"><code>//</code></span>
<span class="codeline" id="line-88"><code>// Since arenas are aligned, the address space can be viewed as a</code></span>
<span class="codeline" id="line-89"><code>// series of arena frames. The arena map (mheap_.arenas) maps from</code></span>
<span class="codeline" id="line-90"><code>// arena frame number to *heapArena, or nil for parts of the address</code></span>
<span class="codeline" id="line-91"><code>// space not backed by the Go heap. The arena map is structured as a</code></span>
<span class="codeline" id="line-92"><code>// two-level array consisting of a "L1" arena map and many "L2" arena</code></span>
<span class="codeline" id="line-93"><code>// maps; however, since arenas are large, on many architectures, the</code></span>
<span class="codeline" id="line-94"><code>// arena map consists of a single, large L2 map.</code></span>
<span class="codeline" id="line-95"><code>//</code></span>
<span class="codeline" id="line-96"><code>// The arena map covers the entire possible address space, allowing</code></span>
<span class="codeline" id="line-97"><code>// the Go heap to use any part of the address space. The allocator</code></span>
<span class="codeline" id="line-98"><code>// attempts to keep arenas contiguous so that large spans (and hence</code></span>
<span class="codeline" id="line-99"><code>// large objects) can cross arenas.</code></span>
<span class="codeline" id="line-100"><code></code></span>
<span class="codeline" id="line-101"><code>package runtime</code></span>
<span class="codeline" id="line-102"><code></code></span>
<span class="codeline" id="line-103"><code>import (</code></span>
<span class="codeline" id="line-104"><code>	"runtime/internal/atomic"</code></span>
<span class="codeline" id="line-105"><code>	"runtime/internal/math"</code></span>
<span class="codeline" id="line-106"><code>	"runtime/internal/sys"</code></span>
<span class="codeline" id="line-107"><code>	"unsafe"</code></span>
<span class="codeline" id="line-108"><code>)</code></span>
<span class="codeline" id="line-109"><code></code></span>
<span class="codeline" id="line-110"><code>const (</code></span>
<span class="codeline" id="line-111"><code>	debugMalloc = false</code></span>
<span class="codeline" id="line-112"><code></code></span>
<span class="codeline" id="line-113"><code>	maxTinySize   = _TinySize</code></span>
<span class="codeline" id="line-114"><code>	tinySizeClass = _TinySizeClass</code></span>
<span class="codeline" id="line-115"><code>	maxSmallSize  = _MaxSmallSize</code></span>
<span class="codeline" id="line-116"><code></code></span>
<span class="codeline" id="line-117"><code>	pageShift = _PageShift</code></span>
<span class="codeline" id="line-118"><code>	pageSize  = _PageSize</code></span>
<span class="codeline" id="line-119"><code>	pageMask  = _PageMask</code></span>
<span class="codeline" id="line-120"><code>	// By construction, single page spans of the smallest object class</code></span>
<span class="codeline" id="line-121"><code>	// have the most objects per span.</code></span>
<span class="codeline" id="line-122"><code>	maxObjsPerSpan = pageSize / 8</code></span>
<span class="codeline" id="line-123"><code></code></span>
<span class="codeline" id="line-124"><code>	concurrentSweep = _ConcurrentSweep</code></span>
<span class="codeline" id="line-125"><code></code></span>
<span class="codeline" id="line-126"><code>	_PageSize = 1 &lt;&lt; _PageShift</code></span>
<span class="codeline" id="line-127"><code>	_PageMask = _PageSize - 1</code></span>
<span class="codeline" id="line-128"><code></code></span>
<span class="codeline" id="line-129"><code>	// _64bit = 1 on 64-bit systems, 0 on 32-bit systems</code></span>
<span class="codeline" id="line-130"><code>	_64bit = 1 &lt;&lt; (^uintptr(0) &gt;&gt; 63) / 2</code></span>
<span class="codeline" id="line-131"><code></code></span>
<span class="codeline" id="line-132"><code>	// Tiny allocator parameters, see "Tiny allocator" comment in malloc.go.</code></span>
<span class="codeline" id="line-133"><code>	_TinySize      = 16</code></span>
<span class="codeline" id="line-134"><code>	_TinySizeClass = int8(2)</code></span>
<span class="codeline" id="line-135"><code></code></span>
<span class="codeline" id="line-136"><code>	_FixAllocChunk = 16 &lt;&lt; 10 // Chunk size for FixAlloc</code></span>
<span class="codeline" id="line-137"><code></code></span>
<span class="codeline" id="line-138"><code>	// Per-P, per order stack segment cache size.</code></span>
<span class="codeline" id="line-139"><code>	_StackCacheSize = 32 * 1024</code></span>
<span class="codeline" id="line-140"><code></code></span>
<span class="codeline" id="line-141"><code>	// Number of orders that get caching. Order 0 is FixedStack</code></span>
<span class="codeline" id="line-142"><code>	// and each successive order is twice as large.</code></span>
<span class="codeline" id="line-143"><code>	// We want to cache 2KB, 4KB, 8KB, and 16KB stacks. Larger stacks</code></span>
<span class="codeline" id="line-144"><code>	// will be allocated directly.</code></span>
<span class="codeline" id="line-145"><code>	// Since FixedStack is different on different systems, we</code></span>
<span class="codeline" id="line-146"><code>	// must vary NumStackOrders to keep the same maximum cached size.</code></span>
<span class="codeline" id="line-147"><code>	//   OS               | FixedStack | NumStackOrders</code></span>
<span class="codeline" id="line-148"><code>	//   -----------------+------------+---------------</code></span>
<span class="codeline" id="line-149"><code>	//   linux/darwin/bsd | 2KB        | 4</code></span>
<span class="codeline" id="line-150"><code>	//   windows/32       | 4KB        | 3</code></span>
<span class="codeline" id="line-151"><code>	//   windows/64       | 8KB        | 2</code></span>
<span class="codeline" id="line-152"><code>	//   plan9            | 4KB        | 3</code></span>
<span class="codeline" id="line-153"><code>	_NumStackOrders = 4 - sys.PtrSize/4*sys.GoosWindows - 1*sys.GoosPlan9</code></span>
<span class="codeline" id="line-154"><code></code></span>
<span class="codeline" id="line-155"><code>	// heapAddrBits is the number of bits in a heap address. On</code></span>
<span class="codeline" id="line-156"><code>	// amd64, addresses are sign-extended beyond heapAddrBits. On</code></span>
<span class="codeline" id="line-157"><code>	// other arches, they are zero-extended.</code></span>
<span class="codeline" id="line-158"><code>	//</code></span>
<span class="codeline" id="line-159"><code>	// On most 64-bit platforms, we limit this to 48 bits based on a</code></span>
<span class="codeline" id="line-160"><code>	// combination of hardware and OS limitations.</code></span>
<span class="codeline" id="line-161"><code>	//</code></span>
<span class="codeline" id="line-162"><code>	// amd64 hardware limits addresses to 48 bits, sign-extended</code></span>
<span class="codeline" id="line-163"><code>	// to 64 bits. Addresses where the top 16 bits are not either</code></span>
<span class="codeline" id="line-164"><code>	// all 0 or all 1 are "non-canonical" and invalid. Because of</code></span>
<span class="codeline" id="line-165"><code>	// these "negative" addresses, we offset addresses by 1&lt;&lt;47</code></span>
<span class="codeline" id="line-166"><code>	// (arenaBaseOffset) on amd64 before computing indexes into</code></span>
<span class="codeline" id="line-167"><code>	// the heap arenas index. In 2017, amd64 hardware added</code></span>
<span class="codeline" id="line-168"><code>	// support for 57 bit addresses; however, currently only Linux</code></span>
<span class="codeline" id="line-169"><code>	// supports this extension and the kernel will never choose an</code></span>
<span class="codeline" id="line-170"><code>	// address above 1&lt;&lt;47 unless mmap is called with a hint</code></span>
<span class="codeline" id="line-171"><code>	// address above 1&lt;&lt;47 (which we never do).</code></span>
<span class="codeline" id="line-172"><code>	//</code></span>
<span class="codeline" id="line-173"><code>	// arm64 hardware (as of ARMv8) limits user addresses to 48</code></span>
<span class="codeline" id="line-174"><code>	// bits, in the range [0, 1&lt;&lt;48).</code></span>
<span class="codeline" id="line-175"><code>	//</code></span>
<span class="codeline" id="line-176"><code>	// ppc64, mips64, and s390x support arbitrary 64 bit addresses</code></span>
<span class="codeline" id="line-177"><code>	// in hardware. On Linux, Go leans on stricter OS limits. Based</code></span>
<span class="codeline" id="line-178"><code>	// on Linux's processor.h, the user address space is limited as</code></span>
<span class="codeline" id="line-179"><code>	// follows on 64-bit architectures:</code></span>
<span class="codeline" id="line-180"><code>	//</code></span>
<span class="codeline" id="line-181"><code>	// Architecture  Name              Maximum Value (exclusive)</code></span>
<span class="codeline" id="line-182"><code>	// ---------------------------------------------------------------------</code></span>
<span class="codeline" id="line-183"><code>	// amd64         TASK_SIZE_MAX     0x007ffffffff000 (47 bit addresses)</code></span>
<span class="codeline" id="line-184"><code>	// arm64         TASK_SIZE_64      0x01000000000000 (48 bit addresses)</code></span>
<span class="codeline" id="line-185"><code>	// ppc64{,le}    TASK_SIZE_USER64  0x00400000000000 (46 bit addresses)</code></span>
<span class="codeline" id="line-186"><code>	// mips64{,le}   TASK_SIZE64       0x00010000000000 (40 bit addresses)</code></span>
<span class="codeline" id="line-187"><code>	// s390x         TASK_SIZE         1&lt;&lt;64 (64 bit addresses)</code></span>
<span class="codeline" id="line-188"><code>	//</code></span>
<span class="codeline" id="line-189"><code>	// These limits may increase over time, but are currently at</code></span>
<span class="codeline" id="line-190"><code>	// most 48 bits except on s390x. On all architectures, Linux</code></span>
<span class="codeline" id="line-191"><code>	// starts placing mmap'd regions at addresses that are</code></span>
<span class="codeline" id="line-192"><code>	// significantly below 48 bits, so even if it's possible to</code></span>
<span class="codeline" id="line-193"><code>	// exceed Go's 48 bit limit, it's extremely unlikely in</code></span>
<span class="codeline" id="line-194"><code>	// practice.</code></span>
<span class="codeline" id="line-195"><code>	//</code></span>
<span class="codeline" id="line-196"><code>	// On 32-bit platforms, we accept the full 32-bit address</code></span>
<span class="codeline" id="line-197"><code>	// space because doing so is cheap.</code></span>
<span class="codeline" id="line-198"><code>	// mips32 only has access to the low 2GB of virtual memory, so</code></span>
<span class="codeline" id="line-199"><code>	// we further limit it to 31 bits.</code></span>
<span class="codeline" id="line-200"><code>	//</code></span>
<span class="codeline" id="line-201"><code>	// On ios/arm64, although 64-bit pointers are presumably</code></span>
<span class="codeline" id="line-202"><code>	// available, pointers are truncated to 33 bits. Furthermore,</code></span>
<span class="codeline" id="line-203"><code>	// only the top 4 GiB of the address space are actually available</code></span>
<span class="codeline" id="line-204"><code>	// to the application, but we allow the whole 33 bits anyway for</code></span>
<span class="codeline" id="line-205"><code>	// simplicity.</code></span>
<span class="codeline" id="line-206"><code>	// TODO(mknyszek): Consider limiting it to 32 bits and using</code></span>
<span class="codeline" id="line-207"><code>	// arenaBaseOffset to offset into the top 4 GiB.</code></span>
<span class="codeline" id="line-208"><code>	//</code></span>
<span class="codeline" id="line-209"><code>	// WebAssembly currently has a limit of 4GB linear memory.</code></span>
<span class="codeline" id="line-210"><code>	heapAddrBits = (_64bit*(1-sys.GoarchWasm)*(1-sys.GoosIos*sys.GoarchArm64))*48 + (1-_64bit+sys.GoarchWasm)*(32-(sys.GoarchMips+sys.GoarchMipsle)) + 33*sys.GoosIos*sys.GoarchArm64</code></span>
<span class="codeline" id="line-211"><code></code></span>
<span class="codeline" id="line-212"><code>	// maxAlloc is the maximum size of an allocation. On 64-bit,</code></span>
<span class="codeline" id="line-213"><code>	// it's theoretically possible to allocate 1&lt;&lt;heapAddrBits bytes. On</code></span>
<span class="codeline" id="line-214"><code>	// 32-bit, however, this is one less than 1&lt;&lt;32 because the</code></span>
<span class="codeline" id="line-215"><code>	// number of bytes in the address space doesn't actually fit</code></span>
<span class="codeline" id="line-216"><code>	// in a uintptr.</code></span>
<span class="codeline" id="line-217"><code>	maxAlloc = (1 &lt;&lt; heapAddrBits) - (1-_64bit)*1</code></span>
<span class="codeline" id="line-218"><code></code></span>
<span class="codeline" id="line-219"><code>	// The number of bits in a heap address, the size of heap</code></span>
<span class="codeline" id="line-220"><code>	// arenas, and the L1 and L2 arena map sizes are related by</code></span>
<span class="codeline" id="line-221"><code>	//</code></span>
<span class="codeline" id="line-222"><code>	//   (1 &lt;&lt; addr bits) = arena size * L1 entries * L2 entries</code></span>
<span class="codeline" id="line-223"><code>	//</code></span>
<span class="codeline" id="line-224"><code>	// Currently, we balance these as follows:</code></span>
<span class="codeline" id="line-225"><code>	//</code></span>
<span class="codeline" id="line-226"><code>	//       Platform  Addr bits  Arena size  L1 entries   L2 entries</code></span>
<span class="codeline" id="line-227"><code>	// --------------  ---------  ----------  ----------  -----------</code></span>
<span class="codeline" id="line-228"><code>	//       */64-bit         48        64MB           1    4M (32MB)</code></span>
<span class="codeline" id="line-229"><code>	// windows/64-bit         48         4MB          64    1M  (8MB)</code></span>
<span class="codeline" id="line-230"><code>	//       */32-bit         32         4MB           1  1024  (4KB)</code></span>
<span class="codeline" id="line-231"><code>	//     */mips(le)         31         4MB           1   512  (2KB)</code></span>
<span class="codeline" id="line-232"><code></code></span>
<span class="codeline" id="line-233"><code>	// heapArenaBytes is the size of a heap arena. The heap</code></span>
<span class="codeline" id="line-234"><code>	// consists of mappings of size heapArenaBytes, aligned to</code></span>
<span class="codeline" id="line-235"><code>	// heapArenaBytes. The initial heap mapping is one arena.</code></span>
<span class="codeline" id="line-236"><code>	//</code></span>
<span class="codeline" id="line-237"><code>	// This is currently 64MB on 64-bit non-Windows and 4MB on</code></span>
<span class="codeline" id="line-238"><code>	// 32-bit and on Windows. We use smaller arenas on Windows</code></span>
<span class="codeline" id="line-239"><code>	// because all committed memory is charged to the process,</code></span>
<span class="codeline" id="line-240"><code>	// even if it's not touched. Hence, for processes with small</code></span>
<span class="codeline" id="line-241"><code>	// heaps, the mapped arena space needs to be commensurate.</code></span>
<span class="codeline" id="line-242"><code>	// This is particularly important with the race detector,</code></span>
<span class="codeline" id="line-243"><code>	// since it significantly amplifies the cost of committed</code></span>
<span class="codeline" id="line-244"><code>	// memory.</code></span>
<span class="codeline" id="line-245"><code>	heapArenaBytes = 1 &lt;&lt; logHeapArenaBytes</code></span>
<span class="codeline" id="line-246"><code></code></span>
<span class="codeline" id="line-247"><code>	// logHeapArenaBytes is log_2 of heapArenaBytes. For clarity,</code></span>
<span class="codeline" id="line-248"><code>	// prefer using heapArenaBytes where possible (we need the</code></span>
<span class="codeline" id="line-249"><code>	// constant to compute some other constants).</code></span>
<span class="codeline" id="line-250"><code>	logHeapArenaBytes = (6+20)*(_64bit*(1-sys.GoosWindows)*(1-sys.GoarchWasm)) + (2+20)*(_64bit*sys.GoosWindows) + (2+20)*(1-_64bit) + (2+20)*sys.GoarchWasm</code></span>
<span class="codeline" id="line-251"><code></code></span>
<span class="codeline" id="line-252"><code>	// heapArenaBitmapBytes is the size of each heap arena's bitmap.</code></span>
<span class="codeline" id="line-253"><code>	heapArenaBitmapBytes = heapArenaBytes / (sys.PtrSize * 8 / 2)</code></span>
<span class="codeline" id="line-254"><code></code></span>
<span class="codeline" id="line-255"><code>	pagesPerArena = heapArenaBytes / pageSize</code></span>
<span class="codeline" id="line-256"><code></code></span>
<span class="codeline" id="line-257"><code>	// arenaL1Bits is the number of bits of the arena number</code></span>
<span class="codeline" id="line-258"><code>	// covered by the first level arena map.</code></span>
<span class="codeline" id="line-259"><code>	//</code></span>
<span class="codeline" id="line-260"><code>	// This number should be small, since the first level arena</code></span>
<span class="codeline" id="line-261"><code>	// map requires PtrSize*(1&lt;&lt;arenaL1Bits) of space in the</code></span>
<span class="codeline" id="line-262"><code>	// binary's BSS. It can be zero, in which case the first level</code></span>
<span class="codeline" id="line-263"><code>	// index is effectively unused. There is a performance benefit</code></span>
<span class="codeline" id="line-264"><code>	// to this, since the generated code can be more efficient,</code></span>
<span class="codeline" id="line-265"><code>	// but comes at the cost of having a large L2 mapping.</code></span>
<span class="codeline" id="line-266"><code>	//</code></span>
<span class="codeline" id="line-267"><code>	// We use the L1 map on 64-bit Windows because the arena size</code></span>
<span class="codeline" id="line-268"><code>	// is small, but the address space is still 48 bits, and</code></span>
<span class="codeline" id="line-269"><code>	// there's a high cost to having a large L2.</code></span>
<span class="codeline" id="line-270"><code>	arenaL1Bits = 6 * (_64bit * sys.GoosWindows)</code></span>
<span class="codeline" id="line-271"><code></code></span>
<span class="codeline" id="line-272"><code>	// arenaL2Bits is the number of bits of the arena number</code></span>
<span class="codeline" id="line-273"><code>	// covered by the second level arena index.</code></span>
<span class="codeline" id="line-274"><code>	//</code></span>
<span class="codeline" id="line-275"><code>	// The size of each arena map allocation is proportional to</code></span>
<span class="codeline" id="line-276"><code>	// 1&lt;&lt;arenaL2Bits, so it's important that this not be too</code></span>
<span class="codeline" id="line-277"><code>	// large. 48 bits leads to 32MB arena index allocations, which</code></span>
<span class="codeline" id="line-278"><code>	// is about the practical threshold.</code></span>
<span class="codeline" id="line-279"><code>	arenaL2Bits = heapAddrBits - logHeapArenaBytes - arenaL1Bits</code></span>
<span class="codeline" id="line-280"><code></code></span>
<span class="codeline" id="line-281"><code>	// arenaL1Shift is the number of bits to shift an arena frame</code></span>
<span class="codeline" id="line-282"><code>	// number by to compute an index into the first level arena map.</code></span>
<span class="codeline" id="line-283"><code>	arenaL1Shift = arenaL2Bits</code></span>
<span class="codeline" id="line-284"><code></code></span>
<span class="codeline" id="line-285"><code>	// arenaBits is the total bits in a combined arena map index.</code></span>
<span class="codeline" id="line-286"><code>	// This is split between the index into the L1 arena map and</code></span>
<span class="codeline" id="line-287"><code>	// the L2 arena map.</code></span>
<span class="codeline" id="line-288"><code>	arenaBits = arenaL1Bits + arenaL2Bits</code></span>
<span class="codeline" id="line-289"><code></code></span>
<span class="codeline" id="line-290"><code>	// arenaBaseOffset is the pointer value that corresponds to</code></span>
<span class="codeline" id="line-291"><code>	// index 0 in the heap arena map.</code></span>
<span class="codeline" id="line-292"><code>	//</code></span>
<span class="codeline" id="line-293"><code>	// On amd64, the address space is 48 bits, sign extended to 64</code></span>
<span class="codeline" id="line-294"><code>	// bits. This offset lets us handle "negative" addresses (or</code></span>
<span class="codeline" id="line-295"><code>	// high addresses if viewed as unsigned).</code></span>
<span class="codeline" id="line-296"><code>	//</code></span>
<span class="codeline" id="line-297"><code>	// On aix/ppc64, this offset allows to keep the heapAddrBits to</code></span>
<span class="codeline" id="line-298"><code>	// 48. Otherwize, it would be 60 in order to handle mmap addresses</code></span>
<span class="codeline" id="line-299"><code>	// (in range 0x0a00000000000000 - 0x0afffffffffffff). But in this</code></span>
<span class="codeline" id="line-300"><code>	// case, the memory reserved in (s *pageAlloc).init for chunks</code></span>
<span class="codeline" id="line-301"><code>	// is causing important slowdowns.</code></span>
<span class="codeline" id="line-302"><code>	//</code></span>
<span class="codeline" id="line-303"><code>	// On other platforms, the user address space is contiguous</code></span>
<span class="codeline" id="line-304"><code>	// and starts at 0, so no offset is necessary.</code></span>
<span class="codeline" id="line-305"><code>	arenaBaseOffset = 0xffff800000000000*sys.GoarchAmd64 + 0x0a00000000000000*sys.GoosAix</code></span>
<span class="codeline" id="line-306"><code>	// A typed version of this constant that will make it into DWARF (for viewcore).</code></span>
<span class="codeline" id="line-307"><code>	arenaBaseOffsetUintptr = uintptr(arenaBaseOffset)</code></span>
<span class="codeline" id="line-308"><code></code></span>
<span class="codeline" id="line-309"><code>	// Max number of threads to run garbage collection.</code></span>
<span class="codeline" id="line-310"><code>	// 2, 3, and 4 are all plausible maximums depending</code></span>
<span class="codeline" id="line-311"><code>	// on the hardware details of the machine. The garbage</code></span>
<span class="codeline" id="line-312"><code>	// collector scales well to 32 cpus.</code></span>
<span class="codeline" id="line-313"><code>	_MaxGcproc = 32</code></span>
<span class="codeline" id="line-314"><code></code></span>
<span class="codeline" id="line-315"><code>	// minLegalPointer is the smallest possible legal pointer.</code></span>
<span class="codeline" id="line-316"><code>	// This is the smallest possible architectural page size,</code></span>
<span class="codeline" id="line-317"><code>	// since we assume that the first page is never mapped.</code></span>
<span class="codeline" id="line-318"><code>	//</code></span>
<span class="codeline" id="line-319"><code>	// This should agree with minZeroPage in the compiler.</code></span>
<span class="codeline" id="line-320"><code>	minLegalPointer uintptr = 4096</code></span>
<span class="codeline" id="line-321"><code>)</code></span>
<span class="codeline" id="line-322"><code></code></span>
<span class="codeline" id="line-323"><code>// physPageSize is the size in bytes of the OS's physical pages.</code></span>
<span class="codeline" id="line-324"><code>// Mapping and unmapping operations must be done at multiples of</code></span>
<span class="codeline" id="line-325"><code>// physPageSize.</code></span>
<span class="codeline" id="line-326"><code>//</code></span>
<span class="codeline" id="line-327"><code>// This must be set by the OS init code (typically in osinit) before</code></span>
<span class="codeline" id="line-328"><code>// mallocinit.</code></span>
<span class="codeline" id="line-329"><code>var physPageSize uintptr</code></span>
<span class="codeline" id="line-330"><code></code></span>
<span class="codeline" id="line-331"><code>// physHugePageSize is the size in bytes of the OS's default physical huge</code></span>
<span class="codeline" id="line-332"><code>// page size whose allocation is opaque to the application. It is assumed</code></span>
<span class="codeline" id="line-333"><code>// and verified to be a power of two.</code></span>
<span class="codeline" id="line-334"><code>//</code></span>
<span class="codeline" id="line-335"><code>// If set, this must be set by the OS init code (typically in osinit) before</code></span>
<span class="codeline" id="line-336"><code>// mallocinit. However, setting it at all is optional, and leaving the default</code></span>
<span class="codeline" id="line-337"><code>// value is always safe (though potentially less efficient).</code></span>
<span class="codeline" id="line-338"><code>//</code></span>
<span class="codeline" id="line-339"><code>// Since physHugePageSize is always assumed to be a power of two,</code></span>
<span class="codeline" id="line-340"><code>// physHugePageShift is defined as physHugePageSize == 1 &lt;&lt; physHugePageShift.</code></span>
<span class="codeline" id="line-341"><code>// The purpose of physHugePageShift is to avoid doing divisions in</code></span>
<span class="codeline" id="line-342"><code>// performance critical functions.</code></span>
<span class="codeline" id="line-343"><code>var (</code></span>
<span class="codeline" id="line-344"><code>	physHugePageSize  uintptr</code></span>
<span class="codeline" id="line-345"><code>	physHugePageShift uint</code></span>
<span class="codeline" id="line-346"><code>)</code></span>
<span class="codeline" id="line-347"><code></code></span>
<span class="codeline" id="line-348"><code>// OS memory management abstraction layer</code></span>
<span class="codeline" id="line-349"><code>//</code></span>
<span class="codeline" id="line-350"><code>// Regions of the address space managed by the runtime may be in one of four</code></span>
<span class="codeline" id="line-351"><code>// states at any given time:</code></span>
<span class="codeline" id="line-352"><code>// 1) None - Unreserved and unmapped, the default state of any region.</code></span>
<span class="codeline" id="line-353"><code>// 2) Reserved - Owned by the runtime, but accessing it would cause a fault.</code></span>
<span class="codeline" id="line-354"><code>//               Does not count against the process' memory footprint.</code></span>
<span class="codeline" id="line-355"><code>// 3) Prepared - Reserved, intended not to be backed by physical memory (though</code></span>
<span class="codeline" id="line-356"><code>//               an OS may implement this lazily). Can transition efficiently to</code></span>
<span class="codeline" id="line-357"><code>//               Ready. Accessing memory in such a region is undefined (may</code></span>
<span class="codeline" id="line-358"><code>//               fault, may give back unexpected zeroes, etc.).</code></span>
<span class="codeline" id="line-359"><code>// 4) Ready - may be accessed safely.</code></span>
<span class="codeline" id="line-360"><code>//</code></span>
<span class="codeline" id="line-361"><code>// This set of states is more than is strictly necessary to support all the</code></span>
<span class="codeline" id="line-362"><code>// currently supported platforms. One could get by with just None, Reserved, and</code></span>
<span class="codeline" id="line-363"><code>// Ready. However, the Prepared state gives us flexibility for performance</code></span>
<span class="codeline" id="line-364"><code>// purposes. For example, on POSIX-y operating systems, Reserved is usually a</code></span>
<span class="codeline" id="line-365"><code>// private anonymous mmap'd region with PROT_NONE set, and to transition</code></span>
<span class="codeline" id="line-366"><code>// to Ready would require setting PROT_READ|PROT_WRITE. However the</code></span>
<span class="codeline" id="line-367"><code>// underspecification of Prepared lets us use just MADV_FREE to transition from</code></span>
<span class="codeline" id="line-368"><code>// Ready to Prepared. Thus with the Prepared state we can set the permission</code></span>
<span class="codeline" id="line-369"><code>// bits just once early on, we can efficiently tell the OS that it's free to</code></span>
<span class="codeline" id="line-370"><code>// take pages away from us when we don't strictly need them.</code></span>
<span class="codeline" id="line-371"><code>//</code></span>
<span class="codeline" id="line-372"><code>// For each OS there is a common set of helpers defined that transition</code></span>
<span class="codeline" id="line-373"><code>// memory regions between these states. The helpers are as follows:</code></span>
<span class="codeline" id="line-374"><code>//</code></span>
<span class="codeline" id="line-375"><code>// sysAlloc transitions an OS-chosen region of memory from None to Ready.</code></span>
<span class="codeline" id="line-376"><code>// More specifically, it obtains a large chunk of zeroed memory from the</code></span>
<span class="codeline" id="line-377"><code>// operating system, typically on the order of a hundred kilobytes</code></span>
<span class="codeline" id="line-378"><code>// or a megabyte. This memory is always immediately available for use.</code></span>
<span class="codeline" id="line-379"><code>//</code></span>
<span class="codeline" id="line-380"><code>// sysFree transitions a memory region from any state to None. Therefore, it</code></span>
<span class="codeline" id="line-381"><code>// returns memory unconditionally. It is used if an out-of-memory error has been</code></span>
<span class="codeline" id="line-382"><code>// detected midway through an allocation or to carve out an aligned section of</code></span>
<span class="codeline" id="line-383"><code>// the address space. It is okay if sysFree is a no-op only if sysReserve always</code></span>
<span class="codeline" id="line-384"><code>// returns a memory region aligned to the heap allocator's alignment</code></span>
<span class="codeline" id="line-385"><code>// restrictions.</code></span>
<span class="codeline" id="line-386"><code>//</code></span>
<span class="codeline" id="line-387"><code>// sysReserve transitions a memory region from None to Reserved. It reserves</code></span>
<span class="codeline" id="line-388"><code>// address space in such a way that it would cause a fatal fault upon access</code></span>
<span class="codeline" id="line-389"><code>// (either via permissions or not committing the memory). Such a reservation is</code></span>
<span class="codeline" id="line-390"><code>// thus never backed by physical memory.</code></span>
<span class="codeline" id="line-391"><code>// If the pointer passed to it is non-nil, the caller wants the</code></span>
<span class="codeline" id="line-392"><code>// reservation there, but sysReserve can still choose another</code></span>
<span class="codeline" id="line-393"><code>// location if that one is unavailable.</code></span>
<span class="codeline" id="line-394"><code>// NOTE: sysReserve returns OS-aligned memory, but the heap allocator</code></span>
<span class="codeline" id="line-395"><code>// may use larger alignment, so the caller must be careful to realign the</code></span>
<span class="codeline" id="line-396"><code>// memory obtained by sysReserve.</code></span>
<span class="codeline" id="line-397"><code>//</code></span>
<span class="codeline" id="line-398"><code>// sysMap transitions a memory region from Reserved to Prepared. It ensures the</code></span>
<span class="codeline" id="line-399"><code>// memory region can be efficiently transitioned to Ready.</code></span>
<span class="codeline" id="line-400"><code>//</code></span>
<span class="codeline" id="line-401"><code>// sysUsed transitions a memory region from Prepared to Ready. It notifies the</code></span>
<span class="codeline" id="line-402"><code>// operating system that the memory region is needed and ensures that the region</code></span>
<span class="codeline" id="line-403"><code>// may be safely accessed. This is typically a no-op on systems that don't have</code></span>
<span class="codeline" id="line-404"><code>// an explicit commit step and hard over-commit limits, but is critical on</code></span>
<span class="codeline" id="line-405"><code>// Windows, for example.</code></span>
<span class="codeline" id="line-406"><code>//</code></span>
<span class="codeline" id="line-407"><code>// sysUnused transitions a memory region from Ready to Prepared. It notifies the</code></span>
<span class="codeline" id="line-408"><code>// operating system that the physical pages backing this memory region are no</code></span>
<span class="codeline" id="line-409"><code>// longer needed and can be reused for other purposes. The contents of a</code></span>
<span class="codeline" id="line-410"><code>// sysUnused memory region are considered forfeit and the region must not be</code></span>
<span class="codeline" id="line-411"><code>// accessed again until sysUsed is called.</code></span>
<span class="codeline" id="line-412"><code>//</code></span>
<span class="codeline" id="line-413"><code>// sysFault transitions a memory region from Ready or Prepared to Reserved. It</code></span>
<span class="codeline" id="line-414"><code>// marks a region such that it will always fault if accessed. Used only for</code></span>
<span class="codeline" id="line-415"><code>// debugging the runtime.</code></span>
<span class="codeline" id="line-416"><code></code></span>
<span class="codeline" id="line-417"><code>func mallocinit() {</code></span>
<span class="codeline" id="line-418"><code>	if class_to_size[_TinySizeClass] != _TinySize {</code></span>
<span class="codeline" id="line-419"><code>		throw("bad TinySizeClass")</code></span>
<span class="codeline" id="line-420"><code>	}</code></span>
<span class="codeline" id="line-421"><code></code></span>
<span class="codeline" id="line-422"><code>	testdefersizes()</code></span>
<span class="codeline" id="line-423"><code></code></span>
<span class="codeline" id="line-424"><code>	if heapArenaBitmapBytes&amp;(heapArenaBitmapBytes-1) != 0 {</code></span>
<span class="codeline" id="line-425"><code>		// heapBits expects modular arithmetic on bitmap</code></span>
<span class="codeline" id="line-426"><code>		// addresses to work.</code></span>
<span class="codeline" id="line-427"><code>		throw("heapArenaBitmapBytes not a power of 2")</code></span>
<span class="codeline" id="line-428"><code>	}</code></span>
<span class="codeline" id="line-429"><code></code></span>
<span class="codeline" id="line-430"><code>	// Copy class sizes out for statistics table.</code></span>
<span class="codeline" id="line-431"><code>	for i := range class_to_size {</code></span>
<span class="codeline" id="line-432"><code>		memstats.by_size[i].size = uint32(class_to_size[i])</code></span>
<span class="codeline" id="line-433"><code>	}</code></span>
<span class="codeline" id="line-434"><code></code></span>
<span class="codeline" id="line-435"><code>	// Check physPageSize.</code></span>
<span class="codeline" id="line-436"><code>	if physPageSize == 0 {</code></span>
<span class="codeline" id="line-437"><code>		// The OS init code failed to fetch the physical page size.</code></span>
<span class="codeline" id="line-438"><code>		throw("failed to get system page size")</code></span>
<span class="codeline" id="line-439"><code>	}</code></span>
<span class="codeline" id="line-440"><code>	if physPageSize &gt; maxPhysPageSize {</code></span>
<span class="codeline" id="line-441"><code>		print("system page size (", physPageSize, ") is larger than maximum page size (", maxPhysPageSize, ")\n")</code></span>
<span class="codeline" id="line-442"><code>		throw("bad system page size")</code></span>
<span class="codeline" id="line-443"><code>	}</code></span>
<span class="codeline" id="line-444"><code>	if physPageSize &lt; minPhysPageSize {</code></span>
<span class="codeline" id="line-445"><code>		print("system page size (", physPageSize, ") is smaller than minimum page size (", minPhysPageSize, ")\n")</code></span>
<span class="codeline" id="line-446"><code>		throw("bad system page size")</code></span>
<span class="codeline" id="line-447"><code>	}</code></span>
<span class="codeline" id="line-448"><code>	if physPageSize&amp;(physPageSize-1) != 0 {</code></span>
<span class="codeline" id="line-449"><code>		print("system page size (", physPageSize, ") must be a power of 2\n")</code></span>
<span class="codeline" id="line-450"><code>		throw("bad system page size")</code></span>
<span class="codeline" id="line-451"><code>	}</code></span>
<span class="codeline" id="line-452"><code>	if physHugePageSize&amp;(physHugePageSize-1) != 0 {</code></span>
<span class="codeline" id="line-453"><code>		print("system huge page size (", physHugePageSize, ") must be a power of 2\n")</code></span>
<span class="codeline" id="line-454"><code>		throw("bad system huge page size")</code></span>
<span class="codeline" id="line-455"><code>	}</code></span>
<span class="codeline" id="line-456"><code>	if physHugePageSize &gt; maxPhysHugePageSize {</code></span>
<span class="codeline" id="line-457"><code>		// physHugePageSize is greater than the maximum supported huge page size.</code></span>
<span class="codeline" id="line-458"><code>		// Don't throw here, like in the other cases, since a system configured</code></span>
<span class="codeline" id="line-459"><code>		// in this way isn't wrong, we just don't have the code to support them.</code></span>
<span class="codeline" id="line-460"><code>		// Instead, silently set the huge page size to zero.</code></span>
<span class="codeline" id="line-461"><code>		physHugePageSize = 0</code></span>
<span class="codeline" id="line-462"><code>	}</code></span>
<span class="codeline" id="line-463"><code>	if physHugePageSize != 0 {</code></span>
<span class="codeline" id="line-464"><code>		// Since physHugePageSize is a power of 2, it suffices to increase</code></span>
<span class="codeline" id="line-465"><code>		// physHugePageShift until 1&lt;&lt;physHugePageShift == physHugePageSize.</code></span>
<span class="codeline" id="line-466"><code>		for 1&lt;&lt;physHugePageShift != physHugePageSize {</code></span>
<span class="codeline" id="line-467"><code>			physHugePageShift++</code></span>
<span class="codeline" id="line-468"><code>		}</code></span>
<span class="codeline" id="line-469"><code>	}</code></span>
<span class="codeline" id="line-470"><code>	if pagesPerArena%pagesPerSpanRoot != 0 {</code></span>
<span class="codeline" id="line-471"><code>		print("pagesPerArena (", pagesPerArena, ") is not divisible by pagesPerSpanRoot (", pagesPerSpanRoot, ")\n")</code></span>
<span class="codeline" id="line-472"><code>		throw("bad pagesPerSpanRoot")</code></span>
<span class="codeline" id="line-473"><code>	}</code></span>
<span class="codeline" id="line-474"><code>	if pagesPerArena%pagesPerReclaimerChunk != 0 {</code></span>
<span class="codeline" id="line-475"><code>		print("pagesPerArena (", pagesPerArena, ") is not divisible by pagesPerReclaimerChunk (", pagesPerReclaimerChunk, ")\n")</code></span>
<span class="codeline" id="line-476"><code>		throw("bad pagesPerReclaimerChunk")</code></span>
<span class="codeline" id="line-477"><code>	}</code></span>
<span class="codeline" id="line-478"><code></code></span>
<span class="codeline" id="line-479"><code>	// Initialize the heap.</code></span>
<span class="codeline" id="line-480"><code>	mheap_.init()</code></span>
<span class="codeline" id="line-481"><code>	mcache0 = allocmcache()</code></span>
<span class="codeline" id="line-482"><code>	lockInit(&amp;gcBitsArenas.lock, lockRankGcBitsArenas)</code></span>
<span class="codeline" id="line-483"><code>	lockInit(&amp;proflock, lockRankProf)</code></span>
<span class="codeline" id="line-484"><code>	lockInit(&amp;globalAlloc.mutex, lockRankGlobalAlloc)</code></span>
<span class="codeline" id="line-485"><code></code></span>
<span class="codeline" id="line-486"><code>	// Create initial arena growth hints.</code></span>
<span class="codeline" id="line-487"><code>	if sys.PtrSize == 8 {</code></span>
<span class="codeline" id="line-488"><code>		// On a 64-bit machine, we pick the following hints</code></span>
<span class="codeline" id="line-489"><code>		// because:</code></span>
<span class="codeline" id="line-490"><code>		//</code></span>
<span class="codeline" id="line-491"><code>		// 1. Starting from the middle of the address space</code></span>
<span class="codeline" id="line-492"><code>		// makes it easier to grow out a contiguous range</code></span>
<span class="codeline" id="line-493"><code>		// without running in to some other mapping.</code></span>
<span class="codeline" id="line-494"><code>		//</code></span>
<span class="codeline" id="line-495"><code>		// 2. This makes Go heap addresses more easily</code></span>
<span class="codeline" id="line-496"><code>		// recognizable when debugging.</code></span>
<span class="codeline" id="line-497"><code>		//</code></span>
<span class="codeline" id="line-498"><code>		// 3. Stack scanning in gccgo is still conservative,</code></span>
<span class="codeline" id="line-499"><code>		// so it's important that addresses be distinguishable</code></span>
<span class="codeline" id="line-500"><code>		// from other data.</code></span>
<span class="codeline" id="line-501"><code>		//</code></span>
<span class="codeline" id="line-502"><code>		// Starting at 0x00c0 means that the valid memory addresses</code></span>
<span class="codeline" id="line-503"><code>		// will begin 0x00c0, 0x00c1, ...</code></span>
<span class="codeline" id="line-504"><code>		// In little-endian, that's c0 00, c1 00, ... None of those are valid</code></span>
<span class="codeline" id="line-505"><code>		// UTF-8 sequences, and they are otherwise as far away from</code></span>
<span class="codeline" id="line-506"><code>		// ff (likely a common byte) as possible. If that fails, we try other 0xXXc0</code></span>
<span class="codeline" id="line-507"><code>		// addresses. An earlier attempt to use 0x11f8 caused out of memory errors</code></span>
<span class="codeline" id="line-508"><code>		// on OS X during thread allocations.  0x00c0 causes conflicts with</code></span>
<span class="codeline" id="line-509"><code>		// AddressSanitizer which reserves all memory up to 0x0100.</code></span>
<span class="codeline" id="line-510"><code>		// These choices reduce the odds of a conservative garbage collector</code></span>
<span class="codeline" id="line-511"><code>		// not collecting memory because some non-pointer block of memory</code></span>
<span class="codeline" id="line-512"><code>		// had a bit pattern that matched a memory address.</code></span>
<span class="codeline" id="line-513"><code>		//</code></span>
<span class="codeline" id="line-514"><code>		// However, on arm64, we ignore all this advice above and slam the</code></span>
<span class="codeline" id="line-515"><code>		// allocation at 0x40 &lt;&lt; 32 because when using 4k pages with 3-level</code></span>
<span class="codeline" id="line-516"><code>		// translation buffers, the user address space is limited to 39 bits</code></span>
<span class="codeline" id="line-517"><code>		// On ios/arm64, the address space is even smaller.</code></span>
<span class="codeline" id="line-518"><code>		//</code></span>
<span class="codeline" id="line-519"><code>		// On AIX, mmaps starts at 0x0A00000000000000 for 64-bit.</code></span>
<span class="codeline" id="line-520"><code>		// processes.</code></span>
<span class="codeline" id="line-521"><code>		for i := 0x7f; i &gt;= 0; i-- {</code></span>
<span class="codeline" id="line-522"><code>			var p uintptr</code></span>
<span class="codeline" id="line-523"><code>			switch {</code></span>
<span class="codeline" id="line-524"><code>			case raceenabled:</code></span>
<span class="codeline" id="line-525"><code>				// The TSAN runtime requires the heap</code></span>
<span class="codeline" id="line-526"><code>				// to be in the range [0x00c000000000,</code></span>
<span class="codeline" id="line-527"><code>				// 0x00e000000000).</code></span>
<span class="codeline" id="line-528"><code>				p = uintptr(i)&lt;&lt;32 | uintptrMask&amp;(0x00c0&lt;&lt;32)</code></span>
<span class="codeline" id="line-529"><code>				if p &gt;= uintptrMask&amp;0x00e000000000 {</code></span>
<span class="codeline" id="line-530"><code>					continue</code></span>
<span class="codeline" id="line-531"><code>				}</code></span>
<span class="codeline" id="line-532"><code>			case GOARCH == "arm64" &amp;&amp; GOOS == "ios":</code></span>
<span class="codeline" id="line-533"><code>				p = uintptr(i)&lt;&lt;40 | uintptrMask&amp;(0x0013&lt;&lt;28)</code></span>
<span class="codeline" id="line-534"><code>			case GOARCH == "arm64":</code></span>
<span class="codeline" id="line-535"><code>				p = uintptr(i)&lt;&lt;40 | uintptrMask&amp;(0x0040&lt;&lt;32)</code></span>
<span class="codeline" id="line-536"><code>			case GOOS == "aix":</code></span>
<span class="codeline" id="line-537"><code>				if i == 0 {</code></span>
<span class="codeline" id="line-538"><code>					// We don't use addresses directly after 0x0A00000000000000</code></span>
<span class="codeline" id="line-539"><code>					// to avoid collisions with others mmaps done by non-go programs.</code></span>
<span class="codeline" id="line-540"><code>					continue</code></span>
<span class="codeline" id="line-541"><code>				}</code></span>
<span class="codeline" id="line-542"><code>				p = uintptr(i)&lt;&lt;40 | uintptrMask&amp;(0xa0&lt;&lt;52)</code></span>
<span class="codeline" id="line-543"><code>			default:</code></span>
<span class="codeline" id="line-544"><code>				p = uintptr(i)&lt;&lt;40 | uintptrMask&amp;(0x00c0&lt;&lt;32)</code></span>
<span class="codeline" id="line-545"><code>			}</code></span>
<span class="codeline" id="line-546"><code>			hint := (*arenaHint)(mheap_.arenaHintAlloc.alloc())</code></span>
<span class="codeline" id="line-547"><code>			hint.addr = p</code></span>
<span class="codeline" id="line-548"><code>			hint.next, mheap_.arenaHints = mheap_.arenaHints, hint</code></span>
<span class="codeline" id="line-549"><code>		}</code></span>
<span class="codeline" id="line-550"><code>	} else {</code></span>
<span class="codeline" id="line-551"><code>		// On a 32-bit machine, we're much more concerned</code></span>
<span class="codeline" id="line-552"><code>		// about keeping the usable heap contiguous.</code></span>
<span class="codeline" id="line-553"><code>		// Hence:</code></span>
<span class="codeline" id="line-554"><code>		//</code></span>
<span class="codeline" id="line-555"><code>		// 1. We reserve space for all heapArenas up front so</code></span>
<span class="codeline" id="line-556"><code>		// they don't get interleaved with the heap. They're</code></span>
<span class="codeline" id="line-557"><code>		// ~258MB, so this isn't too bad. (We could reserve a</code></span>
<span class="codeline" id="line-558"><code>		// smaller amount of space up front if this is a</code></span>
<span class="codeline" id="line-559"><code>		// problem.)</code></span>
<span class="codeline" id="line-560"><code>		//</code></span>
<span class="codeline" id="line-561"><code>		// 2. We hint the heap to start right above the end of</code></span>
<span class="codeline" id="line-562"><code>		// the binary so we have the best chance of keeping it</code></span>
<span class="codeline" id="line-563"><code>		// contiguous.</code></span>
<span class="codeline" id="line-564"><code>		//</code></span>
<span class="codeline" id="line-565"><code>		// 3. We try to stake out a reasonably large initial</code></span>
<span class="codeline" id="line-566"><code>		// heap reservation.</code></span>
<span class="codeline" id="line-567"><code></code></span>
<span class="codeline" id="line-568"><code>		const arenaMetaSize = (1 &lt;&lt; arenaBits) * unsafe.Sizeof(heapArena{})</code></span>
<span class="codeline" id="line-569"><code>		meta := uintptr(sysReserve(nil, arenaMetaSize))</code></span>
<span class="codeline" id="line-570"><code>		if meta != 0 {</code></span>
<span class="codeline" id="line-571"><code>			mheap_.heapArenaAlloc.init(meta, arenaMetaSize)</code></span>
<span class="codeline" id="line-572"><code>		}</code></span>
<span class="codeline" id="line-573"><code></code></span>
<span class="codeline" id="line-574"><code>		// We want to start the arena low, but if we're linked</code></span>
<span class="codeline" id="line-575"><code>		// against C code, it's possible global constructors</code></span>
<span class="codeline" id="line-576"><code>		// have called malloc and adjusted the process' brk.</code></span>
<span class="codeline" id="line-577"><code>		// Query the brk so we can avoid trying to map the</code></span>
<span class="codeline" id="line-578"><code>		// region over it (which will cause the kernel to put</code></span>
<span class="codeline" id="line-579"><code>		// the region somewhere else, likely at a high</code></span>
<span class="codeline" id="line-580"><code>		// address).</code></span>
<span class="codeline" id="line-581"><code>		procBrk := sbrk0()</code></span>
<span class="codeline" id="line-582"><code></code></span>
<span class="codeline" id="line-583"><code>		// If we ask for the end of the data segment but the</code></span>
<span class="codeline" id="line-584"><code>		// operating system requires a little more space</code></span>
<span class="codeline" id="line-585"><code>		// before we can start allocating, it will give out a</code></span>
<span class="codeline" id="line-586"><code>		// slightly higher pointer. Except QEMU, which is</code></span>
<span class="codeline" id="line-587"><code>		// buggy, as usual: it won't adjust the pointer</code></span>
<span class="codeline" id="line-588"><code>		// upward. So adjust it upward a little bit ourselves:</code></span>
<span class="codeline" id="line-589"><code>		// 1/4 MB to get away from the running binary image.</code></span>
<span class="codeline" id="line-590"><code>		p := firstmoduledata.end</code></span>
<span class="codeline" id="line-591"><code>		if p &lt; procBrk {</code></span>
<span class="codeline" id="line-592"><code>			p = procBrk</code></span>
<span class="codeline" id="line-593"><code>		}</code></span>
<span class="codeline" id="line-594"><code>		if mheap_.heapArenaAlloc.next &lt;= p &amp;&amp; p &lt; mheap_.heapArenaAlloc.end {</code></span>
<span class="codeline" id="line-595"><code>			p = mheap_.heapArenaAlloc.end</code></span>
<span class="codeline" id="line-596"><code>		}</code></span>
<span class="codeline" id="line-597"><code>		p = alignUp(p+(256&lt;&lt;10), heapArenaBytes)</code></span>
<span class="codeline" id="line-598"><code>		// Because we're worried about fragmentation on</code></span>
<span class="codeline" id="line-599"><code>		// 32-bit, we try to make a large initial reservation.</code></span>
<span class="codeline" id="line-600"><code>		arenaSizes := []uintptr{</code></span>
<span class="codeline" id="line-601"><code>			512 &lt;&lt; 20,</code></span>
<span class="codeline" id="line-602"><code>			256 &lt;&lt; 20,</code></span>
<span class="codeline" id="line-603"><code>			128 &lt;&lt; 20,</code></span>
<span class="codeline" id="line-604"><code>		}</code></span>
<span class="codeline" id="line-605"><code>		for _, arenaSize := range arenaSizes {</code></span>
<span class="codeline" id="line-606"><code>			a, size := sysReserveAligned(unsafe.Pointer(p), arenaSize, heapArenaBytes)</code></span>
<span class="codeline" id="line-607"><code>			if a != nil {</code></span>
<span class="codeline" id="line-608"><code>				mheap_.arena.init(uintptr(a), size)</code></span>
<span class="codeline" id="line-609"><code>				p = mheap_.arena.end // For hint below</code></span>
<span class="codeline" id="line-610"><code>				break</code></span>
<span class="codeline" id="line-611"><code>			}</code></span>
<span class="codeline" id="line-612"><code>		}</code></span>
<span class="codeline" id="line-613"><code>		hint := (*arenaHint)(mheap_.arenaHintAlloc.alloc())</code></span>
<span class="codeline" id="line-614"><code>		hint.addr = p</code></span>
<span class="codeline" id="line-615"><code>		hint.next, mheap_.arenaHints = mheap_.arenaHints, hint</code></span>
<span class="codeline" id="line-616"><code>	}</code></span>
<span class="codeline" id="line-617"><code>}</code></span>
<span class="codeline" id="line-618"><code></code></span>
<span class="codeline" id="line-619"><code>// sysAlloc allocates heap arena space for at least n bytes. The</code></span>
<span class="codeline" id="line-620"><code>// returned pointer is always heapArenaBytes-aligned and backed by</code></span>
<span class="codeline" id="line-621"><code>// h.arenas metadata. The returned size is always a multiple of</code></span>
<span class="codeline" id="line-622"><code>// heapArenaBytes. sysAlloc returns nil on failure.</code></span>
<span class="codeline" id="line-623"><code>// There is no corresponding free function.</code></span>
<span class="codeline" id="line-624"><code>//</code></span>
<span class="codeline" id="line-625"><code>// sysAlloc returns a memory region in the Prepared state. This region must</code></span>
<span class="codeline" id="line-626"><code>// be transitioned to Ready before use.</code></span>
<span class="codeline" id="line-627"><code>//</code></span>
<span class="codeline" id="line-628"><code>// h must be locked.</code></span>
<span class="codeline" id="line-629"><code>func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr) {</code></span>
<span class="codeline" id="line-630"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-631"><code></code></span>
<span class="codeline" id="line-632"><code>	n = alignUp(n, heapArenaBytes)</code></span>
<span class="codeline" id="line-633"><code></code></span>
<span class="codeline" id="line-634"><code>	// First, try the arena pre-reservation.</code></span>
<span class="codeline" id="line-635"><code>	v = h.arena.alloc(n, heapArenaBytes, &amp;memstats.heap_sys)</code></span>
<span class="codeline" id="line-636"><code>	if v != nil {</code></span>
<span class="codeline" id="line-637"><code>		size = n</code></span>
<span class="codeline" id="line-638"><code>		goto mapped</code></span>
<span class="codeline" id="line-639"><code>	}</code></span>
<span class="codeline" id="line-640"><code></code></span>
<span class="codeline" id="line-641"><code>	// Try to grow the heap at a hint address.</code></span>
<span class="codeline" id="line-642"><code>	for h.arenaHints != nil {</code></span>
<span class="codeline" id="line-643"><code>		hint := h.arenaHints</code></span>
<span class="codeline" id="line-644"><code>		p := hint.addr</code></span>
<span class="codeline" id="line-645"><code>		if hint.down {</code></span>
<span class="codeline" id="line-646"><code>			p -= n</code></span>
<span class="codeline" id="line-647"><code>		}</code></span>
<span class="codeline" id="line-648"><code>		if p+n &lt; p {</code></span>
<span class="codeline" id="line-649"><code>			// We can't use this, so don't ask.</code></span>
<span class="codeline" id="line-650"><code>			v = nil</code></span>
<span class="codeline" id="line-651"><code>		} else if arenaIndex(p+n-1) &gt;= 1&lt;&lt;arenaBits {</code></span>
<span class="codeline" id="line-652"><code>			// Outside addressable heap. Can't use.</code></span>
<span class="codeline" id="line-653"><code>			v = nil</code></span>
<span class="codeline" id="line-654"><code>		} else {</code></span>
<span class="codeline" id="line-655"><code>			v = sysReserve(unsafe.Pointer(p), n)</code></span>
<span class="codeline" id="line-656"><code>		}</code></span>
<span class="codeline" id="line-657"><code>		if p == uintptr(v) {</code></span>
<span class="codeline" id="line-658"><code>			// Success. Update the hint.</code></span>
<span class="codeline" id="line-659"><code>			if !hint.down {</code></span>
<span class="codeline" id="line-660"><code>				p += n</code></span>
<span class="codeline" id="line-661"><code>			}</code></span>
<span class="codeline" id="line-662"><code>			hint.addr = p</code></span>
<span class="codeline" id="line-663"><code>			size = n</code></span>
<span class="codeline" id="line-664"><code>			break</code></span>
<span class="codeline" id="line-665"><code>		}</code></span>
<span class="codeline" id="line-666"><code>		// Failed. Discard this hint and try the next.</code></span>
<span class="codeline" id="line-667"><code>		//</code></span>
<span class="codeline" id="line-668"><code>		// TODO: This would be cleaner if sysReserve could be</code></span>
<span class="codeline" id="line-669"><code>		// told to only return the requested address. In</code></span>
<span class="codeline" id="line-670"><code>		// particular, this is already how Windows behaves, so</code></span>
<span class="codeline" id="line-671"><code>		// it would simplify things there.</code></span>
<span class="codeline" id="line-672"><code>		if v != nil {</code></span>
<span class="codeline" id="line-673"><code>			sysFree(v, n, nil)</code></span>
<span class="codeline" id="line-674"><code>		}</code></span>
<span class="codeline" id="line-675"><code>		h.arenaHints = hint.next</code></span>
<span class="codeline" id="line-676"><code>		h.arenaHintAlloc.free(unsafe.Pointer(hint))</code></span>
<span class="codeline" id="line-677"><code>	}</code></span>
<span class="codeline" id="line-678"><code></code></span>
<span class="codeline" id="line-679"><code>	if size == 0 {</code></span>
<span class="codeline" id="line-680"><code>		if raceenabled {</code></span>
<span class="codeline" id="line-681"><code>			// The race detector assumes the heap lives in</code></span>
<span class="codeline" id="line-682"><code>			// [0x00c000000000, 0x00e000000000), but we</code></span>
<span class="codeline" id="line-683"><code>			// just ran out of hints in this region. Give</code></span>
<span class="codeline" id="line-684"><code>			// a nice failure.</code></span>
<span class="codeline" id="line-685"><code>			throw("too many address space collisions for -race mode")</code></span>
<span class="codeline" id="line-686"><code>		}</code></span>
<span class="codeline" id="line-687"><code></code></span>
<span class="codeline" id="line-688"><code>		// All of the hints failed, so we'll take any</code></span>
<span class="codeline" id="line-689"><code>		// (sufficiently aligned) address the kernel will give</code></span>
<span class="codeline" id="line-690"><code>		// us.</code></span>
<span class="codeline" id="line-691"><code>		v, size = sysReserveAligned(nil, n, heapArenaBytes)</code></span>
<span class="codeline" id="line-692"><code>		if v == nil {</code></span>
<span class="codeline" id="line-693"><code>			return nil, 0</code></span>
<span class="codeline" id="line-694"><code>		}</code></span>
<span class="codeline" id="line-695"><code></code></span>
<span class="codeline" id="line-696"><code>		// Create new hints for extending this region.</code></span>
<span class="codeline" id="line-697"><code>		hint := (*arenaHint)(h.arenaHintAlloc.alloc())</code></span>
<span class="codeline" id="line-698"><code>		hint.addr, hint.down = uintptr(v), true</code></span>
<span class="codeline" id="line-699"><code>		hint.next, mheap_.arenaHints = mheap_.arenaHints, hint</code></span>
<span class="codeline" id="line-700"><code>		hint = (*arenaHint)(h.arenaHintAlloc.alloc())</code></span>
<span class="codeline" id="line-701"><code>		hint.addr = uintptr(v) + size</code></span>
<span class="codeline" id="line-702"><code>		hint.next, mheap_.arenaHints = mheap_.arenaHints, hint</code></span>
<span class="codeline" id="line-703"><code>	}</code></span>
<span class="codeline" id="line-704"><code></code></span>
<span class="codeline" id="line-705"><code>	// Check for bad pointers or pointers we can't use.</code></span>
<span class="codeline" id="line-706"><code>	{</code></span>
<span class="codeline" id="line-707"><code>		var bad string</code></span>
<span class="codeline" id="line-708"><code>		p := uintptr(v)</code></span>
<span class="codeline" id="line-709"><code>		if p+size &lt; p {</code></span>
<span class="codeline" id="line-710"><code>			bad = "region exceeds uintptr range"</code></span>
<span class="codeline" id="line-711"><code>		} else if arenaIndex(p) &gt;= 1&lt;&lt;arenaBits {</code></span>
<span class="codeline" id="line-712"><code>			bad = "base outside usable address space"</code></span>
<span class="codeline" id="line-713"><code>		} else if arenaIndex(p+size-1) &gt;= 1&lt;&lt;arenaBits {</code></span>
<span class="codeline" id="line-714"><code>			bad = "end outside usable address space"</code></span>
<span class="codeline" id="line-715"><code>		}</code></span>
<span class="codeline" id="line-716"><code>		if bad != "" {</code></span>
<span class="codeline" id="line-717"><code>			// This should be impossible on most architectures,</code></span>
<span class="codeline" id="line-718"><code>			// but it would be really confusing to debug.</code></span>
<span class="codeline" id="line-719"><code>			print("runtime: memory allocated by OS [", hex(p), ", ", hex(p+size), ") not in usable address space: ", bad, "\n")</code></span>
<span class="codeline" id="line-720"><code>			throw("memory reservation exceeds address space limit")</code></span>
<span class="codeline" id="line-721"><code>		}</code></span>
<span class="codeline" id="line-722"><code>	}</code></span>
<span class="codeline" id="line-723"><code></code></span>
<span class="codeline" id="line-724"><code>	if uintptr(v)&amp;(heapArenaBytes-1) != 0 {</code></span>
<span class="codeline" id="line-725"><code>		throw("misrounded allocation in sysAlloc")</code></span>
<span class="codeline" id="line-726"><code>	}</code></span>
<span class="codeline" id="line-727"><code></code></span>
<span class="codeline" id="line-728"><code>	// Transition from Reserved to Prepared.</code></span>
<span class="codeline" id="line-729"><code>	sysMap(v, size, &amp;memstats.heap_sys)</code></span>
<span class="codeline" id="line-730"><code></code></span>
<span class="codeline" id="line-731"><code>mapped:</code></span>
<span class="codeline" id="line-732"><code>	// Create arena metadata.</code></span>
<span class="codeline" id="line-733"><code>	for ri := arenaIndex(uintptr(v)); ri &lt;= arenaIndex(uintptr(v)+size-1); ri++ {</code></span>
<span class="codeline" id="line-734"><code>		l2 := h.arenas[ri.l1()]</code></span>
<span class="codeline" id="line-735"><code>		if l2 == nil {</code></span>
<span class="codeline" id="line-736"><code>			// Allocate an L2 arena map.</code></span>
<span class="codeline" id="line-737"><code>			l2 = (*[1 &lt;&lt; arenaL2Bits]*heapArena)(persistentalloc(unsafe.Sizeof(*l2), sys.PtrSize, nil))</code></span>
<span class="codeline" id="line-738"><code>			if l2 == nil {</code></span>
<span class="codeline" id="line-739"><code>				throw("out of memory allocating heap arena map")</code></span>
<span class="codeline" id="line-740"><code>			}</code></span>
<span class="codeline" id="line-741"><code>			atomic.StorepNoWB(unsafe.Pointer(&amp;h.arenas[ri.l1()]), unsafe.Pointer(l2))</code></span>
<span class="codeline" id="line-742"><code>		}</code></span>
<span class="codeline" id="line-743"><code></code></span>
<span class="codeline" id="line-744"><code>		if l2[ri.l2()] != nil {</code></span>
<span class="codeline" id="line-745"><code>			throw("arena already initialized")</code></span>
<span class="codeline" id="line-746"><code>		}</code></span>
<span class="codeline" id="line-747"><code>		var r *heapArena</code></span>
<span class="codeline" id="line-748"><code>		r = (*heapArena)(h.heapArenaAlloc.alloc(unsafe.Sizeof(*r), sys.PtrSize, &amp;memstats.gcMiscSys))</code></span>
<span class="codeline" id="line-749"><code>		if r == nil {</code></span>
<span class="codeline" id="line-750"><code>			r = (*heapArena)(persistentalloc(unsafe.Sizeof(*r), sys.PtrSize, &amp;memstats.gcMiscSys))</code></span>
<span class="codeline" id="line-751"><code>			if r == nil {</code></span>
<span class="codeline" id="line-752"><code>				throw("out of memory allocating heap arena metadata")</code></span>
<span class="codeline" id="line-753"><code>			}</code></span>
<span class="codeline" id="line-754"><code>		}</code></span>
<span class="codeline" id="line-755"><code></code></span>
<span class="codeline" id="line-756"><code>		// Add the arena to the arenas list.</code></span>
<span class="codeline" id="line-757"><code>		if len(h.allArenas) == cap(h.allArenas) {</code></span>
<span class="codeline" id="line-758"><code>			size := 2 * uintptr(cap(h.allArenas)) * sys.PtrSize</code></span>
<span class="codeline" id="line-759"><code>			if size == 0 {</code></span>
<span class="codeline" id="line-760"><code>				size = physPageSize</code></span>
<span class="codeline" id="line-761"><code>			}</code></span>
<span class="codeline" id="line-762"><code>			newArray := (*notInHeap)(persistentalloc(size, sys.PtrSize, &amp;memstats.gcMiscSys))</code></span>
<span class="codeline" id="line-763"><code>			if newArray == nil {</code></span>
<span class="codeline" id="line-764"><code>				throw("out of memory allocating allArenas")</code></span>
<span class="codeline" id="line-765"><code>			}</code></span>
<span class="codeline" id="line-766"><code>			oldSlice := h.allArenas</code></span>
<span class="codeline" id="line-767"><code>			*(*notInHeapSlice)(unsafe.Pointer(&amp;h.allArenas)) = notInHeapSlice{newArray, len(h.allArenas), int(size / sys.PtrSize)}</code></span>
<span class="codeline" id="line-768"><code>			copy(h.allArenas, oldSlice)</code></span>
<span class="codeline" id="line-769"><code>			// Do not free the old backing array because</code></span>
<span class="codeline" id="line-770"><code>			// there may be concurrent readers. Since we</code></span>
<span class="codeline" id="line-771"><code>			// double the array each time, this can lead</code></span>
<span class="codeline" id="line-772"><code>			// to at most 2x waste.</code></span>
<span class="codeline" id="line-773"><code>		}</code></span>
<span class="codeline" id="line-774"><code>		h.allArenas = h.allArenas[:len(h.allArenas)+1]</code></span>
<span class="codeline" id="line-775"><code>		h.allArenas[len(h.allArenas)-1] = ri</code></span>
<span class="codeline" id="line-776"><code></code></span>
<span class="codeline" id="line-777"><code>		// Store atomically just in case an object from the</code></span>
<span class="codeline" id="line-778"><code>		// new heap arena becomes visible before the heap lock</code></span>
<span class="codeline" id="line-779"><code>		// is released (which shouldn't happen, but there's</code></span>
<span class="codeline" id="line-780"><code>		// little downside to this).</code></span>
<span class="codeline" id="line-781"><code>		atomic.StorepNoWB(unsafe.Pointer(&amp;l2[ri.l2()]), unsafe.Pointer(r))</code></span>
<span class="codeline" id="line-782"><code>	}</code></span>
<span class="codeline" id="line-783"><code></code></span>
<span class="codeline" id="line-784"><code>	// Tell the race detector about the new heap memory.</code></span>
<span class="codeline" id="line-785"><code>	if raceenabled {</code></span>
<span class="codeline" id="line-786"><code>		racemapshadow(v, size)</code></span>
<span class="codeline" id="line-787"><code>	}</code></span>
<span class="codeline" id="line-788"><code></code></span>
<span class="codeline" id="line-789"><code>	return</code></span>
<span class="codeline" id="line-790"><code>}</code></span>
<span class="codeline" id="line-791"><code></code></span>
<span class="codeline" id="line-792"><code>// sysReserveAligned is like sysReserve, but the returned pointer is</code></span>
<span class="codeline" id="line-793"><code>// aligned to align bytes. It may reserve either n or n+align bytes,</code></span>
<span class="codeline" id="line-794"><code>// so it returns the size that was reserved.</code></span>
<span class="codeline" id="line-795"><code>func sysReserveAligned(v unsafe.Pointer, size, align uintptr) (unsafe.Pointer, uintptr) {</code></span>
<span class="codeline" id="line-796"><code>	// Since the alignment is rather large in uses of this</code></span>
<span class="codeline" id="line-797"><code>	// function, we're not likely to get it by chance, so we ask</code></span>
<span class="codeline" id="line-798"><code>	// for a larger region and remove the parts we don't need.</code></span>
<span class="codeline" id="line-799"><code>	retries := 0</code></span>
<span class="codeline" id="line-800"><code>retry:</code></span>
<span class="codeline" id="line-801"><code>	p := uintptr(sysReserve(v, size+align))</code></span>
<span class="codeline" id="line-802"><code>	switch {</code></span>
<span class="codeline" id="line-803"><code>	case p == 0:</code></span>
<span class="codeline" id="line-804"><code>		return nil, 0</code></span>
<span class="codeline" id="line-805"><code>	case p&amp;(align-1) == 0:</code></span>
<span class="codeline" id="line-806"><code>		// We got lucky and got an aligned region, so we can</code></span>
<span class="codeline" id="line-807"><code>		// use the whole thing.</code></span>
<span class="codeline" id="line-808"><code>		return unsafe.Pointer(p), size + align</code></span>
<span class="codeline" id="line-809"><code>	case GOOS == "windows":</code></span>
<span class="codeline" id="line-810"><code>		// On Windows we can't release pieces of a</code></span>
<span class="codeline" id="line-811"><code>		// reservation, so we release the whole thing and</code></span>
<span class="codeline" id="line-812"><code>		// re-reserve the aligned sub-region. This may race,</code></span>
<span class="codeline" id="line-813"><code>		// so we may have to try again.</code></span>
<span class="codeline" id="line-814"><code>		sysFree(unsafe.Pointer(p), size+align, nil)</code></span>
<span class="codeline" id="line-815"><code>		p = alignUp(p, align)</code></span>
<span class="codeline" id="line-816"><code>		p2 := sysReserve(unsafe.Pointer(p), size)</code></span>
<span class="codeline" id="line-817"><code>		if p != uintptr(p2) {</code></span>
<span class="codeline" id="line-818"><code>			// Must have raced. Try again.</code></span>
<span class="codeline" id="line-819"><code>			sysFree(p2, size, nil)</code></span>
<span class="codeline" id="line-820"><code>			if retries++; retries == 100 {</code></span>
<span class="codeline" id="line-821"><code>				throw("failed to allocate aligned heap memory; too many retries")</code></span>
<span class="codeline" id="line-822"><code>			}</code></span>
<span class="codeline" id="line-823"><code>			goto retry</code></span>
<span class="codeline" id="line-824"><code>		}</code></span>
<span class="codeline" id="line-825"><code>		// Success.</code></span>
<span class="codeline" id="line-826"><code>		return p2, size</code></span>
<span class="codeline" id="line-827"><code>	default:</code></span>
<span class="codeline" id="line-828"><code>		// Trim off the unaligned parts.</code></span>
<span class="codeline" id="line-829"><code>		pAligned := alignUp(p, align)</code></span>
<span class="codeline" id="line-830"><code>		sysFree(unsafe.Pointer(p), pAligned-p, nil)</code></span>
<span class="codeline" id="line-831"><code>		end := pAligned + size</code></span>
<span class="codeline" id="line-832"><code>		endLen := (p + size + align) - end</code></span>
<span class="codeline" id="line-833"><code>		if endLen &gt; 0 {</code></span>
<span class="codeline" id="line-834"><code>			sysFree(unsafe.Pointer(end), endLen, nil)</code></span>
<span class="codeline" id="line-835"><code>		}</code></span>
<span class="codeline" id="line-836"><code>		return unsafe.Pointer(pAligned), size</code></span>
<span class="codeline" id="line-837"><code>	}</code></span>
<span class="codeline" id="line-838"><code>}</code></span>
<span class="codeline" id="line-839"><code></code></span>
<span class="codeline" id="line-840"><code>// base address for all 0-byte allocations</code></span>
<span class="codeline" id="line-841"><code>var zerobase uintptr</code></span>
<span class="codeline" id="line-842"><code></code></span>
<span class="codeline" id="line-843"><code>// nextFreeFast returns the next free object if one is quickly available.</code></span>
<span class="codeline" id="line-844"><code>// Otherwise it returns 0.</code></span>
<span class="codeline" id="line-845"><code>func nextFreeFast(s *mspan) gclinkptr {</code></span>
<span class="codeline" id="line-846"><code>	theBit := sys.Ctz64(s.allocCache) // Is there a free object in the allocCache?</code></span>
<span class="codeline" id="line-847"><code>	if theBit &lt; 64 {</code></span>
<span class="codeline" id="line-848"><code>		result := s.freeindex + uintptr(theBit)</code></span>
<span class="codeline" id="line-849"><code>		if result &lt; s.nelems {</code></span>
<span class="codeline" id="line-850"><code>			freeidx := result + 1</code></span>
<span class="codeline" id="line-851"><code>			if freeidx%64 == 0 &amp;&amp; freeidx != s.nelems {</code></span>
<span class="codeline" id="line-852"><code>				return 0</code></span>
<span class="codeline" id="line-853"><code>			}</code></span>
<span class="codeline" id="line-854"><code>			s.allocCache &gt;&gt;= uint(theBit + 1)</code></span>
<span class="codeline" id="line-855"><code>			s.freeindex = freeidx</code></span>
<span class="codeline" id="line-856"><code>			s.allocCount++</code></span>
<span class="codeline" id="line-857"><code>			return gclinkptr(result*s.elemsize + s.base())</code></span>
<span class="codeline" id="line-858"><code>		}</code></span>
<span class="codeline" id="line-859"><code>	}</code></span>
<span class="codeline" id="line-860"><code>	return 0</code></span>
<span class="codeline" id="line-861"><code>}</code></span>
<span class="codeline" id="line-862"><code></code></span>
<span class="codeline" id="line-863"><code>// nextFree returns the next free object from the cached span if one is available.</code></span>
<span class="codeline" id="line-864"><code>// Otherwise it refills the cache with a span with an available object and</code></span>
<span class="codeline" id="line-865"><code>// returns that object along with a flag indicating that this was a heavy</code></span>
<span class="codeline" id="line-866"><code>// weight allocation. If it is a heavy weight allocation the caller must</code></span>
<span class="codeline" id="line-867"><code>// determine whether a new GC cycle needs to be started or if the GC is active</code></span>
<span class="codeline" id="line-868"><code>// whether this goroutine needs to assist the GC.</code></span>
<span class="codeline" id="line-869"><code>//</code></span>
<span class="codeline" id="line-870"><code>// Must run in a non-preemptible context since otherwise the owner of</code></span>
<span class="codeline" id="line-871"><code>// c could change.</code></span>
<span class="codeline" id="line-872"><code>func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool) {</code></span>
<span class="codeline" id="line-873"><code>	s = c.alloc[spc]</code></span>
<span class="codeline" id="line-874"><code>	shouldhelpgc = false</code></span>
<span class="codeline" id="line-875"><code>	freeIndex := s.nextFreeIndex()</code></span>
<span class="codeline" id="line-876"><code>	if freeIndex == s.nelems {</code></span>
<span class="codeline" id="line-877"><code>		// The span is full.</code></span>
<span class="codeline" id="line-878"><code>		if uintptr(s.allocCount) != s.nelems {</code></span>
<span class="codeline" id="line-879"><code>			println("runtime: s.allocCount=", s.allocCount, "s.nelems=", s.nelems)</code></span>
<span class="codeline" id="line-880"><code>			throw("s.allocCount != s.nelems &amp;&amp; freeIndex == s.nelems")</code></span>
<span class="codeline" id="line-881"><code>		}</code></span>
<span class="codeline" id="line-882"><code>		c.refill(spc)</code></span>
<span class="codeline" id="line-883"><code>		shouldhelpgc = true</code></span>
<span class="codeline" id="line-884"><code>		s = c.alloc[spc]</code></span>
<span class="codeline" id="line-885"><code></code></span>
<span class="codeline" id="line-886"><code>		freeIndex = s.nextFreeIndex()</code></span>
<span class="codeline" id="line-887"><code>	}</code></span>
<span class="codeline" id="line-888"><code></code></span>
<span class="codeline" id="line-889"><code>	if freeIndex &gt;= s.nelems {</code></span>
<span class="codeline" id="line-890"><code>		throw("freeIndex is not valid")</code></span>
<span class="codeline" id="line-891"><code>	}</code></span>
<span class="codeline" id="line-892"><code></code></span>
<span class="codeline" id="line-893"><code>	v = gclinkptr(freeIndex*s.elemsize + s.base())</code></span>
<span class="codeline" id="line-894"><code>	s.allocCount++</code></span>
<span class="codeline" id="line-895"><code>	if uintptr(s.allocCount) &gt; s.nelems {</code></span>
<span class="codeline" id="line-896"><code>		println("s.allocCount=", s.allocCount, "s.nelems=", s.nelems)</code></span>
<span class="codeline" id="line-897"><code>		throw("s.allocCount &gt; s.nelems")</code></span>
<span class="codeline" id="line-898"><code>	}</code></span>
<span class="codeline" id="line-899"><code>	return</code></span>
<span class="codeline" id="line-900"><code>}</code></span>
<span class="codeline" id="line-901"><code></code></span>
<span class="codeline" id="line-902"><code>// Allocate an object of size bytes.</code></span>
<span class="codeline" id="line-903"><code>// Small objects are allocated from the per-P cache's free lists.</code></span>
<span class="codeline" id="line-904"><code>// Large objects (&gt; 32 kB) are allocated straight from the heap.</code></span>
<span class="codeline" id="line-905"><code>func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {</code></span>
<span class="codeline" id="line-906"><code>	if gcphase == _GCmarktermination {</code></span>
<span class="codeline" id="line-907"><code>		throw("mallocgc called with gcphase == _GCmarktermination")</code></span>
<span class="codeline" id="line-908"><code>	}</code></span>
<span class="codeline" id="line-909"><code></code></span>
<span class="codeline" id="line-910"><code>	if size == 0 {</code></span>
<span class="codeline" id="line-911"><code>		return unsafe.Pointer(&amp;zerobase)</code></span>
<span class="codeline" id="line-912"><code>	}</code></span>
<span class="codeline" id="line-913"><code></code></span>
<span class="codeline" id="line-914"><code>	if debug.malloc {</code></span>
<span class="codeline" id="line-915"><code>		if debug.sbrk != 0 {</code></span>
<span class="codeline" id="line-916"><code>			align := uintptr(16)</code></span>
<span class="codeline" id="line-917"><code>			if typ != nil {</code></span>
<span class="codeline" id="line-918"><code>				// TODO(austin): This should be just</code></span>
<span class="codeline" id="line-919"><code>				//   align = uintptr(typ.align)</code></span>
<span class="codeline" id="line-920"><code>				// but that's only 4 on 32-bit platforms,</code></span>
<span class="codeline" id="line-921"><code>				// even if there's a uint64 field in typ (see #599).</code></span>
<span class="codeline" id="line-922"><code>				// This causes 64-bit atomic accesses to panic.</code></span>
<span class="codeline" id="line-923"><code>				// Hence, we use stricter alignment that matches</code></span>
<span class="codeline" id="line-924"><code>				// the normal allocator better.</code></span>
<span class="codeline" id="line-925"><code>				if size&amp;7 == 0 {</code></span>
<span class="codeline" id="line-926"><code>					align = 8</code></span>
<span class="codeline" id="line-927"><code>				} else if size&amp;3 == 0 {</code></span>
<span class="codeline" id="line-928"><code>					align = 4</code></span>
<span class="codeline" id="line-929"><code>				} else if size&amp;1 == 0 {</code></span>
<span class="codeline" id="line-930"><code>					align = 2</code></span>
<span class="codeline" id="line-931"><code>				} else {</code></span>
<span class="codeline" id="line-932"><code>					align = 1</code></span>
<span class="codeline" id="line-933"><code>				}</code></span>
<span class="codeline" id="line-934"><code>			}</code></span>
<span class="codeline" id="line-935"><code>			return persistentalloc(size, align, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-936"><code>		}</code></span>
<span class="codeline" id="line-937"><code></code></span>
<span class="codeline" id="line-938"><code>		if inittrace.active &amp;&amp; inittrace.id == getg().goid {</code></span>
<span class="codeline" id="line-939"><code>			// Init functions are executed sequentially in a single Go routine.</code></span>
<span class="codeline" id="line-940"><code>			inittrace.allocs += 1</code></span>
<span class="codeline" id="line-941"><code>		}</code></span>
<span class="codeline" id="line-942"><code>	}</code></span>
<span class="codeline" id="line-943"><code></code></span>
<span class="codeline" id="line-944"><code>	// assistG is the G to charge for this allocation, or nil if</code></span>
<span class="codeline" id="line-945"><code>	// GC is not currently active.</code></span>
<span class="codeline" id="line-946"><code>	var assistG *g</code></span>
<span class="codeline" id="line-947"><code>	if gcBlackenEnabled != 0 {</code></span>
<span class="codeline" id="line-948"><code>		// Charge the current user G for this allocation.</code></span>
<span class="codeline" id="line-949"><code>		assistG = getg()</code></span>
<span class="codeline" id="line-950"><code>		if assistG.m.curg != nil {</code></span>
<span class="codeline" id="line-951"><code>			assistG = assistG.m.curg</code></span>
<span class="codeline" id="line-952"><code>		}</code></span>
<span class="codeline" id="line-953"><code>		// Charge the allocation against the G. We'll account</code></span>
<span class="codeline" id="line-954"><code>		// for internal fragmentation at the end of mallocgc.</code></span>
<span class="codeline" id="line-955"><code>		assistG.gcAssistBytes -= int64(size)</code></span>
<span class="codeline" id="line-956"><code></code></span>
<span class="codeline" id="line-957"><code>		if assistG.gcAssistBytes &lt; 0 {</code></span>
<span class="codeline" id="line-958"><code>			// This G is in debt. Assist the GC to correct</code></span>
<span class="codeline" id="line-959"><code>			// this before allocating. This must happen</code></span>
<span class="codeline" id="line-960"><code>			// before disabling preemption.</code></span>
<span class="codeline" id="line-961"><code>			gcAssistAlloc(assistG)</code></span>
<span class="codeline" id="line-962"><code>		}</code></span>
<span class="codeline" id="line-963"><code>	}</code></span>
<span class="codeline" id="line-964"><code></code></span>
<span class="codeline" id="line-965"><code>	// Set mp.mallocing to keep from being preempted by GC.</code></span>
<span class="codeline" id="line-966"><code>	mp := acquirem()</code></span>
<span class="codeline" id="line-967"><code>	if mp.mallocing != 0 {</code></span>
<span class="codeline" id="line-968"><code>		throw("malloc deadlock")</code></span>
<span class="codeline" id="line-969"><code>	}</code></span>
<span class="codeline" id="line-970"><code>	if mp.gsignal == getg() {</code></span>
<span class="codeline" id="line-971"><code>		throw("malloc during signal")</code></span>
<span class="codeline" id="line-972"><code>	}</code></span>
<span class="codeline" id="line-973"><code>	mp.mallocing = 1</code></span>
<span class="codeline" id="line-974"><code></code></span>
<span class="codeline" id="line-975"><code>	shouldhelpgc := false</code></span>
<span class="codeline" id="line-976"><code>	dataSize := size</code></span>
<span class="codeline" id="line-977"><code>	c := getMCache()</code></span>
<span class="codeline" id="line-978"><code>	if c == nil {</code></span>
<span class="codeline" id="line-979"><code>		throw("mallocgc called without a P or outside bootstrapping")</code></span>
<span class="codeline" id="line-980"><code>	}</code></span>
<span class="codeline" id="line-981"><code>	var span *mspan</code></span>
<span class="codeline" id="line-982"><code>	var x unsafe.Pointer</code></span>
<span class="codeline" id="line-983"><code>	noscan := typ == nil || typ.ptrdata == 0</code></span>
<span class="codeline" id="line-984"><code>	if size &lt;= maxSmallSize {</code></span>
<span class="codeline" id="line-985"><code>		if noscan &amp;&amp; size &lt; maxTinySize {</code></span>
<span class="codeline" id="line-986"><code>			// Tiny allocator.</code></span>
<span class="codeline" id="line-987"><code>			//</code></span>
<span class="codeline" id="line-988"><code>			// Tiny allocator combines several tiny allocation requests</code></span>
<span class="codeline" id="line-989"><code>			// into a single memory block. The resulting memory block</code></span>
<span class="codeline" id="line-990"><code>			// is freed when all subobjects are unreachable. The subobjects</code></span>
<span class="codeline" id="line-991"><code>			// must be noscan (don't have pointers), this ensures that</code></span>
<span class="codeline" id="line-992"><code>			// the amount of potentially wasted memory is bounded.</code></span>
<span class="codeline" id="line-993"><code>			//</code></span>
<span class="codeline" id="line-994"><code>			// Size of the memory block used for combining (maxTinySize) is tunable.</code></span>
<span class="codeline" id="line-995"><code>			// Current setting is 16 bytes, which relates to 2x worst case memory</code></span>
<span class="codeline" id="line-996"><code>			// wastage (when all but one subobjects are unreachable).</code></span>
<span class="codeline" id="line-997"><code>			// 8 bytes would result in no wastage at all, but provides less</code></span>
<span class="codeline" id="line-998"><code>			// opportunities for combining.</code></span>
<span class="codeline" id="line-999"><code>			// 32 bytes provides more opportunities for combining,</code></span>
<span class="codeline" id="line-1000"><code>			// but can lead to 4x worst case wastage.</code></span>
<span class="codeline" id="line-1001"><code>			// The best case winning is 8x regardless of block size.</code></span>
<span class="codeline" id="line-1002"><code>			//</code></span>
<span class="codeline" id="line-1003"><code>			// Objects obtained from tiny allocator must not be freed explicitly.</code></span>
<span class="codeline" id="line-1004"><code>			// So when an object will be freed explicitly, we ensure that</code></span>
<span class="codeline" id="line-1005"><code>			// its size &gt;= maxTinySize.</code></span>
<span class="codeline" id="line-1006"><code>			//</code></span>
<span class="codeline" id="line-1007"><code>			// SetFinalizer has a special case for objects potentially coming</code></span>
<span class="codeline" id="line-1008"><code>			// from tiny allocator, it such case it allows to set finalizers</code></span>
<span class="codeline" id="line-1009"><code>			// for an inner byte of a memory block.</code></span>
<span class="codeline" id="line-1010"><code>			//</code></span>
<span class="codeline" id="line-1011"><code>			// The main targets of tiny allocator are small strings and</code></span>
<span class="codeline" id="line-1012"><code>			// standalone escaping variables. On a json benchmark</code></span>
<span class="codeline" id="line-1013"><code>			// the allocator reduces number of allocations by ~12% and</code></span>
<span class="codeline" id="line-1014"><code>			// reduces heap size by ~20%.</code></span>
<span class="codeline" id="line-1015"><code>			off := c.tinyoffset</code></span>
<span class="codeline" id="line-1016"><code>			// Align tiny pointer for required (conservative) alignment.</code></span>
<span class="codeline" id="line-1017"><code>			if size&amp;7 == 0 {</code></span>
<span class="codeline" id="line-1018"><code>				off = alignUp(off, 8)</code></span>
<span class="codeline" id="line-1019"><code>			} else if sys.PtrSize == 4 &amp;&amp; size == 12 {</code></span>
<span class="codeline" id="line-1020"><code>				// Conservatively align 12-byte objects to 8 bytes on 32-bit</code></span>
<span class="codeline" id="line-1021"><code>				// systems so that objects whose first field is a 64-bit</code></span>
<span class="codeline" id="line-1022"><code>				// value is aligned to 8 bytes and does not cause a fault on</code></span>
<span class="codeline" id="line-1023"><code>				// atomic access. See issue 37262.</code></span>
<span class="codeline" id="line-1024"><code>				// TODO(mknyszek): Remove this workaround if/when issue 36606</code></span>
<span class="codeline" id="line-1025"><code>				// is resolved.</code></span>
<span class="codeline" id="line-1026"><code>				off = alignUp(off, 8)</code></span>
<span class="codeline" id="line-1027"><code>			} else if size&amp;3 == 0 {</code></span>
<span class="codeline" id="line-1028"><code>				off = alignUp(off, 4)</code></span>
<span class="codeline" id="line-1029"><code>			} else if size&amp;1 == 0 {</code></span>
<span class="codeline" id="line-1030"><code>				off = alignUp(off, 2)</code></span>
<span class="codeline" id="line-1031"><code>			}</code></span>
<span class="codeline" id="line-1032"><code>			if off+size &lt;= maxTinySize &amp;&amp; c.tiny != 0 {</code></span>
<span class="codeline" id="line-1033"><code>				// The object fits into existing tiny block.</code></span>
<span class="codeline" id="line-1034"><code>				x = unsafe.Pointer(c.tiny + off)</code></span>
<span class="codeline" id="line-1035"><code>				c.tinyoffset = off + size</code></span>
<span class="codeline" id="line-1036"><code>				c.tinyAllocs++</code></span>
<span class="codeline" id="line-1037"><code>				mp.mallocing = 0</code></span>
<span class="codeline" id="line-1038"><code>				releasem(mp)</code></span>
<span class="codeline" id="line-1039"><code>				return x</code></span>
<span class="codeline" id="line-1040"><code>			}</code></span>
<span class="codeline" id="line-1041"><code>			// Allocate a new maxTinySize block.</code></span>
<span class="codeline" id="line-1042"><code>			span = c.alloc[tinySpanClass]</code></span>
<span class="codeline" id="line-1043"><code>			v := nextFreeFast(span)</code></span>
<span class="codeline" id="line-1044"><code>			if v == 0 {</code></span>
<span class="codeline" id="line-1045"><code>				v, span, shouldhelpgc = c.nextFree(tinySpanClass)</code></span>
<span class="codeline" id="line-1046"><code>			}</code></span>
<span class="codeline" id="line-1047"><code>			x = unsafe.Pointer(v)</code></span>
<span class="codeline" id="line-1048"><code>			(*[2]uint64)(x)[0] = 0</code></span>
<span class="codeline" id="line-1049"><code>			(*[2]uint64)(x)[1] = 0</code></span>
<span class="codeline" id="line-1050"><code>			// See if we need to replace the existing tiny block with the new one</code></span>
<span class="codeline" id="line-1051"><code>			// based on amount of remaining free space.</code></span>
<span class="codeline" id="line-1052"><code>			if size &lt; c.tinyoffset || c.tiny == 0 {</code></span>
<span class="codeline" id="line-1053"><code>				c.tiny = uintptr(x)</code></span>
<span class="codeline" id="line-1054"><code>				c.tinyoffset = size</code></span>
<span class="codeline" id="line-1055"><code>			}</code></span>
<span class="codeline" id="line-1056"><code>			size = maxTinySize</code></span>
<span class="codeline" id="line-1057"><code>		} else {</code></span>
<span class="codeline" id="line-1058"><code>			var sizeclass uint8</code></span>
<span class="codeline" id="line-1059"><code>			if size &lt;= smallSizeMax-8 {</code></span>
<span class="codeline" id="line-1060"><code>				sizeclass = size_to_class8[divRoundUp(size, smallSizeDiv)]</code></span>
<span class="codeline" id="line-1061"><code>			} else {</code></span>
<span class="codeline" id="line-1062"><code>				sizeclass = size_to_class128[divRoundUp(size-smallSizeMax, largeSizeDiv)]</code></span>
<span class="codeline" id="line-1063"><code>			}</code></span>
<span class="codeline" id="line-1064"><code>			size = uintptr(class_to_size[sizeclass])</code></span>
<span class="codeline" id="line-1065"><code>			spc := makeSpanClass(sizeclass, noscan)</code></span>
<span class="codeline" id="line-1066"><code>			span = c.alloc[spc]</code></span>
<span class="codeline" id="line-1067"><code>			v := nextFreeFast(span)</code></span>
<span class="codeline" id="line-1068"><code>			if v == 0 {</code></span>
<span class="codeline" id="line-1069"><code>				v, span, shouldhelpgc = c.nextFree(spc)</code></span>
<span class="codeline" id="line-1070"><code>			}</code></span>
<span class="codeline" id="line-1071"><code>			x = unsafe.Pointer(v)</code></span>
<span class="codeline" id="line-1072"><code>			if needzero &amp;&amp; span.needzero != 0 {</code></span>
<span class="codeline" id="line-1073"><code>				memclrNoHeapPointers(unsafe.Pointer(v), size)</code></span>
<span class="codeline" id="line-1074"><code>			}</code></span>
<span class="codeline" id="line-1075"><code>		}</code></span>
<span class="codeline" id="line-1076"><code>	} else {</code></span>
<span class="codeline" id="line-1077"><code>		shouldhelpgc = true</code></span>
<span class="codeline" id="line-1078"><code>		span = c.allocLarge(size, needzero, noscan)</code></span>
<span class="codeline" id="line-1079"><code>		span.freeindex = 1</code></span>
<span class="codeline" id="line-1080"><code>		span.allocCount = 1</code></span>
<span class="codeline" id="line-1081"><code>		x = unsafe.Pointer(span.base())</code></span>
<span class="codeline" id="line-1082"><code>		size = span.elemsize</code></span>
<span class="codeline" id="line-1083"><code>	}</code></span>
<span class="codeline" id="line-1084"><code></code></span>
<span class="codeline" id="line-1085"><code>	var scanSize uintptr</code></span>
<span class="codeline" id="line-1086"><code>	if !noscan {</code></span>
<span class="codeline" id="line-1087"><code>		// If allocating a defer+arg block, now that we've picked a malloc size</code></span>
<span class="codeline" id="line-1088"><code>		// large enough to hold everything, cut the "asked for" size down to</code></span>
<span class="codeline" id="line-1089"><code>		// just the defer header, so that the GC bitmap will record the arg block</code></span>
<span class="codeline" id="line-1090"><code>		// as containing nothing at all (as if it were unused space at the end of</code></span>
<span class="codeline" id="line-1091"><code>		// a malloc block caused by size rounding).</code></span>
<span class="codeline" id="line-1092"><code>		// The defer arg areas are scanned as part of scanstack.</code></span>
<span class="codeline" id="line-1093"><code>		if typ == deferType {</code></span>
<span class="codeline" id="line-1094"><code>			dataSize = unsafe.Sizeof(_defer{})</code></span>
<span class="codeline" id="line-1095"><code>		}</code></span>
<span class="codeline" id="line-1096"><code>		heapBitsSetType(uintptr(x), size, dataSize, typ)</code></span>
<span class="codeline" id="line-1097"><code>		if dataSize &gt; typ.size {</code></span>
<span class="codeline" id="line-1098"><code>			// Array allocation. If there are any</code></span>
<span class="codeline" id="line-1099"><code>			// pointers, GC has to scan to the last</code></span>
<span class="codeline" id="line-1100"><code>			// element.</code></span>
<span class="codeline" id="line-1101"><code>			if typ.ptrdata != 0 {</code></span>
<span class="codeline" id="line-1102"><code>				scanSize = dataSize - typ.size + typ.ptrdata</code></span>
<span class="codeline" id="line-1103"><code>			}</code></span>
<span class="codeline" id="line-1104"><code>		} else {</code></span>
<span class="codeline" id="line-1105"><code>			scanSize = typ.ptrdata</code></span>
<span class="codeline" id="line-1106"><code>		}</code></span>
<span class="codeline" id="line-1107"><code>		c.scanAlloc += scanSize</code></span>
<span class="codeline" id="line-1108"><code>	}</code></span>
<span class="codeline" id="line-1109"><code></code></span>
<span class="codeline" id="line-1110"><code>	// Ensure that the stores above that initialize x to</code></span>
<span class="codeline" id="line-1111"><code>	// type-safe memory and set the heap bits occur before</code></span>
<span class="codeline" id="line-1112"><code>	// the caller can make x observable to the garbage</code></span>
<span class="codeline" id="line-1113"><code>	// collector. Otherwise, on weakly ordered machines,</code></span>
<span class="codeline" id="line-1114"><code>	// the garbage collector could follow a pointer to x,</code></span>
<span class="codeline" id="line-1115"><code>	// but see uninitialized memory or stale heap bits.</code></span>
<span class="codeline" id="line-1116"><code>	publicationBarrier()</code></span>
<span class="codeline" id="line-1117"><code></code></span>
<span class="codeline" id="line-1118"><code>	// Allocate black during GC.</code></span>
<span class="codeline" id="line-1119"><code>	// All slots hold nil so no scanning is needed.</code></span>
<span class="codeline" id="line-1120"><code>	// This may be racing with GC so do it atomically if there can be</code></span>
<span class="codeline" id="line-1121"><code>	// a race marking the bit.</code></span>
<span class="codeline" id="line-1122"><code>	if gcphase != _GCoff {</code></span>
<span class="codeline" id="line-1123"><code>		gcmarknewobject(span, uintptr(x), size, scanSize)</code></span>
<span class="codeline" id="line-1124"><code>	}</code></span>
<span class="codeline" id="line-1125"><code></code></span>
<span class="codeline" id="line-1126"><code>	if raceenabled {</code></span>
<span class="codeline" id="line-1127"><code>		racemalloc(x, size)</code></span>
<span class="codeline" id="line-1128"><code>	}</code></span>
<span class="codeline" id="line-1129"><code></code></span>
<span class="codeline" id="line-1130"><code>	if msanenabled {</code></span>
<span class="codeline" id="line-1131"><code>		msanmalloc(x, size)</code></span>
<span class="codeline" id="line-1132"><code>	}</code></span>
<span class="codeline" id="line-1133"><code></code></span>
<span class="codeline" id="line-1134"><code>	mp.mallocing = 0</code></span>
<span class="codeline" id="line-1135"><code>	releasem(mp)</code></span>
<span class="codeline" id="line-1136"><code></code></span>
<span class="codeline" id="line-1137"><code>	if debug.malloc {</code></span>
<span class="codeline" id="line-1138"><code>		if debug.allocfreetrace != 0 {</code></span>
<span class="codeline" id="line-1139"><code>			tracealloc(x, size, typ)</code></span>
<span class="codeline" id="line-1140"><code>		}</code></span>
<span class="codeline" id="line-1141"><code></code></span>
<span class="codeline" id="line-1142"><code>		if inittrace.active &amp;&amp; inittrace.id == getg().goid {</code></span>
<span class="codeline" id="line-1143"><code>			// Init functions are executed sequentially in a single Go routine.</code></span>
<span class="codeline" id="line-1144"><code>			inittrace.bytes += uint64(size)</code></span>
<span class="codeline" id="line-1145"><code>		}</code></span>
<span class="codeline" id="line-1146"><code>	}</code></span>
<span class="codeline" id="line-1147"><code></code></span>
<span class="codeline" id="line-1148"><code>	if rate := MemProfileRate; rate &gt; 0 {</code></span>
<span class="codeline" id="line-1149"><code>		if rate != 1 &amp;&amp; size &lt; c.nextSample {</code></span>
<span class="codeline" id="line-1150"><code>			c.nextSample -= size</code></span>
<span class="codeline" id="line-1151"><code>		} else {</code></span>
<span class="codeline" id="line-1152"><code>			mp := acquirem()</code></span>
<span class="codeline" id="line-1153"><code>			profilealloc(mp, x, size)</code></span>
<span class="codeline" id="line-1154"><code>			releasem(mp)</code></span>
<span class="codeline" id="line-1155"><code>		}</code></span>
<span class="codeline" id="line-1156"><code>	}</code></span>
<span class="codeline" id="line-1157"><code></code></span>
<span class="codeline" id="line-1158"><code>	if assistG != nil {</code></span>
<span class="codeline" id="line-1159"><code>		// Account for internal fragmentation in the assist</code></span>
<span class="codeline" id="line-1160"><code>		// debt now that we know it.</code></span>
<span class="codeline" id="line-1161"><code>		assistG.gcAssistBytes -= int64(size - dataSize)</code></span>
<span class="codeline" id="line-1162"><code>	}</code></span>
<span class="codeline" id="line-1163"><code></code></span>
<span class="codeline" id="line-1164"><code>	if shouldhelpgc {</code></span>
<span class="codeline" id="line-1165"><code>		if t := (gcTrigger{kind: gcTriggerHeap}); t.test() {</code></span>
<span class="codeline" id="line-1166"><code>			gcStart(t)</code></span>
<span class="codeline" id="line-1167"><code>		}</code></span>
<span class="codeline" id="line-1168"><code>	}</code></span>
<span class="codeline" id="line-1169"><code></code></span>
<span class="codeline" id="line-1170"><code>	return x</code></span>
<span class="codeline" id="line-1171"><code>}</code></span>
<span class="codeline" id="line-1172"><code></code></span>
<span class="codeline" id="line-1173"><code>// implementation of new builtin</code></span>
<span class="codeline" id="line-1174"><code>// compiler (both frontend and SSA backend) knows the signature</code></span>
<span class="codeline" id="line-1175"><code>// of this function</code></span>
<span class="codeline" id="line-1176"><code>func newobject(typ *_type) unsafe.Pointer {</code></span>
<span class="codeline" id="line-1177"><code>	return mallocgc(typ.size, typ, true)</code></span>
<span class="codeline" id="line-1178"><code>}</code></span>
<span class="codeline" id="line-1179"><code></code></span>
<span class="codeline" id="line-1180"><code>//go:linkname reflect_unsafe_New reflect.unsafe_New</code></span>
<span class="codeline" id="line-1181"><code>func reflect_unsafe_New(typ *_type) unsafe.Pointer {</code></span>
<span class="codeline" id="line-1182"><code>	return mallocgc(typ.size, typ, true)</code></span>
<span class="codeline" id="line-1183"><code>}</code></span>
<span class="codeline" id="line-1184"><code></code></span>
<span class="codeline" id="line-1185"><code>//go:linkname reflectlite_unsafe_New internal/reflectlite.unsafe_New</code></span>
<span class="codeline" id="line-1186"><code>func reflectlite_unsafe_New(typ *_type) unsafe.Pointer {</code></span>
<span class="codeline" id="line-1187"><code>	return mallocgc(typ.size, typ, true)</code></span>
<span class="codeline" id="line-1188"><code>}</code></span>
<span class="codeline" id="line-1189"><code></code></span>
<span class="codeline" id="line-1190"><code>// newarray allocates an array of n elements of type typ.</code></span>
<span class="codeline" id="line-1191"><code>func newarray(typ *_type, n int) unsafe.Pointer {</code></span>
<span class="codeline" id="line-1192"><code>	if n == 1 {</code></span>
<span class="codeline" id="line-1193"><code>		return mallocgc(typ.size, typ, true)</code></span>
<span class="codeline" id="line-1194"><code>	}</code></span>
<span class="codeline" id="line-1195"><code>	mem, overflow := math.MulUintptr(typ.size, uintptr(n))</code></span>
<span class="codeline" id="line-1196"><code>	if overflow || mem &gt; maxAlloc || n &lt; 0 {</code></span>
<span class="codeline" id="line-1197"><code>		panic(plainError("runtime: allocation size out of range"))</code></span>
<span class="codeline" id="line-1198"><code>	}</code></span>
<span class="codeline" id="line-1199"><code>	return mallocgc(mem, typ, true)</code></span>
<span class="codeline" id="line-1200"><code>}</code></span>
<span class="codeline" id="line-1201"><code></code></span>
<span class="codeline" id="line-1202"><code>//go:linkname reflect_unsafe_NewArray reflect.unsafe_NewArray</code></span>
<span class="codeline" id="line-1203"><code>func reflect_unsafe_NewArray(typ *_type, n int) unsafe.Pointer {</code></span>
<span class="codeline" id="line-1204"><code>	return newarray(typ, n)</code></span>
<span class="codeline" id="line-1205"><code>}</code></span>
<span class="codeline" id="line-1206"><code></code></span>
<span class="codeline" id="line-1207"><code>func profilealloc(mp *m, x unsafe.Pointer, size uintptr) {</code></span>
<span class="codeline" id="line-1208"><code>	c := getMCache()</code></span>
<span class="codeline" id="line-1209"><code>	if c == nil {</code></span>
<span class="codeline" id="line-1210"><code>		throw("profilealloc called without a P or outside bootstrapping")</code></span>
<span class="codeline" id="line-1211"><code>	}</code></span>
<span class="codeline" id="line-1212"><code>	c.nextSample = nextSample()</code></span>
<span class="codeline" id="line-1213"><code>	mProf_Malloc(x, size)</code></span>
<span class="codeline" id="line-1214"><code>}</code></span>
<span class="codeline" id="line-1215"><code></code></span>
<span class="codeline" id="line-1216"><code>// nextSample returns the next sampling point for heap profiling. The goal is</code></span>
<span class="codeline" id="line-1217"><code>// to sample allocations on average every MemProfileRate bytes, but with a</code></span>
<span class="codeline" id="line-1218"><code>// completely random distribution over the allocation timeline; this</code></span>
<span class="codeline" id="line-1219"><code>// corresponds to a Poisson process with parameter MemProfileRate. In Poisson</code></span>
<span class="codeline" id="line-1220"><code>// processes, the distance between two samples follows the exponential</code></span>
<span class="codeline" id="line-1221"><code>// distribution (exp(MemProfileRate)), so the best return value is a random</code></span>
<span class="codeline" id="line-1222"><code>// number taken from an exponential distribution whose mean is MemProfileRate.</code></span>
<span class="codeline" id="line-1223"><code>func nextSample() uintptr {</code></span>
<span class="codeline" id="line-1224"><code>	if MemProfileRate == 1 {</code></span>
<span class="codeline" id="line-1225"><code>		// Callers assign our return value to</code></span>
<span class="codeline" id="line-1226"><code>		// mcache.next_sample, but next_sample is not used</code></span>
<span class="codeline" id="line-1227"><code>		// when the rate is 1. So avoid the math below and</code></span>
<span class="codeline" id="line-1228"><code>		// just return something.</code></span>
<span class="codeline" id="line-1229"><code>		return 0</code></span>
<span class="codeline" id="line-1230"><code>	}</code></span>
<span class="codeline" id="line-1231"><code>	if GOOS == "plan9" {</code></span>
<span class="codeline" id="line-1232"><code>		// Plan 9 doesn't support floating point in note handler.</code></span>
<span class="codeline" id="line-1233"><code>		if g := getg(); g == g.m.gsignal {</code></span>
<span class="codeline" id="line-1234"><code>			return nextSampleNoFP()</code></span>
<span class="codeline" id="line-1235"><code>		}</code></span>
<span class="codeline" id="line-1236"><code>	}</code></span>
<span class="codeline" id="line-1237"><code></code></span>
<span class="codeline" id="line-1238"><code>	return uintptr(fastexprand(MemProfileRate))</code></span>
<span class="codeline" id="line-1239"><code>}</code></span>
<span class="codeline" id="line-1240"><code></code></span>
<span class="codeline" id="line-1241"><code>// fastexprand returns a random number from an exponential distribution with</code></span>
<span class="codeline" id="line-1242"><code>// the specified mean.</code></span>
<span class="codeline" id="line-1243"><code>func fastexprand(mean int) int32 {</code></span>
<span class="codeline" id="line-1244"><code>	// Avoid overflow. Maximum possible step is</code></span>
<span class="codeline" id="line-1245"><code>	// -ln(1/(1&lt;&lt;randomBitCount)) * mean, approximately 20 * mean.</code></span>
<span class="codeline" id="line-1246"><code>	switch {</code></span>
<span class="codeline" id="line-1247"><code>	case mean &gt; 0x7000000:</code></span>
<span class="codeline" id="line-1248"><code>		mean = 0x7000000</code></span>
<span class="codeline" id="line-1249"><code>	case mean == 0:</code></span>
<span class="codeline" id="line-1250"><code>		return 0</code></span>
<span class="codeline" id="line-1251"><code>	}</code></span>
<span class="codeline" id="line-1252"><code></code></span>
<span class="codeline" id="line-1253"><code>	// Take a random sample of the exponential distribution exp(-mean*x).</code></span>
<span class="codeline" id="line-1254"><code>	// The probability distribution function is mean*exp(-mean*x), so the CDF is</code></span>
<span class="codeline" id="line-1255"><code>	// p = 1 - exp(-mean*x), so</code></span>
<span class="codeline" id="line-1256"><code>	// q = 1 - p == exp(-mean*x)</code></span>
<span class="codeline" id="line-1257"><code>	// log_e(q) = -mean*x</code></span>
<span class="codeline" id="line-1258"><code>	// -log_e(q)/mean = x</code></span>
<span class="codeline" id="line-1259"><code>	// x = -log_e(q) * mean</code></span>
<span class="codeline" id="line-1260"><code>	// x = log_2(q) * (-log_e(2)) * mean    ; Using log_2 for efficiency</code></span>
<span class="codeline" id="line-1261"><code>	const randomBitCount = 26</code></span>
<span class="codeline" id="line-1262"><code>	q := fastrand()%(1&lt;&lt;randomBitCount) + 1</code></span>
<span class="codeline" id="line-1263"><code>	qlog := fastlog2(float64(q)) - randomBitCount</code></span>
<span class="codeline" id="line-1264"><code>	if qlog &gt; 0 {</code></span>
<span class="codeline" id="line-1265"><code>		qlog = 0</code></span>
<span class="codeline" id="line-1266"><code>	}</code></span>
<span class="codeline" id="line-1267"><code>	const minusLog2 = -0.6931471805599453 // -ln(2)</code></span>
<span class="codeline" id="line-1268"><code>	return int32(qlog*(minusLog2*float64(mean))) + 1</code></span>
<span class="codeline" id="line-1269"><code>}</code></span>
<span class="codeline" id="line-1270"><code></code></span>
<span class="codeline" id="line-1271"><code>// nextSampleNoFP is similar to nextSample, but uses older,</code></span>
<span class="codeline" id="line-1272"><code>// simpler code to avoid floating point.</code></span>
<span class="codeline" id="line-1273"><code>func nextSampleNoFP() uintptr {</code></span>
<span class="codeline" id="line-1274"><code>	// Set first allocation sample size.</code></span>
<span class="codeline" id="line-1275"><code>	rate := MemProfileRate</code></span>
<span class="codeline" id="line-1276"><code>	if rate &gt; 0x3fffffff { // make 2*rate not overflow</code></span>
<span class="codeline" id="line-1277"><code>		rate = 0x3fffffff</code></span>
<span class="codeline" id="line-1278"><code>	}</code></span>
<span class="codeline" id="line-1279"><code>	if rate != 0 {</code></span>
<span class="codeline" id="line-1280"><code>		return uintptr(fastrand() % uint32(2*rate))</code></span>
<span class="codeline" id="line-1281"><code>	}</code></span>
<span class="codeline" id="line-1282"><code>	return 0</code></span>
<span class="codeline" id="line-1283"><code>}</code></span>
<span class="codeline" id="line-1284"><code></code></span>
<span class="codeline" id="line-1285"><code>type persistentAlloc struct {</code></span>
<span class="codeline" id="line-1286"><code>	base *notInHeap</code></span>
<span class="codeline" id="line-1287"><code>	off  uintptr</code></span>
<span class="codeline" id="line-1288"><code>}</code></span>
<span class="codeline" id="line-1289"><code></code></span>
<span class="codeline" id="line-1290"><code>var globalAlloc struct {</code></span>
<span class="codeline" id="line-1291"><code>	mutex</code></span>
<span class="codeline" id="line-1292"><code>	persistentAlloc</code></span>
<span class="codeline" id="line-1293"><code>}</code></span>
<span class="codeline" id="line-1294"><code></code></span>
<span class="codeline" id="line-1295"><code>// persistentChunkSize is the number of bytes we allocate when we grow</code></span>
<span class="codeline" id="line-1296"><code>// a persistentAlloc.</code></span>
<span class="codeline" id="line-1297"><code>const persistentChunkSize = 256 &lt;&lt; 10</code></span>
<span class="codeline" id="line-1298"><code></code></span>
<span class="codeline" id="line-1299"><code>// persistentChunks is a list of all the persistent chunks we have</code></span>
<span class="codeline" id="line-1300"><code>// allocated. The list is maintained through the first word in the</code></span>
<span class="codeline" id="line-1301"><code>// persistent chunk. This is updated atomically.</code></span>
<span class="codeline" id="line-1302"><code>var persistentChunks *notInHeap</code></span>
<span class="codeline" id="line-1303"><code></code></span>
<span class="codeline" id="line-1304"><code>// Wrapper around sysAlloc that can allocate small chunks.</code></span>
<span class="codeline" id="line-1305"><code>// There is no associated free operation.</code></span>
<span class="codeline" id="line-1306"><code>// Intended for things like function/type/debug-related persistent data.</code></span>
<span class="codeline" id="line-1307"><code>// If align is 0, uses default align (currently 8).</code></span>
<span class="codeline" id="line-1308"><code>// The returned memory will be zeroed.</code></span>
<span class="codeline" id="line-1309"><code>//</code></span>
<span class="codeline" id="line-1310"><code>// Consider marking persistentalloc'd types go:notinheap.</code></span>
<span class="codeline" id="line-1311"><code>func persistentalloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer {</code></span>
<span class="codeline" id="line-1312"><code>	var p *notInHeap</code></span>
<span class="codeline" id="line-1313"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-1314"><code>		p = persistentalloc1(size, align, sysStat)</code></span>
<span class="codeline" id="line-1315"><code>	})</code></span>
<span class="codeline" id="line-1316"><code>	return unsafe.Pointer(p)</code></span>
<span class="codeline" id="line-1317"><code>}</code></span>
<span class="codeline" id="line-1318"><code></code></span>
<span class="codeline" id="line-1319"><code>// Must run on system stack because stack growth can (re)invoke it.</code></span>
<span class="codeline" id="line-1320"><code>// See issue 9174.</code></span>
<span class="codeline" id="line-1321"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1322"><code>func persistentalloc1(size, align uintptr, sysStat *sysMemStat) *notInHeap {</code></span>
<span class="codeline" id="line-1323"><code>	const (</code></span>
<span class="codeline" id="line-1324"><code>		maxBlock = 64 &lt;&lt; 10 // VM reservation granularity is 64K on windows</code></span>
<span class="codeline" id="line-1325"><code>	)</code></span>
<span class="codeline" id="line-1326"><code></code></span>
<span class="codeline" id="line-1327"><code>	if size == 0 {</code></span>
<span class="codeline" id="line-1328"><code>		throw("persistentalloc: size == 0")</code></span>
<span class="codeline" id="line-1329"><code>	}</code></span>
<span class="codeline" id="line-1330"><code>	if align != 0 {</code></span>
<span class="codeline" id="line-1331"><code>		if align&amp;(align-1) != 0 {</code></span>
<span class="codeline" id="line-1332"><code>			throw("persistentalloc: align is not a power of 2")</code></span>
<span class="codeline" id="line-1333"><code>		}</code></span>
<span class="codeline" id="line-1334"><code>		if align &gt; _PageSize {</code></span>
<span class="codeline" id="line-1335"><code>			throw("persistentalloc: align is too large")</code></span>
<span class="codeline" id="line-1336"><code>		}</code></span>
<span class="codeline" id="line-1337"><code>	} else {</code></span>
<span class="codeline" id="line-1338"><code>		align = 8</code></span>
<span class="codeline" id="line-1339"><code>	}</code></span>
<span class="codeline" id="line-1340"><code></code></span>
<span class="codeline" id="line-1341"><code>	if size &gt;= maxBlock {</code></span>
<span class="codeline" id="line-1342"><code>		return (*notInHeap)(sysAlloc(size, sysStat))</code></span>
<span class="codeline" id="line-1343"><code>	}</code></span>
<span class="codeline" id="line-1344"><code></code></span>
<span class="codeline" id="line-1345"><code>	mp := acquirem()</code></span>
<span class="codeline" id="line-1346"><code>	var persistent *persistentAlloc</code></span>
<span class="codeline" id="line-1347"><code>	if mp != nil &amp;&amp; mp.p != 0 {</code></span>
<span class="codeline" id="line-1348"><code>		persistent = &amp;mp.p.ptr().palloc</code></span>
<span class="codeline" id="line-1349"><code>	} else {</code></span>
<span class="codeline" id="line-1350"><code>		lock(&amp;globalAlloc.mutex)</code></span>
<span class="codeline" id="line-1351"><code>		persistent = &amp;globalAlloc.persistentAlloc</code></span>
<span class="codeline" id="line-1352"><code>	}</code></span>
<span class="codeline" id="line-1353"><code>	persistent.off = alignUp(persistent.off, align)</code></span>
<span class="codeline" id="line-1354"><code>	if persistent.off+size &gt; persistentChunkSize || persistent.base == nil {</code></span>
<span class="codeline" id="line-1355"><code>		persistent.base = (*notInHeap)(sysAlloc(persistentChunkSize, &amp;memstats.other_sys))</code></span>
<span class="codeline" id="line-1356"><code>		if persistent.base == nil {</code></span>
<span class="codeline" id="line-1357"><code>			if persistent == &amp;globalAlloc.persistentAlloc {</code></span>
<span class="codeline" id="line-1358"><code>				unlock(&amp;globalAlloc.mutex)</code></span>
<span class="codeline" id="line-1359"><code>			}</code></span>
<span class="codeline" id="line-1360"><code>			throw("runtime: cannot allocate memory")</code></span>
<span class="codeline" id="line-1361"><code>		}</code></span>
<span class="codeline" id="line-1362"><code></code></span>
<span class="codeline" id="line-1363"><code>		// Add the new chunk to the persistentChunks list.</code></span>
<span class="codeline" id="line-1364"><code>		for {</code></span>
<span class="codeline" id="line-1365"><code>			chunks := uintptr(unsafe.Pointer(persistentChunks))</code></span>
<span class="codeline" id="line-1366"><code>			*(*uintptr)(unsafe.Pointer(persistent.base)) = chunks</code></span>
<span class="codeline" id="line-1367"><code>			if atomic.Casuintptr((*uintptr)(unsafe.Pointer(&amp;persistentChunks)), chunks, uintptr(unsafe.Pointer(persistent.base))) {</code></span>
<span class="codeline" id="line-1368"><code>				break</code></span>
<span class="codeline" id="line-1369"><code>			}</code></span>
<span class="codeline" id="line-1370"><code>		}</code></span>
<span class="codeline" id="line-1371"><code>		persistent.off = alignUp(sys.PtrSize, align)</code></span>
<span class="codeline" id="line-1372"><code>	}</code></span>
<span class="codeline" id="line-1373"><code>	p := persistent.base.add(persistent.off)</code></span>
<span class="codeline" id="line-1374"><code>	persistent.off += size</code></span>
<span class="codeline" id="line-1375"><code>	releasem(mp)</code></span>
<span class="codeline" id="line-1376"><code>	if persistent == &amp;globalAlloc.persistentAlloc {</code></span>
<span class="codeline" id="line-1377"><code>		unlock(&amp;globalAlloc.mutex)</code></span>
<span class="codeline" id="line-1378"><code>	}</code></span>
<span class="codeline" id="line-1379"><code></code></span>
<span class="codeline" id="line-1380"><code>	if sysStat != &amp;memstats.other_sys {</code></span>
<span class="codeline" id="line-1381"><code>		sysStat.add(int64(size))</code></span>
<span class="codeline" id="line-1382"><code>		memstats.other_sys.add(-int64(size))</code></span>
<span class="codeline" id="line-1383"><code>	}</code></span>
<span class="codeline" id="line-1384"><code>	return p</code></span>
<span class="codeline" id="line-1385"><code>}</code></span>
<span class="codeline" id="line-1386"><code></code></span>
<span class="codeline" id="line-1387"><code>// inPersistentAlloc reports whether p points to memory allocated by</code></span>
<span class="codeline" id="line-1388"><code>// persistentalloc. This must be nosplit because it is called by the</code></span>
<span class="codeline" id="line-1389"><code>// cgo checker code, which is called by the write barrier code.</code></span>
<span class="codeline" id="line-1390"><code>//go:nosplit</code></span>
<span class="codeline" id="line-1391"><code>func inPersistentAlloc(p uintptr) bool {</code></span>
<span class="codeline" id="line-1392"><code>	chunk := atomic.Loaduintptr((*uintptr)(unsafe.Pointer(&amp;persistentChunks)))</code></span>
<span class="codeline" id="line-1393"><code>	for chunk != 0 {</code></span>
<span class="codeline" id="line-1394"><code>		if p &gt;= chunk &amp;&amp; p &lt; chunk+persistentChunkSize {</code></span>
<span class="codeline" id="line-1395"><code>			return true</code></span>
<span class="codeline" id="line-1396"><code>		}</code></span>
<span class="codeline" id="line-1397"><code>		chunk = *(*uintptr)(unsafe.Pointer(chunk))</code></span>
<span class="codeline" id="line-1398"><code>	}</code></span>
<span class="codeline" id="line-1399"><code>	return false</code></span>
<span class="codeline" id="line-1400"><code>}</code></span>
<span class="codeline" id="line-1401"><code></code></span>
<span class="codeline" id="line-1402"><code>// linearAlloc is a simple linear allocator that pre-reserves a region</code></span>
<span class="codeline" id="line-1403"><code>// of memory and then maps that region into the Ready state as needed. The</code></span>
<span class="codeline" id="line-1404"><code>// caller is responsible for locking.</code></span>
<span class="codeline" id="line-1405"><code>type linearAlloc struct {</code></span>
<span class="codeline" id="line-1406"><code>	next   uintptr // next free byte</code></span>
<span class="codeline" id="line-1407"><code>	mapped uintptr // one byte past end of mapped space</code></span>
<span class="codeline" id="line-1408"><code>	end    uintptr // end of reserved space</code></span>
<span class="codeline" id="line-1409"><code>}</code></span>
<span class="codeline" id="line-1410"><code></code></span>
<span class="codeline" id="line-1411"><code>func (l *linearAlloc) init(base, size uintptr) {</code></span>
<span class="codeline" id="line-1412"><code>	if base+size &lt; base {</code></span>
<span class="codeline" id="line-1413"><code>		// Chop off the last byte. The runtime isn't prepared</code></span>
<span class="codeline" id="line-1414"><code>		// to deal with situations where the bounds could overflow.</code></span>
<span class="codeline" id="line-1415"><code>		// Leave that memory reserved, though, so we don't map it</code></span>
<span class="codeline" id="line-1416"><code>		// later.</code></span>
<span class="codeline" id="line-1417"><code>		size -= 1</code></span>
<span class="codeline" id="line-1418"><code>	}</code></span>
<span class="codeline" id="line-1419"><code>	l.next, l.mapped = base, base</code></span>
<span class="codeline" id="line-1420"><code>	l.end = base + size</code></span>
<span class="codeline" id="line-1421"><code>}</code></span>
<span class="codeline" id="line-1422"><code></code></span>
<span class="codeline" id="line-1423"><code>func (l *linearAlloc) alloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer {</code></span>
<span class="codeline" id="line-1424"><code>	p := alignUp(l.next, align)</code></span>
<span class="codeline" id="line-1425"><code>	if p+size &gt; l.end {</code></span>
<span class="codeline" id="line-1426"><code>		return nil</code></span>
<span class="codeline" id="line-1427"><code>	}</code></span>
<span class="codeline" id="line-1428"><code>	l.next = p + size</code></span>
<span class="codeline" id="line-1429"><code>	if pEnd := alignUp(l.next-1, physPageSize); pEnd &gt; l.mapped {</code></span>
<span class="codeline" id="line-1430"><code>		// Transition from Reserved to Prepared to Ready.</code></span>
<span class="codeline" id="line-1431"><code>		sysMap(unsafe.Pointer(l.mapped), pEnd-l.mapped, sysStat)</code></span>
<span class="codeline" id="line-1432"><code>		sysUsed(unsafe.Pointer(l.mapped), pEnd-l.mapped)</code></span>
<span class="codeline" id="line-1433"><code>		l.mapped = pEnd</code></span>
<span class="codeline" id="line-1434"><code>	}</code></span>
<span class="codeline" id="line-1435"><code>	return unsafe.Pointer(p)</code></span>
<span class="codeline" id="line-1436"><code>}</code></span>
<span class="codeline" id="line-1437"><code></code></span>
<span class="codeline" id="line-1438"><code>// notInHeap is off-heap memory allocated by a lower-level allocator</code></span>
<span class="codeline" id="line-1439"><code>// like sysAlloc or persistentAlloc.</code></span>
<span class="codeline" id="line-1440"><code>//</code></span>
<span class="codeline" id="line-1441"><code>// In general, it's better to use real types marked as go:notinheap,</code></span>
<span class="codeline" id="line-1442"><code>// but this serves as a generic type for situations where that isn't</code></span>
<span class="codeline" id="line-1443"><code>// possible (like in the allocators).</code></span>
<span class="codeline" id="line-1444"><code>//</code></span>
<span class="codeline" id="line-1445"><code>// TODO: Use this as the return type of sysAlloc, persistentAlloc, etc?</code></span>
<span class="codeline" id="line-1446"><code>//</code></span>
<span class="codeline" id="line-1447"><code>//go:notinheap</code></span>
<span class="codeline" id="line-1448"><code>type notInHeap struct{}</code></span>
<span class="codeline" id="line-1449"><code></code></span>
<span class="codeline" id="line-1450"><code>func (p *notInHeap) add(bytes uintptr) *notInHeap {</code></span>
<span class="codeline" id="line-1451"><code>	return (*notInHeap)(unsafe.Pointer(uintptr(unsafe.Pointer(p)) + bytes))</code></span>
<span class="codeline" id="line-1452"><code>}</code></span>
</pre><pre id="footer">
<table><tr><td><img src="../../png/go101-twitter.png"></td>
<td>The pages are generated with <a href="https://go101.org/article/tool-golds.html"><b>Golds</b></a> <i>v0.3.2</i>. (GOOS=linux GOARCH=amd64)
<b>Golds</b> is a <a href="https://go101.org">Go 101</a> project developed by <a href="https://tapirgames.com">Tapir Liu</a>.
PR and bug reports are welcome and can be submitted to <a href="https://github.com/go101/golds">the issue list</a>.
Please follow <a href="https://twitter.com/go100and1">@Go100and1</a> (reachable from the left QR code) to get the latest news of <b>Golds</b>.</td></tr></table></pre>
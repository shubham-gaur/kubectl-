<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Source: mprof.go in package runtime</title>
<link href="../../css/light-v0.3.2.css" rel="stylesheet">
<script src="../../jvs/golds-v0.3.2.js"></script>
<body onload="onPageLoad()"><div>

<pre id="header"><code><span class="title">Source File</span>
	mprof.go

<span class="title">Belonging Package</span>
	<a href="../../pkg/runtime.html">runtime</a>
</code></pre>

<pre class="line-numbers">
<span class="codeline" id="line-1"><code>// Copyright 2009 The Go Authors. All rights reserved.</code></span>
<span class="codeline" id="line-2"><code>// Use of this source code is governed by a BSD-style</code></span>
<span class="codeline" id="line-3"><code>// license that can be found in the LICENSE file.</code></span>
<span class="codeline" id="line-4"><code></code></span>
<span class="codeline" id="line-5"><code>// Malloc profiling.</code></span>
<span class="codeline" id="line-6"><code>// Patterned after tcmalloc's algorithms; shorter code.</code></span>
<span class="codeline" id="line-7"><code></code></span>
<span class="codeline" id="line-8"><code>package runtime</code></span>
<span class="codeline" id="line-9"><code></code></span>
<span class="codeline" id="line-10"><code>import (</code></span>
<span class="codeline" id="line-11"><code>	"runtime/internal/atomic"</code></span>
<span class="codeline" id="line-12"><code>	"unsafe"</code></span>
<span class="codeline" id="line-13"><code>)</code></span>
<span class="codeline" id="line-14"><code></code></span>
<span class="codeline" id="line-15"><code>// NOTE(rsc): Everything here could use cas if contention became an issue.</code></span>
<span class="codeline" id="line-16"><code>var proflock mutex</code></span>
<span class="codeline" id="line-17"><code></code></span>
<span class="codeline" id="line-18"><code>// All memory allocations are local and do not escape outside of the profiler.</code></span>
<span class="codeline" id="line-19"><code>// The profiler is forbidden from referring to garbage-collected memory.</code></span>
<span class="codeline" id="line-20"><code></code></span>
<span class="codeline" id="line-21"><code>const (</code></span>
<span class="codeline" id="line-22"><code>	// profile types</code></span>
<span class="codeline" id="line-23"><code>	memProfile bucketType = 1 + iota</code></span>
<span class="codeline" id="line-24"><code>	blockProfile</code></span>
<span class="codeline" id="line-25"><code>	mutexProfile</code></span>
<span class="codeline" id="line-26"><code></code></span>
<span class="codeline" id="line-27"><code>	// size of bucket hash table</code></span>
<span class="codeline" id="line-28"><code>	buckHashSize = 179999</code></span>
<span class="codeline" id="line-29"><code></code></span>
<span class="codeline" id="line-30"><code>	// max depth of stack to record in bucket</code></span>
<span class="codeline" id="line-31"><code>	maxStack = 32</code></span>
<span class="codeline" id="line-32"><code>)</code></span>
<span class="codeline" id="line-33"><code></code></span>
<span class="codeline" id="line-34"><code>type bucketType int</code></span>
<span class="codeline" id="line-35"><code></code></span>
<span class="codeline" id="line-36"><code>// A bucket holds per-call-stack profiling information.</code></span>
<span class="codeline" id="line-37"><code>// The representation is a bit sleazy, inherited from C.</code></span>
<span class="codeline" id="line-38"><code>// This struct defines the bucket header. It is followed in</code></span>
<span class="codeline" id="line-39"><code>// memory by the stack words and then the actual record</code></span>
<span class="codeline" id="line-40"><code>// data, either a memRecord or a blockRecord.</code></span>
<span class="codeline" id="line-41"><code>//</code></span>
<span class="codeline" id="line-42"><code>// Per-call-stack profiling information.</code></span>
<span class="codeline" id="line-43"><code>// Lookup by hashing call stack into a linked-list hash table.</code></span>
<span class="codeline" id="line-44"><code>//</code></span>
<span class="codeline" id="line-45"><code>// No heap pointers.</code></span>
<span class="codeline" id="line-46"><code>//</code></span>
<span class="codeline" id="line-47"><code>//go:notinheap</code></span>
<span class="codeline" id="line-48"><code>type bucket struct {</code></span>
<span class="codeline" id="line-49"><code>	next    *bucket</code></span>
<span class="codeline" id="line-50"><code>	allnext *bucket</code></span>
<span class="codeline" id="line-51"><code>	typ     bucketType // memBucket or blockBucket (includes mutexProfile)</code></span>
<span class="codeline" id="line-52"><code>	hash    uintptr</code></span>
<span class="codeline" id="line-53"><code>	size    uintptr</code></span>
<span class="codeline" id="line-54"><code>	nstk    uintptr</code></span>
<span class="codeline" id="line-55"><code>}</code></span>
<span class="codeline" id="line-56"><code></code></span>
<span class="codeline" id="line-57"><code>// A memRecord is the bucket data for a bucket of type memProfile,</code></span>
<span class="codeline" id="line-58"><code>// part of the memory profile.</code></span>
<span class="codeline" id="line-59"><code>type memRecord struct {</code></span>
<span class="codeline" id="line-60"><code>	// The following complex 3-stage scheme of stats accumulation</code></span>
<span class="codeline" id="line-61"><code>	// is required to obtain a consistent picture of mallocs and frees</code></span>
<span class="codeline" id="line-62"><code>	// for some point in time.</code></span>
<span class="codeline" id="line-63"><code>	// The problem is that mallocs come in real time, while frees</code></span>
<span class="codeline" id="line-64"><code>	// come only after a GC during concurrent sweeping. So if we would</code></span>
<span class="codeline" id="line-65"><code>	// naively count them, we would get a skew toward mallocs.</code></span>
<span class="codeline" id="line-66"><code>	//</code></span>
<span class="codeline" id="line-67"><code>	// Hence, we delay information to get consistent snapshots as</code></span>
<span class="codeline" id="line-68"><code>	// of mark termination. Allocations count toward the next mark</code></span>
<span class="codeline" id="line-69"><code>	// termination's snapshot, while sweep frees count toward the</code></span>
<span class="codeline" id="line-70"><code>	// previous mark termination's snapshot:</code></span>
<span class="codeline" id="line-71"><code>	//</code></span>
<span class="codeline" id="line-72"><code>	//              MT          MT          MT          MT</code></span>
<span class="codeline" id="line-73"><code>	//             .·|         .·|         .·|         .·|</code></span>
<span class="codeline" id="line-74"><code>	//          .·˙  |      .·˙  |      .·˙  |      .·˙  |</code></span>
<span class="codeline" id="line-75"><code>	//       .·˙     |   .·˙     |   .·˙     |   .·˙     |</code></span>
<span class="codeline" id="line-76"><code>	//    .·˙        |.·˙        |.·˙        |.·˙        |</code></span>
<span class="codeline" id="line-77"><code>	//</code></span>
<span class="codeline" id="line-78"><code>	//       alloc → ▲ ← free</code></span>
<span class="codeline" id="line-79"><code>	//               ┠┅┅┅┅┅┅┅┅┅┅┅P</code></span>
<span class="codeline" id="line-80"><code>	//       C+2     →    C+1    →  C</code></span>
<span class="codeline" id="line-81"><code>	//</code></span>
<span class="codeline" id="line-82"><code>	//                   alloc → ▲ ← free</code></span>
<span class="codeline" id="line-83"><code>	//                           ┠┅┅┅┅┅┅┅┅┅┅┅P</code></span>
<span class="codeline" id="line-84"><code>	//                   C+2     →    C+1    →  C</code></span>
<span class="codeline" id="line-85"><code>	//</code></span>
<span class="codeline" id="line-86"><code>	// Since we can't publish a consistent snapshot until all of</code></span>
<span class="codeline" id="line-87"><code>	// the sweep frees are accounted for, we wait until the next</code></span>
<span class="codeline" id="line-88"><code>	// mark termination ("MT" above) to publish the previous mark</code></span>
<span class="codeline" id="line-89"><code>	// termination's snapshot ("P" above). To do this, allocation</code></span>
<span class="codeline" id="line-90"><code>	// and free events are accounted to *future* heap profile</code></span>
<span class="codeline" id="line-91"><code>	// cycles ("C+n" above) and we only publish a cycle once all</code></span>
<span class="codeline" id="line-92"><code>	// of the events from that cycle must be done. Specifically:</code></span>
<span class="codeline" id="line-93"><code>	//</code></span>
<span class="codeline" id="line-94"><code>	// Mallocs are accounted to cycle C+2.</code></span>
<span class="codeline" id="line-95"><code>	// Explicit frees are accounted to cycle C+2.</code></span>
<span class="codeline" id="line-96"><code>	// GC frees (done during sweeping) are accounted to cycle C+1.</code></span>
<span class="codeline" id="line-97"><code>	//</code></span>
<span class="codeline" id="line-98"><code>	// After mark termination, we increment the global heap</code></span>
<span class="codeline" id="line-99"><code>	// profile cycle counter and accumulate the stats from cycle C</code></span>
<span class="codeline" id="line-100"><code>	// into the active profile.</code></span>
<span class="codeline" id="line-101"><code></code></span>
<span class="codeline" id="line-102"><code>	// active is the currently published profile. A profiling</code></span>
<span class="codeline" id="line-103"><code>	// cycle can be accumulated into active once its complete.</code></span>
<span class="codeline" id="line-104"><code>	active memRecordCycle</code></span>
<span class="codeline" id="line-105"><code></code></span>
<span class="codeline" id="line-106"><code>	// future records the profile events we're counting for cycles</code></span>
<span class="codeline" id="line-107"><code>	// that have not yet been published. This is ring buffer</code></span>
<span class="codeline" id="line-108"><code>	// indexed by the global heap profile cycle C and stores</code></span>
<span class="codeline" id="line-109"><code>	// cycles C, C+1, and C+2. Unlike active, these counts are</code></span>
<span class="codeline" id="line-110"><code>	// only for a single cycle; they are not cumulative across</code></span>
<span class="codeline" id="line-111"><code>	// cycles.</code></span>
<span class="codeline" id="line-112"><code>	//</code></span>
<span class="codeline" id="line-113"><code>	// We store cycle C here because there's a window between when</code></span>
<span class="codeline" id="line-114"><code>	// C becomes the active cycle and when we've flushed it to</code></span>
<span class="codeline" id="line-115"><code>	// active.</code></span>
<span class="codeline" id="line-116"><code>	future [3]memRecordCycle</code></span>
<span class="codeline" id="line-117"><code>}</code></span>
<span class="codeline" id="line-118"><code></code></span>
<span class="codeline" id="line-119"><code>// memRecordCycle</code></span>
<span class="codeline" id="line-120"><code>type memRecordCycle struct {</code></span>
<span class="codeline" id="line-121"><code>	allocs, frees           uintptr</code></span>
<span class="codeline" id="line-122"><code>	alloc_bytes, free_bytes uintptr</code></span>
<span class="codeline" id="line-123"><code>}</code></span>
<span class="codeline" id="line-124"><code></code></span>
<span class="codeline" id="line-125"><code>// add accumulates b into a. It does not zero b.</code></span>
<span class="codeline" id="line-126"><code>func (a *memRecordCycle) add(b *memRecordCycle) {</code></span>
<span class="codeline" id="line-127"><code>	a.allocs += b.allocs</code></span>
<span class="codeline" id="line-128"><code>	a.frees += b.frees</code></span>
<span class="codeline" id="line-129"><code>	a.alloc_bytes += b.alloc_bytes</code></span>
<span class="codeline" id="line-130"><code>	a.free_bytes += b.free_bytes</code></span>
<span class="codeline" id="line-131"><code>}</code></span>
<span class="codeline" id="line-132"><code></code></span>
<span class="codeline" id="line-133"><code>// A blockRecord is the bucket data for a bucket of type blockProfile,</code></span>
<span class="codeline" id="line-134"><code>// which is used in blocking and mutex profiles.</code></span>
<span class="codeline" id="line-135"><code>type blockRecord struct {</code></span>
<span class="codeline" id="line-136"><code>	count  int64</code></span>
<span class="codeline" id="line-137"><code>	cycles int64</code></span>
<span class="codeline" id="line-138"><code>}</code></span>
<span class="codeline" id="line-139"><code></code></span>
<span class="codeline" id="line-140"><code>var (</code></span>
<span class="codeline" id="line-141"><code>	mbuckets  *bucket // memory profile buckets</code></span>
<span class="codeline" id="line-142"><code>	bbuckets  *bucket // blocking profile buckets</code></span>
<span class="codeline" id="line-143"><code>	xbuckets  *bucket // mutex profile buckets</code></span>
<span class="codeline" id="line-144"><code>	buckhash  *[179999]*bucket</code></span>
<span class="codeline" id="line-145"><code>	bucketmem uintptr</code></span>
<span class="codeline" id="line-146"><code></code></span>
<span class="codeline" id="line-147"><code>	mProf struct {</code></span>
<span class="codeline" id="line-148"><code>		// All fields in mProf are protected by proflock.</code></span>
<span class="codeline" id="line-149"><code></code></span>
<span class="codeline" id="line-150"><code>		// cycle is the global heap profile cycle. This wraps</code></span>
<span class="codeline" id="line-151"><code>		// at mProfCycleWrap.</code></span>
<span class="codeline" id="line-152"><code>		cycle uint32</code></span>
<span class="codeline" id="line-153"><code>		// flushed indicates that future[cycle] in all buckets</code></span>
<span class="codeline" id="line-154"><code>		// has been flushed to the active profile.</code></span>
<span class="codeline" id="line-155"><code>		flushed bool</code></span>
<span class="codeline" id="line-156"><code>	}</code></span>
<span class="codeline" id="line-157"><code>)</code></span>
<span class="codeline" id="line-158"><code></code></span>
<span class="codeline" id="line-159"><code>const mProfCycleWrap = uint32(len(memRecord{}.future)) * (2 &lt;&lt; 24)</code></span>
<span class="codeline" id="line-160"><code></code></span>
<span class="codeline" id="line-161"><code>// newBucket allocates a bucket with the given type and number of stack entries.</code></span>
<span class="codeline" id="line-162"><code>func newBucket(typ bucketType, nstk int) *bucket {</code></span>
<span class="codeline" id="line-163"><code>	size := unsafe.Sizeof(bucket{}) + uintptr(nstk)*unsafe.Sizeof(uintptr(0))</code></span>
<span class="codeline" id="line-164"><code>	switch typ {</code></span>
<span class="codeline" id="line-165"><code>	default:</code></span>
<span class="codeline" id="line-166"><code>		throw("invalid profile bucket type")</code></span>
<span class="codeline" id="line-167"><code>	case memProfile:</code></span>
<span class="codeline" id="line-168"><code>		size += unsafe.Sizeof(memRecord{})</code></span>
<span class="codeline" id="line-169"><code>	case blockProfile, mutexProfile:</code></span>
<span class="codeline" id="line-170"><code>		size += unsafe.Sizeof(blockRecord{})</code></span>
<span class="codeline" id="line-171"><code>	}</code></span>
<span class="codeline" id="line-172"><code></code></span>
<span class="codeline" id="line-173"><code>	b := (*bucket)(persistentalloc(size, 0, &amp;memstats.buckhash_sys))</code></span>
<span class="codeline" id="line-174"><code>	bucketmem += size</code></span>
<span class="codeline" id="line-175"><code>	b.typ = typ</code></span>
<span class="codeline" id="line-176"><code>	b.nstk = uintptr(nstk)</code></span>
<span class="codeline" id="line-177"><code>	return b</code></span>
<span class="codeline" id="line-178"><code>}</code></span>
<span class="codeline" id="line-179"><code></code></span>
<span class="codeline" id="line-180"><code>// stk returns the slice in b holding the stack.</code></span>
<span class="codeline" id="line-181"><code>func (b *bucket) stk() []uintptr {</code></span>
<span class="codeline" id="line-182"><code>	stk := (*[maxStack]uintptr)(add(unsafe.Pointer(b), unsafe.Sizeof(*b)))</code></span>
<span class="codeline" id="line-183"><code>	return stk[:b.nstk:b.nstk]</code></span>
<span class="codeline" id="line-184"><code>}</code></span>
<span class="codeline" id="line-185"><code></code></span>
<span class="codeline" id="line-186"><code>// mp returns the memRecord associated with the memProfile bucket b.</code></span>
<span class="codeline" id="line-187"><code>func (b *bucket) mp() *memRecord {</code></span>
<span class="codeline" id="line-188"><code>	if b.typ != memProfile {</code></span>
<span class="codeline" id="line-189"><code>		throw("bad use of bucket.mp")</code></span>
<span class="codeline" id="line-190"><code>	}</code></span>
<span class="codeline" id="line-191"><code>	data := add(unsafe.Pointer(b), unsafe.Sizeof(*b)+b.nstk*unsafe.Sizeof(uintptr(0)))</code></span>
<span class="codeline" id="line-192"><code>	return (*memRecord)(data)</code></span>
<span class="codeline" id="line-193"><code>}</code></span>
<span class="codeline" id="line-194"><code></code></span>
<span class="codeline" id="line-195"><code>// bp returns the blockRecord associated with the blockProfile bucket b.</code></span>
<span class="codeline" id="line-196"><code>func (b *bucket) bp() *blockRecord {</code></span>
<span class="codeline" id="line-197"><code>	if b.typ != blockProfile &amp;&amp; b.typ != mutexProfile {</code></span>
<span class="codeline" id="line-198"><code>		throw("bad use of bucket.bp")</code></span>
<span class="codeline" id="line-199"><code>	}</code></span>
<span class="codeline" id="line-200"><code>	data := add(unsafe.Pointer(b), unsafe.Sizeof(*b)+b.nstk*unsafe.Sizeof(uintptr(0)))</code></span>
<span class="codeline" id="line-201"><code>	return (*blockRecord)(data)</code></span>
<span class="codeline" id="line-202"><code>}</code></span>
<span class="codeline" id="line-203"><code></code></span>
<span class="codeline" id="line-204"><code>// Return the bucket for stk[0:nstk], allocating new bucket if needed.</code></span>
<span class="codeline" id="line-205"><code>func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket {</code></span>
<span class="codeline" id="line-206"><code>	if buckhash == nil {</code></span>
<span class="codeline" id="line-207"><code>		buckhash = (*[buckHashSize]*bucket)(sysAlloc(unsafe.Sizeof(*buckhash), &amp;memstats.buckhash_sys))</code></span>
<span class="codeline" id="line-208"><code>		if buckhash == nil {</code></span>
<span class="codeline" id="line-209"><code>			throw("runtime: cannot allocate memory")</code></span>
<span class="codeline" id="line-210"><code>		}</code></span>
<span class="codeline" id="line-211"><code>	}</code></span>
<span class="codeline" id="line-212"><code></code></span>
<span class="codeline" id="line-213"><code>	// Hash stack.</code></span>
<span class="codeline" id="line-214"><code>	var h uintptr</code></span>
<span class="codeline" id="line-215"><code>	for _, pc := range stk {</code></span>
<span class="codeline" id="line-216"><code>		h += pc</code></span>
<span class="codeline" id="line-217"><code>		h += h &lt;&lt; 10</code></span>
<span class="codeline" id="line-218"><code>		h ^= h &gt;&gt; 6</code></span>
<span class="codeline" id="line-219"><code>	}</code></span>
<span class="codeline" id="line-220"><code>	// hash in size</code></span>
<span class="codeline" id="line-221"><code>	h += size</code></span>
<span class="codeline" id="line-222"><code>	h += h &lt;&lt; 10</code></span>
<span class="codeline" id="line-223"><code>	h ^= h &gt;&gt; 6</code></span>
<span class="codeline" id="line-224"><code>	// finalize</code></span>
<span class="codeline" id="line-225"><code>	h += h &lt;&lt; 3</code></span>
<span class="codeline" id="line-226"><code>	h ^= h &gt;&gt; 11</code></span>
<span class="codeline" id="line-227"><code></code></span>
<span class="codeline" id="line-228"><code>	i := int(h % buckHashSize)</code></span>
<span class="codeline" id="line-229"><code>	for b := buckhash[i]; b != nil; b = b.next {</code></span>
<span class="codeline" id="line-230"><code>		if b.typ == typ &amp;&amp; b.hash == h &amp;&amp; b.size == size &amp;&amp; eqslice(b.stk(), stk) {</code></span>
<span class="codeline" id="line-231"><code>			return b</code></span>
<span class="codeline" id="line-232"><code>		}</code></span>
<span class="codeline" id="line-233"><code>	}</code></span>
<span class="codeline" id="line-234"><code></code></span>
<span class="codeline" id="line-235"><code>	if !alloc {</code></span>
<span class="codeline" id="line-236"><code>		return nil</code></span>
<span class="codeline" id="line-237"><code>	}</code></span>
<span class="codeline" id="line-238"><code></code></span>
<span class="codeline" id="line-239"><code>	// Create new bucket.</code></span>
<span class="codeline" id="line-240"><code>	b := newBucket(typ, len(stk))</code></span>
<span class="codeline" id="line-241"><code>	copy(b.stk(), stk)</code></span>
<span class="codeline" id="line-242"><code>	b.hash = h</code></span>
<span class="codeline" id="line-243"><code>	b.size = size</code></span>
<span class="codeline" id="line-244"><code>	b.next = buckhash[i]</code></span>
<span class="codeline" id="line-245"><code>	buckhash[i] = b</code></span>
<span class="codeline" id="line-246"><code>	if typ == memProfile {</code></span>
<span class="codeline" id="line-247"><code>		b.allnext = mbuckets</code></span>
<span class="codeline" id="line-248"><code>		mbuckets = b</code></span>
<span class="codeline" id="line-249"><code>	} else if typ == mutexProfile {</code></span>
<span class="codeline" id="line-250"><code>		b.allnext = xbuckets</code></span>
<span class="codeline" id="line-251"><code>		xbuckets = b</code></span>
<span class="codeline" id="line-252"><code>	} else {</code></span>
<span class="codeline" id="line-253"><code>		b.allnext = bbuckets</code></span>
<span class="codeline" id="line-254"><code>		bbuckets = b</code></span>
<span class="codeline" id="line-255"><code>	}</code></span>
<span class="codeline" id="line-256"><code>	return b</code></span>
<span class="codeline" id="line-257"><code>}</code></span>
<span class="codeline" id="line-258"><code></code></span>
<span class="codeline" id="line-259"><code>func eqslice(x, y []uintptr) bool {</code></span>
<span class="codeline" id="line-260"><code>	if len(x) != len(y) {</code></span>
<span class="codeline" id="line-261"><code>		return false</code></span>
<span class="codeline" id="line-262"><code>	}</code></span>
<span class="codeline" id="line-263"><code>	for i, xi := range x {</code></span>
<span class="codeline" id="line-264"><code>		if xi != y[i] {</code></span>
<span class="codeline" id="line-265"><code>			return false</code></span>
<span class="codeline" id="line-266"><code>		}</code></span>
<span class="codeline" id="line-267"><code>	}</code></span>
<span class="codeline" id="line-268"><code>	return true</code></span>
<span class="codeline" id="line-269"><code>}</code></span>
<span class="codeline" id="line-270"><code></code></span>
<span class="codeline" id="line-271"><code>// mProf_NextCycle publishes the next heap profile cycle and creates a</code></span>
<span class="codeline" id="line-272"><code>// fresh heap profile cycle. This operation is fast and can be done</code></span>
<span class="codeline" id="line-273"><code>// during STW. The caller must call mProf_Flush before calling</code></span>
<span class="codeline" id="line-274"><code>// mProf_NextCycle again.</code></span>
<span class="codeline" id="line-275"><code>//</code></span>
<span class="codeline" id="line-276"><code>// This is called by mark termination during STW so allocations and</code></span>
<span class="codeline" id="line-277"><code>// frees after the world is started again count towards a new heap</code></span>
<span class="codeline" id="line-278"><code>// profiling cycle.</code></span>
<span class="codeline" id="line-279"><code>func mProf_NextCycle() {</code></span>
<span class="codeline" id="line-280"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-281"><code>	// We explicitly wrap mProf.cycle rather than depending on</code></span>
<span class="codeline" id="line-282"><code>	// uint wraparound because the memRecord.future ring does not</code></span>
<span class="codeline" id="line-283"><code>	// itself wrap at a power of two.</code></span>
<span class="codeline" id="line-284"><code>	mProf.cycle = (mProf.cycle + 1) % mProfCycleWrap</code></span>
<span class="codeline" id="line-285"><code>	mProf.flushed = false</code></span>
<span class="codeline" id="line-286"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-287"><code>}</code></span>
<span class="codeline" id="line-288"><code></code></span>
<span class="codeline" id="line-289"><code>// mProf_Flush flushes the events from the current heap profiling</code></span>
<span class="codeline" id="line-290"><code>// cycle into the active profile. After this it is safe to start a new</code></span>
<span class="codeline" id="line-291"><code>// heap profiling cycle with mProf_NextCycle.</code></span>
<span class="codeline" id="line-292"><code>//</code></span>
<span class="codeline" id="line-293"><code>// This is called by GC after mark termination starts the world. In</code></span>
<span class="codeline" id="line-294"><code>// contrast with mProf_NextCycle, this is somewhat expensive, but safe</code></span>
<span class="codeline" id="line-295"><code>// to do concurrently.</code></span>
<span class="codeline" id="line-296"><code>func mProf_Flush() {</code></span>
<span class="codeline" id="line-297"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-298"><code>	if !mProf.flushed {</code></span>
<span class="codeline" id="line-299"><code>		mProf_FlushLocked()</code></span>
<span class="codeline" id="line-300"><code>		mProf.flushed = true</code></span>
<span class="codeline" id="line-301"><code>	}</code></span>
<span class="codeline" id="line-302"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-303"><code>}</code></span>
<span class="codeline" id="line-304"><code></code></span>
<span class="codeline" id="line-305"><code>func mProf_FlushLocked() {</code></span>
<span class="codeline" id="line-306"><code>	c := mProf.cycle</code></span>
<span class="codeline" id="line-307"><code>	for b := mbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-308"><code>		mp := b.mp()</code></span>
<span class="codeline" id="line-309"><code></code></span>
<span class="codeline" id="line-310"><code>		// Flush cycle C into the published profile and clear</code></span>
<span class="codeline" id="line-311"><code>		// it for reuse.</code></span>
<span class="codeline" id="line-312"><code>		mpc := &amp;mp.future[c%uint32(len(mp.future))]</code></span>
<span class="codeline" id="line-313"><code>		mp.active.add(mpc)</code></span>
<span class="codeline" id="line-314"><code>		*mpc = memRecordCycle{}</code></span>
<span class="codeline" id="line-315"><code>	}</code></span>
<span class="codeline" id="line-316"><code>}</code></span>
<span class="codeline" id="line-317"><code></code></span>
<span class="codeline" id="line-318"><code>// mProf_PostSweep records that all sweep frees for this GC cycle have</code></span>
<span class="codeline" id="line-319"><code>// completed. This has the effect of publishing the heap profile</code></span>
<span class="codeline" id="line-320"><code>// snapshot as of the last mark termination without advancing the heap</code></span>
<span class="codeline" id="line-321"><code>// profile cycle.</code></span>
<span class="codeline" id="line-322"><code>func mProf_PostSweep() {</code></span>
<span class="codeline" id="line-323"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-324"><code>	// Flush cycle C+1 to the active profile so everything as of</code></span>
<span class="codeline" id="line-325"><code>	// the last mark termination becomes visible. *Don't* advance</code></span>
<span class="codeline" id="line-326"><code>	// the cycle, since we're still accumulating allocs in cycle</code></span>
<span class="codeline" id="line-327"><code>	// C+2, which have to become C+1 in the next mark termination</code></span>
<span class="codeline" id="line-328"><code>	// and so on.</code></span>
<span class="codeline" id="line-329"><code>	c := mProf.cycle</code></span>
<span class="codeline" id="line-330"><code>	for b := mbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-331"><code>		mp := b.mp()</code></span>
<span class="codeline" id="line-332"><code>		mpc := &amp;mp.future[(c+1)%uint32(len(mp.future))]</code></span>
<span class="codeline" id="line-333"><code>		mp.active.add(mpc)</code></span>
<span class="codeline" id="line-334"><code>		*mpc = memRecordCycle{}</code></span>
<span class="codeline" id="line-335"><code>	}</code></span>
<span class="codeline" id="line-336"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-337"><code>}</code></span>
<span class="codeline" id="line-338"><code></code></span>
<span class="codeline" id="line-339"><code>// Called by malloc to record a profiled block.</code></span>
<span class="codeline" id="line-340"><code>func mProf_Malloc(p unsafe.Pointer, size uintptr) {</code></span>
<span class="codeline" id="line-341"><code>	var stk [maxStack]uintptr</code></span>
<span class="codeline" id="line-342"><code>	nstk := callers(4, stk[:])</code></span>
<span class="codeline" id="line-343"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-344"><code>	b := stkbucket(memProfile, size, stk[:nstk], true)</code></span>
<span class="codeline" id="line-345"><code>	c := mProf.cycle</code></span>
<span class="codeline" id="line-346"><code>	mp := b.mp()</code></span>
<span class="codeline" id="line-347"><code>	mpc := &amp;mp.future[(c+2)%uint32(len(mp.future))]</code></span>
<span class="codeline" id="line-348"><code>	mpc.allocs++</code></span>
<span class="codeline" id="line-349"><code>	mpc.alloc_bytes += size</code></span>
<span class="codeline" id="line-350"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-351"><code></code></span>
<span class="codeline" id="line-352"><code>	// Setprofilebucket locks a bunch of other mutexes, so we call it outside of proflock.</code></span>
<span class="codeline" id="line-353"><code>	// This reduces potential contention and chances of deadlocks.</code></span>
<span class="codeline" id="line-354"><code>	// Since the object must be alive during call to mProf_Malloc,</code></span>
<span class="codeline" id="line-355"><code>	// it's fine to do this non-atomically.</code></span>
<span class="codeline" id="line-356"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-357"><code>		setprofilebucket(p, b)</code></span>
<span class="codeline" id="line-358"><code>	})</code></span>
<span class="codeline" id="line-359"><code>}</code></span>
<span class="codeline" id="line-360"><code></code></span>
<span class="codeline" id="line-361"><code>// Called when freeing a profiled block.</code></span>
<span class="codeline" id="line-362"><code>func mProf_Free(b *bucket, size uintptr) {</code></span>
<span class="codeline" id="line-363"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-364"><code>	c := mProf.cycle</code></span>
<span class="codeline" id="line-365"><code>	mp := b.mp()</code></span>
<span class="codeline" id="line-366"><code>	mpc := &amp;mp.future[(c+1)%uint32(len(mp.future))]</code></span>
<span class="codeline" id="line-367"><code>	mpc.frees++</code></span>
<span class="codeline" id="line-368"><code>	mpc.free_bytes += size</code></span>
<span class="codeline" id="line-369"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-370"><code>}</code></span>
<span class="codeline" id="line-371"><code></code></span>
<span class="codeline" id="line-372"><code>var blockprofilerate uint64 // in CPU ticks</code></span>
<span class="codeline" id="line-373"><code></code></span>
<span class="codeline" id="line-374"><code>// SetBlockProfileRate controls the fraction of goroutine blocking events</code></span>
<span class="codeline" id="line-375"><code>// that are reported in the blocking profile. The profiler aims to sample</code></span>
<span class="codeline" id="line-376"><code>// an average of one blocking event per rate nanoseconds spent blocked.</code></span>
<span class="codeline" id="line-377"><code>//</code></span>
<span class="codeline" id="line-378"><code>// To include every blocking event in the profile, pass rate = 1.</code></span>
<span class="codeline" id="line-379"><code>// To turn off profiling entirely, pass rate &lt;= 0.</code></span>
<span class="codeline" id="line-380"><code>func SetBlockProfileRate(rate int) {</code></span>
<span class="codeline" id="line-381"><code>	var r int64</code></span>
<span class="codeline" id="line-382"><code>	if rate &lt;= 0 {</code></span>
<span class="codeline" id="line-383"><code>		r = 0 // disable profiling</code></span>
<span class="codeline" id="line-384"><code>	} else if rate == 1 {</code></span>
<span class="codeline" id="line-385"><code>		r = 1 // profile everything</code></span>
<span class="codeline" id="line-386"><code>	} else {</code></span>
<span class="codeline" id="line-387"><code>		// convert ns to cycles, use float64 to prevent overflow during multiplication</code></span>
<span class="codeline" id="line-388"><code>		r = int64(float64(rate) * float64(tickspersecond()) / (1000 * 1000 * 1000))</code></span>
<span class="codeline" id="line-389"><code>		if r == 0 {</code></span>
<span class="codeline" id="line-390"><code>			r = 1</code></span>
<span class="codeline" id="line-391"><code>		}</code></span>
<span class="codeline" id="line-392"><code>	}</code></span>
<span class="codeline" id="line-393"><code></code></span>
<span class="codeline" id="line-394"><code>	atomic.Store64(&amp;blockprofilerate, uint64(r))</code></span>
<span class="codeline" id="line-395"><code>}</code></span>
<span class="codeline" id="line-396"><code></code></span>
<span class="codeline" id="line-397"><code>func blockevent(cycles int64, skip int) {</code></span>
<span class="codeline" id="line-398"><code>	if cycles &lt;= 0 {</code></span>
<span class="codeline" id="line-399"><code>		cycles = 1</code></span>
<span class="codeline" id="line-400"><code>	}</code></span>
<span class="codeline" id="line-401"><code>	if blocksampled(cycles) {</code></span>
<span class="codeline" id="line-402"><code>		saveblockevent(cycles, skip+1, blockProfile)</code></span>
<span class="codeline" id="line-403"><code>	}</code></span>
<span class="codeline" id="line-404"><code>}</code></span>
<span class="codeline" id="line-405"><code></code></span>
<span class="codeline" id="line-406"><code>func blocksampled(cycles int64) bool {</code></span>
<span class="codeline" id="line-407"><code>	rate := int64(atomic.Load64(&amp;blockprofilerate))</code></span>
<span class="codeline" id="line-408"><code>	if rate &lt;= 0 || (rate &gt; cycles &amp;&amp; int64(fastrand())%rate &gt; cycles) {</code></span>
<span class="codeline" id="line-409"><code>		return false</code></span>
<span class="codeline" id="line-410"><code>	}</code></span>
<span class="codeline" id="line-411"><code>	return true</code></span>
<span class="codeline" id="line-412"><code>}</code></span>
<span class="codeline" id="line-413"><code></code></span>
<span class="codeline" id="line-414"><code>func saveblockevent(cycles int64, skip int, which bucketType) {</code></span>
<span class="codeline" id="line-415"><code>	gp := getg()</code></span>
<span class="codeline" id="line-416"><code>	var nstk int</code></span>
<span class="codeline" id="line-417"><code>	var stk [maxStack]uintptr</code></span>
<span class="codeline" id="line-418"><code>	if gp.m.curg == nil || gp.m.curg == gp {</code></span>
<span class="codeline" id="line-419"><code>		nstk = callers(skip, stk[:])</code></span>
<span class="codeline" id="line-420"><code>	} else {</code></span>
<span class="codeline" id="line-421"><code>		nstk = gcallers(gp.m.curg, skip, stk[:])</code></span>
<span class="codeline" id="line-422"><code>	}</code></span>
<span class="codeline" id="line-423"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-424"><code>	b := stkbucket(which, 0, stk[:nstk], true)</code></span>
<span class="codeline" id="line-425"><code>	b.bp().count++</code></span>
<span class="codeline" id="line-426"><code>	b.bp().cycles += cycles</code></span>
<span class="codeline" id="line-427"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-428"><code>}</code></span>
<span class="codeline" id="line-429"><code></code></span>
<span class="codeline" id="line-430"><code>var mutexprofilerate uint64 // fraction sampled</code></span>
<span class="codeline" id="line-431"><code></code></span>
<span class="codeline" id="line-432"><code>// SetMutexProfileFraction controls the fraction of mutex contention events</code></span>
<span class="codeline" id="line-433"><code>// that are reported in the mutex profile. On average 1/rate events are</code></span>
<span class="codeline" id="line-434"><code>// reported. The previous rate is returned.</code></span>
<span class="codeline" id="line-435"><code>//</code></span>
<span class="codeline" id="line-436"><code>// To turn off profiling entirely, pass rate 0.</code></span>
<span class="codeline" id="line-437"><code>// To just read the current rate, pass rate &lt; 0.</code></span>
<span class="codeline" id="line-438"><code>// (For n&gt;1 the details of sampling may change.)</code></span>
<span class="codeline" id="line-439"><code>func SetMutexProfileFraction(rate int) int {</code></span>
<span class="codeline" id="line-440"><code>	if rate &lt; 0 {</code></span>
<span class="codeline" id="line-441"><code>		return int(mutexprofilerate)</code></span>
<span class="codeline" id="line-442"><code>	}</code></span>
<span class="codeline" id="line-443"><code>	old := mutexprofilerate</code></span>
<span class="codeline" id="line-444"><code>	atomic.Store64(&amp;mutexprofilerate, uint64(rate))</code></span>
<span class="codeline" id="line-445"><code>	return int(old)</code></span>
<span class="codeline" id="line-446"><code>}</code></span>
<span class="codeline" id="line-447"><code></code></span>
<span class="codeline" id="line-448"><code>//go:linkname mutexevent sync.event</code></span>
<span class="codeline" id="line-449"><code>func mutexevent(cycles int64, skip int) {</code></span>
<span class="codeline" id="line-450"><code>	if cycles &lt; 0 {</code></span>
<span class="codeline" id="line-451"><code>		cycles = 0</code></span>
<span class="codeline" id="line-452"><code>	}</code></span>
<span class="codeline" id="line-453"><code>	rate := int64(atomic.Load64(&amp;mutexprofilerate))</code></span>
<span class="codeline" id="line-454"><code>	// TODO(pjw): measure impact of always calling fastrand vs using something</code></span>
<span class="codeline" id="line-455"><code>	// like malloc.go:nextSample()</code></span>
<span class="codeline" id="line-456"><code>	if rate &gt; 0 &amp;&amp; int64(fastrand())%rate == 0 {</code></span>
<span class="codeline" id="line-457"><code>		saveblockevent(cycles, skip+1, mutexProfile)</code></span>
<span class="codeline" id="line-458"><code>	}</code></span>
<span class="codeline" id="line-459"><code>}</code></span>
<span class="codeline" id="line-460"><code></code></span>
<span class="codeline" id="line-461"><code>// Go interface to profile data.</code></span>
<span class="codeline" id="line-462"><code></code></span>
<span class="codeline" id="line-463"><code>// A StackRecord describes a single execution stack.</code></span>
<span class="codeline" id="line-464"><code>type StackRecord struct {</code></span>
<span class="codeline" id="line-465"><code>	Stack0 [32]uintptr // stack trace for this record; ends at first 0 entry</code></span>
<span class="codeline" id="line-466"><code>}</code></span>
<span class="codeline" id="line-467"><code></code></span>
<span class="codeline" id="line-468"><code>// Stack returns the stack trace associated with the record,</code></span>
<span class="codeline" id="line-469"><code>// a prefix of r.Stack0.</code></span>
<span class="codeline" id="line-470"><code>func (r *StackRecord) Stack() []uintptr {</code></span>
<span class="codeline" id="line-471"><code>	for i, v := range r.Stack0 {</code></span>
<span class="codeline" id="line-472"><code>		if v == 0 {</code></span>
<span class="codeline" id="line-473"><code>			return r.Stack0[0:i]</code></span>
<span class="codeline" id="line-474"><code>		}</code></span>
<span class="codeline" id="line-475"><code>	}</code></span>
<span class="codeline" id="line-476"><code>	return r.Stack0[0:]</code></span>
<span class="codeline" id="line-477"><code>}</code></span>
<span class="codeline" id="line-478"><code></code></span>
<span class="codeline" id="line-479"><code>// MemProfileRate controls the fraction of memory allocations</code></span>
<span class="codeline" id="line-480"><code>// that are recorded and reported in the memory profile.</code></span>
<span class="codeline" id="line-481"><code>// The profiler aims to sample an average of</code></span>
<span class="codeline" id="line-482"><code>// one allocation per MemProfileRate bytes allocated.</code></span>
<span class="codeline" id="line-483"><code>//</code></span>
<span class="codeline" id="line-484"><code>// To include every allocated block in the profile, set MemProfileRate to 1.</code></span>
<span class="codeline" id="line-485"><code>// To turn off profiling entirely, set MemProfileRate to 0.</code></span>
<span class="codeline" id="line-486"><code>//</code></span>
<span class="codeline" id="line-487"><code>// The tools that process the memory profiles assume that the</code></span>
<span class="codeline" id="line-488"><code>// profile rate is constant across the lifetime of the program</code></span>
<span class="codeline" id="line-489"><code>// and equal to the current value. Programs that change the</code></span>
<span class="codeline" id="line-490"><code>// memory profiling rate should do so just once, as early as</code></span>
<span class="codeline" id="line-491"><code>// possible in the execution of the program (for example,</code></span>
<span class="codeline" id="line-492"><code>// at the beginning of main).</code></span>
<span class="codeline" id="line-493"><code>var MemProfileRate int = 512 * 1024</code></span>
<span class="codeline" id="line-494"><code></code></span>
<span class="codeline" id="line-495"><code>// A MemProfileRecord describes the live objects allocated</code></span>
<span class="codeline" id="line-496"><code>// by a particular call sequence (stack trace).</code></span>
<span class="codeline" id="line-497"><code>type MemProfileRecord struct {</code></span>
<span class="codeline" id="line-498"><code>	AllocBytes, FreeBytes     int64       // number of bytes allocated, freed</code></span>
<span class="codeline" id="line-499"><code>	AllocObjects, FreeObjects int64       // number of objects allocated, freed</code></span>
<span class="codeline" id="line-500"><code>	Stack0                    [32]uintptr // stack trace for this record; ends at first 0 entry</code></span>
<span class="codeline" id="line-501"><code>}</code></span>
<span class="codeline" id="line-502"><code></code></span>
<span class="codeline" id="line-503"><code>// InUseBytes returns the number of bytes in use (AllocBytes - FreeBytes).</code></span>
<span class="codeline" id="line-504"><code>func (r *MemProfileRecord) InUseBytes() int64 { return r.AllocBytes - r.FreeBytes }</code></span>
<span class="codeline" id="line-505"><code></code></span>
<span class="codeline" id="line-506"><code>// InUseObjects returns the number of objects in use (AllocObjects - FreeObjects).</code></span>
<span class="codeline" id="line-507"><code>func (r *MemProfileRecord) InUseObjects() int64 {</code></span>
<span class="codeline" id="line-508"><code>	return r.AllocObjects - r.FreeObjects</code></span>
<span class="codeline" id="line-509"><code>}</code></span>
<span class="codeline" id="line-510"><code></code></span>
<span class="codeline" id="line-511"><code>// Stack returns the stack trace associated with the record,</code></span>
<span class="codeline" id="line-512"><code>// a prefix of r.Stack0.</code></span>
<span class="codeline" id="line-513"><code>func (r *MemProfileRecord) Stack() []uintptr {</code></span>
<span class="codeline" id="line-514"><code>	for i, v := range r.Stack0 {</code></span>
<span class="codeline" id="line-515"><code>		if v == 0 {</code></span>
<span class="codeline" id="line-516"><code>			return r.Stack0[0:i]</code></span>
<span class="codeline" id="line-517"><code>		}</code></span>
<span class="codeline" id="line-518"><code>	}</code></span>
<span class="codeline" id="line-519"><code>	return r.Stack0[0:]</code></span>
<span class="codeline" id="line-520"><code>}</code></span>
<span class="codeline" id="line-521"><code></code></span>
<span class="codeline" id="line-522"><code>// MemProfile returns a profile of memory allocated and freed per allocation</code></span>
<span class="codeline" id="line-523"><code>// site.</code></span>
<span class="codeline" id="line-524"><code>//</code></span>
<span class="codeline" id="line-525"><code>// MemProfile returns n, the number of records in the current memory profile.</code></span>
<span class="codeline" id="line-526"><code>// If len(p) &gt;= n, MemProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-527"><code>// If len(p) &lt; n, MemProfile does not change p and returns n, false.</code></span>
<span class="codeline" id="line-528"><code>//</code></span>
<span class="codeline" id="line-529"><code>// If inuseZero is true, the profile includes allocation records</code></span>
<span class="codeline" id="line-530"><code>// where r.AllocBytes &gt; 0 but r.AllocBytes == r.FreeBytes.</code></span>
<span class="codeline" id="line-531"><code>// These are sites where memory was allocated, but it has all</code></span>
<span class="codeline" id="line-532"><code>// been released back to the runtime.</code></span>
<span class="codeline" id="line-533"><code>//</code></span>
<span class="codeline" id="line-534"><code>// The returned profile may be up to two garbage collection cycles old.</code></span>
<span class="codeline" id="line-535"><code>// This is to avoid skewing the profile toward allocations; because</code></span>
<span class="codeline" id="line-536"><code>// allocations happen in real time but frees are delayed until the garbage</code></span>
<span class="codeline" id="line-537"><code>// collector performs sweeping, the profile only accounts for allocations</code></span>
<span class="codeline" id="line-538"><code>// that have had a chance to be freed by the garbage collector.</code></span>
<span class="codeline" id="line-539"><code>//</code></span>
<span class="codeline" id="line-540"><code>// Most clients should use the runtime/pprof package or</code></span>
<span class="codeline" id="line-541"><code>// the testing package's -test.memprofile flag instead</code></span>
<span class="codeline" id="line-542"><code>// of calling MemProfile directly.</code></span>
<span class="codeline" id="line-543"><code>func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool) {</code></span>
<span class="codeline" id="line-544"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-545"><code>	// If we're between mProf_NextCycle and mProf_Flush, take care</code></span>
<span class="codeline" id="line-546"><code>	// of flushing to the active profile so we only have to look</code></span>
<span class="codeline" id="line-547"><code>	// at the active profile below.</code></span>
<span class="codeline" id="line-548"><code>	mProf_FlushLocked()</code></span>
<span class="codeline" id="line-549"><code>	clear := true</code></span>
<span class="codeline" id="line-550"><code>	for b := mbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-551"><code>		mp := b.mp()</code></span>
<span class="codeline" id="line-552"><code>		if inuseZero || mp.active.alloc_bytes != mp.active.free_bytes {</code></span>
<span class="codeline" id="line-553"><code>			n++</code></span>
<span class="codeline" id="line-554"><code>		}</code></span>
<span class="codeline" id="line-555"><code>		if mp.active.allocs != 0 || mp.active.frees != 0 {</code></span>
<span class="codeline" id="line-556"><code>			clear = false</code></span>
<span class="codeline" id="line-557"><code>		}</code></span>
<span class="codeline" id="line-558"><code>	}</code></span>
<span class="codeline" id="line-559"><code>	if clear {</code></span>
<span class="codeline" id="line-560"><code>		// Absolutely no data, suggesting that a garbage collection</code></span>
<span class="codeline" id="line-561"><code>		// has not yet happened. In order to allow profiling when</code></span>
<span class="codeline" id="line-562"><code>		// garbage collection is disabled from the beginning of execution,</code></span>
<span class="codeline" id="line-563"><code>		// accumulate all of the cycles, and recount buckets.</code></span>
<span class="codeline" id="line-564"><code>		n = 0</code></span>
<span class="codeline" id="line-565"><code>		for b := mbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-566"><code>			mp := b.mp()</code></span>
<span class="codeline" id="line-567"><code>			for c := range mp.future {</code></span>
<span class="codeline" id="line-568"><code>				mp.active.add(&amp;mp.future[c])</code></span>
<span class="codeline" id="line-569"><code>				mp.future[c] = memRecordCycle{}</code></span>
<span class="codeline" id="line-570"><code>			}</code></span>
<span class="codeline" id="line-571"><code>			if inuseZero || mp.active.alloc_bytes != mp.active.free_bytes {</code></span>
<span class="codeline" id="line-572"><code>				n++</code></span>
<span class="codeline" id="line-573"><code>			}</code></span>
<span class="codeline" id="line-574"><code>		}</code></span>
<span class="codeline" id="line-575"><code>	}</code></span>
<span class="codeline" id="line-576"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-577"><code>		ok = true</code></span>
<span class="codeline" id="line-578"><code>		idx := 0</code></span>
<span class="codeline" id="line-579"><code>		for b := mbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-580"><code>			mp := b.mp()</code></span>
<span class="codeline" id="line-581"><code>			if inuseZero || mp.active.alloc_bytes != mp.active.free_bytes {</code></span>
<span class="codeline" id="line-582"><code>				record(&amp;p[idx], b)</code></span>
<span class="codeline" id="line-583"><code>				idx++</code></span>
<span class="codeline" id="line-584"><code>			}</code></span>
<span class="codeline" id="line-585"><code>		}</code></span>
<span class="codeline" id="line-586"><code>	}</code></span>
<span class="codeline" id="line-587"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-588"><code>	return</code></span>
<span class="codeline" id="line-589"><code>}</code></span>
<span class="codeline" id="line-590"><code></code></span>
<span class="codeline" id="line-591"><code>// Write b's data to r.</code></span>
<span class="codeline" id="line-592"><code>func record(r *MemProfileRecord, b *bucket) {</code></span>
<span class="codeline" id="line-593"><code>	mp := b.mp()</code></span>
<span class="codeline" id="line-594"><code>	r.AllocBytes = int64(mp.active.alloc_bytes)</code></span>
<span class="codeline" id="line-595"><code>	r.FreeBytes = int64(mp.active.free_bytes)</code></span>
<span class="codeline" id="line-596"><code>	r.AllocObjects = int64(mp.active.allocs)</code></span>
<span class="codeline" id="line-597"><code>	r.FreeObjects = int64(mp.active.frees)</code></span>
<span class="codeline" id="line-598"><code>	if raceenabled {</code></span>
<span class="codeline" id="line-599"><code>		racewriterangepc(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0), getcallerpc(), funcPC(MemProfile))</code></span>
<span class="codeline" id="line-600"><code>	}</code></span>
<span class="codeline" id="line-601"><code>	if msanenabled {</code></span>
<span class="codeline" id="line-602"><code>		msanwrite(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0))</code></span>
<span class="codeline" id="line-603"><code>	}</code></span>
<span class="codeline" id="line-604"><code>	copy(r.Stack0[:], b.stk())</code></span>
<span class="codeline" id="line-605"><code>	for i := int(b.nstk); i &lt; len(r.Stack0); i++ {</code></span>
<span class="codeline" id="line-606"><code>		r.Stack0[i] = 0</code></span>
<span class="codeline" id="line-607"><code>	}</code></span>
<span class="codeline" id="line-608"><code>}</code></span>
<span class="codeline" id="line-609"><code></code></span>
<span class="codeline" id="line-610"><code>func iterate_memprof(fn func(*bucket, uintptr, *uintptr, uintptr, uintptr, uintptr)) {</code></span>
<span class="codeline" id="line-611"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-612"><code>	for b := mbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-613"><code>		mp := b.mp()</code></span>
<span class="codeline" id="line-614"><code>		fn(b, b.nstk, &amp;b.stk()[0], b.size, mp.active.allocs, mp.active.frees)</code></span>
<span class="codeline" id="line-615"><code>	}</code></span>
<span class="codeline" id="line-616"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-617"><code>}</code></span>
<span class="codeline" id="line-618"><code></code></span>
<span class="codeline" id="line-619"><code>// BlockProfileRecord describes blocking events originated</code></span>
<span class="codeline" id="line-620"><code>// at a particular call sequence (stack trace).</code></span>
<span class="codeline" id="line-621"><code>type BlockProfileRecord struct {</code></span>
<span class="codeline" id="line-622"><code>	Count  int64</code></span>
<span class="codeline" id="line-623"><code>	Cycles int64</code></span>
<span class="codeline" id="line-624"><code>	StackRecord</code></span>
<span class="codeline" id="line-625"><code>}</code></span>
<span class="codeline" id="line-626"><code></code></span>
<span class="codeline" id="line-627"><code>// BlockProfile returns n, the number of records in the current blocking profile.</code></span>
<span class="codeline" id="line-628"><code>// If len(p) &gt;= n, BlockProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-629"><code>// If len(p) &lt; n, BlockProfile does not change p and returns n, false.</code></span>
<span class="codeline" id="line-630"><code>//</code></span>
<span class="codeline" id="line-631"><code>// Most clients should use the runtime/pprof package or</code></span>
<span class="codeline" id="line-632"><code>// the testing package's -test.blockprofile flag instead</code></span>
<span class="codeline" id="line-633"><code>// of calling BlockProfile directly.</code></span>
<span class="codeline" id="line-634"><code>func BlockProfile(p []BlockProfileRecord) (n int, ok bool) {</code></span>
<span class="codeline" id="line-635"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-636"><code>	for b := bbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-637"><code>		n++</code></span>
<span class="codeline" id="line-638"><code>	}</code></span>
<span class="codeline" id="line-639"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-640"><code>		ok = true</code></span>
<span class="codeline" id="line-641"><code>		for b := bbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-642"><code>			bp := b.bp()</code></span>
<span class="codeline" id="line-643"><code>			r := &amp;p[0]</code></span>
<span class="codeline" id="line-644"><code>			r.Count = bp.count</code></span>
<span class="codeline" id="line-645"><code>			r.Cycles = bp.cycles</code></span>
<span class="codeline" id="line-646"><code>			if raceenabled {</code></span>
<span class="codeline" id="line-647"><code>				racewriterangepc(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0), getcallerpc(), funcPC(BlockProfile))</code></span>
<span class="codeline" id="line-648"><code>			}</code></span>
<span class="codeline" id="line-649"><code>			if msanenabled {</code></span>
<span class="codeline" id="line-650"><code>				msanwrite(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0))</code></span>
<span class="codeline" id="line-651"><code>			}</code></span>
<span class="codeline" id="line-652"><code>			i := copy(r.Stack0[:], b.stk())</code></span>
<span class="codeline" id="line-653"><code>			for ; i &lt; len(r.Stack0); i++ {</code></span>
<span class="codeline" id="line-654"><code>				r.Stack0[i] = 0</code></span>
<span class="codeline" id="line-655"><code>			}</code></span>
<span class="codeline" id="line-656"><code>			p = p[1:]</code></span>
<span class="codeline" id="line-657"><code>		}</code></span>
<span class="codeline" id="line-658"><code>	}</code></span>
<span class="codeline" id="line-659"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-660"><code>	return</code></span>
<span class="codeline" id="line-661"><code>}</code></span>
<span class="codeline" id="line-662"><code></code></span>
<span class="codeline" id="line-663"><code>// MutexProfile returns n, the number of records in the current mutex profile.</code></span>
<span class="codeline" id="line-664"><code>// If len(p) &gt;= n, MutexProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-665"><code>// Otherwise, MutexProfile does not change p, and returns n, false.</code></span>
<span class="codeline" id="line-666"><code>//</code></span>
<span class="codeline" id="line-667"><code>// Most clients should use the runtime/pprof package</code></span>
<span class="codeline" id="line-668"><code>// instead of calling MutexProfile directly.</code></span>
<span class="codeline" id="line-669"><code>func MutexProfile(p []BlockProfileRecord) (n int, ok bool) {</code></span>
<span class="codeline" id="line-670"><code>	lock(&amp;proflock)</code></span>
<span class="codeline" id="line-671"><code>	for b := xbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-672"><code>		n++</code></span>
<span class="codeline" id="line-673"><code>	}</code></span>
<span class="codeline" id="line-674"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-675"><code>		ok = true</code></span>
<span class="codeline" id="line-676"><code>		for b := xbuckets; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-677"><code>			bp := b.bp()</code></span>
<span class="codeline" id="line-678"><code>			r := &amp;p[0]</code></span>
<span class="codeline" id="line-679"><code>			r.Count = int64(bp.count)</code></span>
<span class="codeline" id="line-680"><code>			r.Cycles = bp.cycles</code></span>
<span class="codeline" id="line-681"><code>			i := copy(r.Stack0[:], b.stk())</code></span>
<span class="codeline" id="line-682"><code>			for ; i &lt; len(r.Stack0); i++ {</code></span>
<span class="codeline" id="line-683"><code>				r.Stack0[i] = 0</code></span>
<span class="codeline" id="line-684"><code>			}</code></span>
<span class="codeline" id="line-685"><code>			p = p[1:]</code></span>
<span class="codeline" id="line-686"><code>		}</code></span>
<span class="codeline" id="line-687"><code>	}</code></span>
<span class="codeline" id="line-688"><code>	unlock(&amp;proflock)</code></span>
<span class="codeline" id="line-689"><code>	return</code></span>
<span class="codeline" id="line-690"><code>}</code></span>
<span class="codeline" id="line-691"><code></code></span>
<span class="codeline" id="line-692"><code>// ThreadCreateProfile returns n, the number of records in the thread creation profile.</code></span>
<span class="codeline" id="line-693"><code>// If len(p) &gt;= n, ThreadCreateProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-694"><code>// If len(p) &lt; n, ThreadCreateProfile does not change p and returns n, false.</code></span>
<span class="codeline" id="line-695"><code>//</code></span>
<span class="codeline" id="line-696"><code>// Most clients should use the runtime/pprof package instead</code></span>
<span class="codeline" id="line-697"><code>// of calling ThreadCreateProfile directly.</code></span>
<span class="codeline" id="line-698"><code>func ThreadCreateProfile(p []StackRecord) (n int, ok bool) {</code></span>
<span class="codeline" id="line-699"><code>	first := (*m)(atomic.Loadp(unsafe.Pointer(&amp;allm)))</code></span>
<span class="codeline" id="line-700"><code>	for mp := first; mp != nil; mp = mp.alllink {</code></span>
<span class="codeline" id="line-701"><code>		n++</code></span>
<span class="codeline" id="line-702"><code>	}</code></span>
<span class="codeline" id="line-703"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-704"><code>		ok = true</code></span>
<span class="codeline" id="line-705"><code>		i := 0</code></span>
<span class="codeline" id="line-706"><code>		for mp := first; mp != nil; mp = mp.alllink {</code></span>
<span class="codeline" id="line-707"><code>			p[i].Stack0 = mp.createstack</code></span>
<span class="codeline" id="line-708"><code>			i++</code></span>
<span class="codeline" id="line-709"><code>		}</code></span>
<span class="codeline" id="line-710"><code>	}</code></span>
<span class="codeline" id="line-711"><code>	return</code></span>
<span class="codeline" id="line-712"><code>}</code></span>
<span class="codeline" id="line-713"><code></code></span>
<span class="codeline" id="line-714"><code>//go:linkname runtime_goroutineProfileWithLabels runtime/pprof.runtime_goroutineProfileWithLabels</code></span>
<span class="codeline" id="line-715"><code>func runtime_goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool) {</code></span>
<span class="codeline" id="line-716"><code>	return goroutineProfileWithLabels(p, labels)</code></span>
<span class="codeline" id="line-717"><code>}</code></span>
<span class="codeline" id="line-718"><code></code></span>
<span class="codeline" id="line-719"><code>// labels may be nil. If labels is non-nil, it must have the same length as p.</code></span>
<span class="codeline" id="line-720"><code>func goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool) {</code></span>
<span class="codeline" id="line-721"><code>	if labels != nil &amp;&amp; len(labels) != len(p) {</code></span>
<span class="codeline" id="line-722"><code>		labels = nil</code></span>
<span class="codeline" id="line-723"><code>	}</code></span>
<span class="codeline" id="line-724"><code>	gp := getg()</code></span>
<span class="codeline" id="line-725"><code></code></span>
<span class="codeline" id="line-726"><code>	isOK := func(gp1 *g) bool {</code></span>
<span class="codeline" id="line-727"><code>		// Checking isSystemGoroutine here makes GoroutineProfile</code></span>
<span class="codeline" id="line-728"><code>		// consistent with both NumGoroutine and Stack.</code></span>
<span class="codeline" id="line-729"><code>		return gp1 != gp &amp;&amp; readgstatus(gp1) != _Gdead &amp;&amp; !isSystemGoroutine(gp1, false)</code></span>
<span class="codeline" id="line-730"><code>	}</code></span>
<span class="codeline" id="line-731"><code></code></span>
<span class="codeline" id="line-732"><code>	stopTheWorld("profile")</code></span>
<span class="codeline" id="line-733"><code></code></span>
<span class="codeline" id="line-734"><code>	n = 1</code></span>
<span class="codeline" id="line-735"><code>	for _, gp1 := range allgs {</code></span>
<span class="codeline" id="line-736"><code>		if isOK(gp1) {</code></span>
<span class="codeline" id="line-737"><code>			n++</code></span>
<span class="codeline" id="line-738"><code>		}</code></span>
<span class="codeline" id="line-739"><code>	}</code></span>
<span class="codeline" id="line-740"><code></code></span>
<span class="codeline" id="line-741"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-742"><code>		ok = true</code></span>
<span class="codeline" id="line-743"><code>		r, lbl := p, labels</code></span>
<span class="codeline" id="line-744"><code></code></span>
<span class="codeline" id="line-745"><code>		// Save current goroutine.</code></span>
<span class="codeline" id="line-746"><code>		sp := getcallersp()</code></span>
<span class="codeline" id="line-747"><code>		pc := getcallerpc()</code></span>
<span class="codeline" id="line-748"><code>		systemstack(func() {</code></span>
<span class="codeline" id="line-749"><code>			saveg(pc, sp, gp, &amp;r[0])</code></span>
<span class="codeline" id="line-750"><code>		})</code></span>
<span class="codeline" id="line-751"><code>		r = r[1:]</code></span>
<span class="codeline" id="line-752"><code></code></span>
<span class="codeline" id="line-753"><code>		// If we have a place to put our goroutine labelmap, insert it there.</code></span>
<span class="codeline" id="line-754"><code>		if labels != nil {</code></span>
<span class="codeline" id="line-755"><code>			lbl[0] = gp.labels</code></span>
<span class="codeline" id="line-756"><code>			lbl = lbl[1:]</code></span>
<span class="codeline" id="line-757"><code>		}</code></span>
<span class="codeline" id="line-758"><code></code></span>
<span class="codeline" id="line-759"><code>		// Save other goroutines.</code></span>
<span class="codeline" id="line-760"><code>		for _, gp1 := range allgs {</code></span>
<span class="codeline" id="line-761"><code>			if isOK(gp1) {</code></span>
<span class="codeline" id="line-762"><code>				if len(r) == 0 {</code></span>
<span class="codeline" id="line-763"><code>					// Should be impossible, but better to return a</code></span>
<span class="codeline" id="line-764"><code>					// truncated profile than to crash the entire process.</code></span>
<span class="codeline" id="line-765"><code>					break</code></span>
<span class="codeline" id="line-766"><code>				}</code></span>
<span class="codeline" id="line-767"><code>				saveg(^uintptr(0), ^uintptr(0), gp1, &amp;r[0])</code></span>
<span class="codeline" id="line-768"><code>				if labels != nil {</code></span>
<span class="codeline" id="line-769"><code>					lbl[0] = gp1.labels</code></span>
<span class="codeline" id="line-770"><code>					lbl = lbl[1:]</code></span>
<span class="codeline" id="line-771"><code>				}</code></span>
<span class="codeline" id="line-772"><code>				r = r[1:]</code></span>
<span class="codeline" id="line-773"><code>			}</code></span>
<span class="codeline" id="line-774"><code>		}</code></span>
<span class="codeline" id="line-775"><code>	}</code></span>
<span class="codeline" id="line-776"><code></code></span>
<span class="codeline" id="line-777"><code>	startTheWorld()</code></span>
<span class="codeline" id="line-778"><code>	return n, ok</code></span>
<span class="codeline" id="line-779"><code>}</code></span>
<span class="codeline" id="line-780"><code></code></span>
<span class="codeline" id="line-781"><code>// GoroutineProfile returns n, the number of records in the active goroutine stack profile.</code></span>
<span class="codeline" id="line-782"><code>// If len(p) &gt;= n, GoroutineProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-783"><code>// If len(p) &lt; n, GoroutineProfile does not change p and returns n, false.</code></span>
<span class="codeline" id="line-784"><code>//</code></span>
<span class="codeline" id="line-785"><code>// Most clients should use the runtime/pprof package instead</code></span>
<span class="codeline" id="line-786"><code>// of calling GoroutineProfile directly.</code></span>
<span class="codeline" id="line-787"><code>func GoroutineProfile(p []StackRecord) (n int, ok bool) {</code></span>
<span class="codeline" id="line-788"><code></code></span>
<span class="codeline" id="line-789"><code>	return goroutineProfileWithLabels(p, nil)</code></span>
<span class="codeline" id="line-790"><code>}</code></span>
<span class="codeline" id="line-791"><code></code></span>
<span class="codeline" id="line-792"><code>func saveg(pc, sp uintptr, gp *g, r *StackRecord) {</code></span>
<span class="codeline" id="line-793"><code>	n := gentraceback(pc, sp, 0, gp, 0, &amp;r.Stack0[0], len(r.Stack0), nil, nil, 0)</code></span>
<span class="codeline" id="line-794"><code>	if n &lt; len(r.Stack0) {</code></span>
<span class="codeline" id="line-795"><code>		r.Stack0[n] = 0</code></span>
<span class="codeline" id="line-796"><code>	}</code></span>
<span class="codeline" id="line-797"><code>}</code></span>
<span class="codeline" id="line-798"><code></code></span>
<span class="codeline" id="line-799"><code>// Stack formats a stack trace of the calling goroutine into buf</code></span>
<span class="codeline" id="line-800"><code>// and returns the number of bytes written to buf.</code></span>
<span class="codeline" id="line-801"><code>// If all is true, Stack formats stack traces of all other goroutines</code></span>
<span class="codeline" id="line-802"><code>// into buf after the trace for the current goroutine.</code></span>
<span class="codeline" id="line-803"><code>func Stack(buf []byte, all bool) int {</code></span>
<span class="codeline" id="line-804"><code>	if all {</code></span>
<span class="codeline" id="line-805"><code>		stopTheWorld("stack trace")</code></span>
<span class="codeline" id="line-806"><code>	}</code></span>
<span class="codeline" id="line-807"><code></code></span>
<span class="codeline" id="line-808"><code>	n := 0</code></span>
<span class="codeline" id="line-809"><code>	if len(buf) &gt; 0 {</code></span>
<span class="codeline" id="line-810"><code>		gp := getg()</code></span>
<span class="codeline" id="line-811"><code>		sp := getcallersp()</code></span>
<span class="codeline" id="line-812"><code>		pc := getcallerpc()</code></span>
<span class="codeline" id="line-813"><code>		systemstack(func() {</code></span>
<span class="codeline" id="line-814"><code>			g0 := getg()</code></span>
<span class="codeline" id="line-815"><code>			// Force traceback=1 to override GOTRACEBACK setting,</code></span>
<span class="codeline" id="line-816"><code>			// so that Stack's results are consistent.</code></span>
<span class="codeline" id="line-817"><code>			// GOTRACEBACK is only about crash dumps.</code></span>
<span class="codeline" id="line-818"><code>			g0.m.traceback = 1</code></span>
<span class="codeline" id="line-819"><code>			g0.writebuf = buf[0:0:len(buf)]</code></span>
<span class="codeline" id="line-820"><code>			goroutineheader(gp)</code></span>
<span class="codeline" id="line-821"><code>			traceback(pc, sp, 0, gp)</code></span>
<span class="codeline" id="line-822"><code>			if all {</code></span>
<span class="codeline" id="line-823"><code>				tracebackothers(gp)</code></span>
<span class="codeline" id="line-824"><code>			}</code></span>
<span class="codeline" id="line-825"><code>			g0.m.traceback = 0</code></span>
<span class="codeline" id="line-826"><code>			n = len(g0.writebuf)</code></span>
<span class="codeline" id="line-827"><code>			g0.writebuf = nil</code></span>
<span class="codeline" id="line-828"><code>		})</code></span>
<span class="codeline" id="line-829"><code>	}</code></span>
<span class="codeline" id="line-830"><code></code></span>
<span class="codeline" id="line-831"><code>	if all {</code></span>
<span class="codeline" id="line-832"><code>		startTheWorld()</code></span>
<span class="codeline" id="line-833"><code>	}</code></span>
<span class="codeline" id="line-834"><code>	return n</code></span>
<span class="codeline" id="line-835"><code>}</code></span>
<span class="codeline" id="line-836"><code></code></span>
<span class="codeline" id="line-837"><code>// Tracing of alloc/free/gc.</code></span>
<span class="codeline" id="line-838"><code></code></span>
<span class="codeline" id="line-839"><code>var tracelock mutex</code></span>
<span class="codeline" id="line-840"><code></code></span>
<span class="codeline" id="line-841"><code>func tracealloc(p unsafe.Pointer, size uintptr, typ *_type) {</code></span>
<span class="codeline" id="line-842"><code>	lock(&amp;tracelock)</code></span>
<span class="codeline" id="line-843"><code>	gp := getg()</code></span>
<span class="codeline" id="line-844"><code>	gp.m.traceback = 2</code></span>
<span class="codeline" id="line-845"><code>	if typ == nil {</code></span>
<span class="codeline" id="line-846"><code>		print("tracealloc(", p, ", ", hex(size), ")\n")</code></span>
<span class="codeline" id="line-847"><code>	} else {</code></span>
<span class="codeline" id="line-848"><code>		print("tracealloc(", p, ", ", hex(size), ", ", typ.string(), ")\n")</code></span>
<span class="codeline" id="line-849"><code>	}</code></span>
<span class="codeline" id="line-850"><code>	if gp.m.curg == nil || gp == gp.m.curg {</code></span>
<span class="codeline" id="line-851"><code>		goroutineheader(gp)</code></span>
<span class="codeline" id="line-852"><code>		pc := getcallerpc()</code></span>
<span class="codeline" id="line-853"><code>		sp := getcallersp()</code></span>
<span class="codeline" id="line-854"><code>		systemstack(func() {</code></span>
<span class="codeline" id="line-855"><code>			traceback(pc, sp, 0, gp)</code></span>
<span class="codeline" id="line-856"><code>		})</code></span>
<span class="codeline" id="line-857"><code>	} else {</code></span>
<span class="codeline" id="line-858"><code>		goroutineheader(gp.m.curg)</code></span>
<span class="codeline" id="line-859"><code>		traceback(^uintptr(0), ^uintptr(0), 0, gp.m.curg)</code></span>
<span class="codeline" id="line-860"><code>	}</code></span>
<span class="codeline" id="line-861"><code>	print("\n")</code></span>
<span class="codeline" id="line-862"><code>	gp.m.traceback = 0</code></span>
<span class="codeline" id="line-863"><code>	unlock(&amp;tracelock)</code></span>
<span class="codeline" id="line-864"><code>}</code></span>
<span class="codeline" id="line-865"><code></code></span>
<span class="codeline" id="line-866"><code>func tracefree(p unsafe.Pointer, size uintptr) {</code></span>
<span class="codeline" id="line-867"><code>	lock(&amp;tracelock)</code></span>
<span class="codeline" id="line-868"><code>	gp := getg()</code></span>
<span class="codeline" id="line-869"><code>	gp.m.traceback = 2</code></span>
<span class="codeline" id="line-870"><code>	print("tracefree(", p, ", ", hex(size), ")\n")</code></span>
<span class="codeline" id="line-871"><code>	goroutineheader(gp)</code></span>
<span class="codeline" id="line-872"><code>	pc := getcallerpc()</code></span>
<span class="codeline" id="line-873"><code>	sp := getcallersp()</code></span>
<span class="codeline" id="line-874"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-875"><code>		traceback(pc, sp, 0, gp)</code></span>
<span class="codeline" id="line-876"><code>	})</code></span>
<span class="codeline" id="line-877"><code>	print("\n")</code></span>
<span class="codeline" id="line-878"><code>	gp.m.traceback = 0</code></span>
<span class="codeline" id="line-879"><code>	unlock(&amp;tracelock)</code></span>
<span class="codeline" id="line-880"><code>}</code></span>
<span class="codeline" id="line-881"><code></code></span>
<span class="codeline" id="line-882"><code>func tracegc() {</code></span>
<span class="codeline" id="line-883"><code>	lock(&amp;tracelock)</code></span>
<span class="codeline" id="line-884"><code>	gp := getg()</code></span>
<span class="codeline" id="line-885"><code>	gp.m.traceback = 2</code></span>
<span class="codeline" id="line-886"><code>	print("tracegc()\n")</code></span>
<span class="codeline" id="line-887"><code>	// running on m-&gt;g0 stack; show all non-g0 goroutines</code></span>
<span class="codeline" id="line-888"><code>	tracebackothers(gp)</code></span>
<span class="codeline" id="line-889"><code>	print("end tracegc\n")</code></span>
<span class="codeline" id="line-890"><code>	print("\n")</code></span>
<span class="codeline" id="line-891"><code>	gp.m.traceback = 0</code></span>
<span class="codeline" id="line-892"><code>	unlock(&amp;tracelock)</code></span>
<span class="codeline" id="line-893"><code>}</code></span>
</pre><pre id="footer">
<table><tr><td><img src="../../png/go101-twitter.png"></td>
<td>The pages are generated with <a href="https://go101.org/article/tool-golds.html"><b>Golds</b></a> <i>v0.3.2</i>. (GOOS=linux GOARCH=amd64)
<b>Golds</b> is a <a href="https://go101.org">Go 101</a> project developed by <a href="https://tapirgames.com">Tapir Liu</a>.
PR and bug reports are welcome and can be submitted to <a href="https://github.com/go101/golds">the issue list</a>.
Please follow <a href="https://twitter.com/go100and1">@Go100and1</a> (reachable from the left QR code) to get the latest news of <b>Golds</b>.</td></tr></table></pre>
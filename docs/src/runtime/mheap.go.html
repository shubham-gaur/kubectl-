<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Source: mheap.go in package runtime</title>
<link href="../../css/light-v0.3.2.css" rel="stylesheet">
<script src="../../jvs/golds-v0.3.2.js"></script>
<body onload="onPageLoad()"><div>

<pre id="header"><code><span class="title">Source File</span>
	mheap.go

<span class="title">Belonging Package</span>
	<a href="../../pkg/runtime.html">runtime</a>
</code></pre>

<pre class="line-numbers">
<span class="codeline" id="line-1"><code>// Copyright 2009 The Go Authors. All rights reserved.</code></span>
<span class="codeline" id="line-2"><code>// Use of this source code is governed by a BSD-style</code></span>
<span class="codeline" id="line-3"><code>// license that can be found in the LICENSE file.</code></span>
<span class="codeline" id="line-4"><code></code></span>
<span class="codeline" id="line-5"><code>// Page heap.</code></span>
<span class="codeline" id="line-6"><code>//</code></span>
<span class="codeline" id="line-7"><code>// See malloc.go for overview.</code></span>
<span class="codeline" id="line-8"><code></code></span>
<span class="codeline" id="line-9"><code>package runtime</code></span>
<span class="codeline" id="line-10"><code></code></span>
<span class="codeline" id="line-11"><code>import (</code></span>
<span class="codeline" id="line-12"><code>	"internal/cpu"</code></span>
<span class="codeline" id="line-13"><code>	"runtime/internal/atomic"</code></span>
<span class="codeline" id="line-14"><code>	"runtime/internal/sys"</code></span>
<span class="codeline" id="line-15"><code>	"unsafe"</code></span>
<span class="codeline" id="line-16"><code>)</code></span>
<span class="codeline" id="line-17"><code></code></span>
<span class="codeline" id="line-18"><code>const (</code></span>
<span class="codeline" id="line-19"><code>	// minPhysPageSize is a lower-bound on the physical page size. The</code></span>
<span class="codeline" id="line-20"><code>	// true physical page size may be larger than this. In contrast,</code></span>
<span class="codeline" id="line-21"><code>	// sys.PhysPageSize is an upper-bound on the physical page size.</code></span>
<span class="codeline" id="line-22"><code>	minPhysPageSize = 4096</code></span>
<span class="codeline" id="line-23"><code></code></span>
<span class="codeline" id="line-24"><code>	// maxPhysPageSize is the maximum page size the runtime supports.</code></span>
<span class="codeline" id="line-25"><code>	maxPhysPageSize = 512 &lt;&lt; 10</code></span>
<span class="codeline" id="line-26"><code></code></span>
<span class="codeline" id="line-27"><code>	// maxPhysHugePageSize sets an upper-bound on the maximum huge page size</code></span>
<span class="codeline" id="line-28"><code>	// that the runtime supports.</code></span>
<span class="codeline" id="line-29"><code>	maxPhysHugePageSize = pallocChunkBytes</code></span>
<span class="codeline" id="line-30"><code></code></span>
<span class="codeline" id="line-31"><code>	// pagesPerReclaimerChunk indicates how many pages to scan from the</code></span>
<span class="codeline" id="line-32"><code>	// pageInUse bitmap at a time. Used by the page reclaimer.</code></span>
<span class="codeline" id="line-33"><code>	//</code></span>
<span class="codeline" id="line-34"><code>	// Higher values reduce contention on scanning indexes (such as</code></span>
<span class="codeline" id="line-35"><code>	// h.reclaimIndex), but increase the minimum latency of the</code></span>
<span class="codeline" id="line-36"><code>	// operation.</code></span>
<span class="codeline" id="line-37"><code>	//</code></span>
<span class="codeline" id="line-38"><code>	// The time required to scan this many pages can vary a lot depending</code></span>
<span class="codeline" id="line-39"><code>	// on how many spans are actually freed. Experimentally, it can</code></span>
<span class="codeline" id="line-40"><code>	// scan for pages at ~300 GB/ms on a 2.6GHz Core i7, but can only</code></span>
<span class="codeline" id="line-41"><code>	// free spans at ~32 MB/ms. Using 512 pages bounds this at</code></span>
<span class="codeline" id="line-42"><code>	// roughly 100Âµs.</code></span>
<span class="codeline" id="line-43"><code>	//</code></span>
<span class="codeline" id="line-44"><code>	// Must be a multiple of the pageInUse bitmap element size and</code></span>
<span class="codeline" id="line-45"><code>	// must also evenly divide pagesPerArena.</code></span>
<span class="codeline" id="line-46"><code>	pagesPerReclaimerChunk = 512</code></span>
<span class="codeline" id="line-47"><code></code></span>
<span class="codeline" id="line-48"><code>	// physPageAlignedStacks indicates whether stack allocations must be</code></span>
<span class="codeline" id="line-49"><code>	// physical page aligned. This is a requirement for MAP_STACK on</code></span>
<span class="codeline" id="line-50"><code>	// OpenBSD.</code></span>
<span class="codeline" id="line-51"><code>	physPageAlignedStacks = GOOS == "openbsd"</code></span>
<span class="codeline" id="line-52"><code>)</code></span>
<span class="codeline" id="line-53"><code></code></span>
<span class="codeline" id="line-54"><code>// Main malloc heap.</code></span>
<span class="codeline" id="line-55"><code>// The heap itself is the "free" and "scav" treaps,</code></span>
<span class="codeline" id="line-56"><code>// but all the other global data is here too.</code></span>
<span class="codeline" id="line-57"><code>//</code></span>
<span class="codeline" id="line-58"><code>// mheap must not be heap-allocated because it contains mSpanLists,</code></span>
<span class="codeline" id="line-59"><code>// which must not be heap-allocated.</code></span>
<span class="codeline" id="line-60"><code>//</code></span>
<span class="codeline" id="line-61"><code>//go:notinheap</code></span>
<span class="codeline" id="line-62"><code>type mheap struct {</code></span>
<span class="codeline" id="line-63"><code>	// lock must only be acquired on the system stack, otherwise a g</code></span>
<span class="codeline" id="line-64"><code>	// could self-deadlock if its stack grows with the lock held.</code></span>
<span class="codeline" id="line-65"><code>	lock      mutex</code></span>
<span class="codeline" id="line-66"><code>	pages     pageAlloc // page allocation data structure</code></span>
<span class="codeline" id="line-67"><code>	sweepgen  uint32    // sweep generation, see comment in mspan; written during STW</code></span>
<span class="codeline" id="line-68"><code>	sweepdone uint32    // all spans are swept</code></span>
<span class="codeline" id="line-69"><code>	sweepers  uint32    // number of active sweepone calls</code></span>
<span class="codeline" id="line-70"><code></code></span>
<span class="codeline" id="line-71"><code>	// allspans is a slice of all mspans ever created. Each mspan</code></span>
<span class="codeline" id="line-72"><code>	// appears exactly once.</code></span>
<span class="codeline" id="line-73"><code>	//</code></span>
<span class="codeline" id="line-74"><code>	// The memory for allspans is manually managed and can be</code></span>
<span class="codeline" id="line-75"><code>	// reallocated and move as the heap grows.</code></span>
<span class="codeline" id="line-76"><code>	//</code></span>
<span class="codeline" id="line-77"><code>	// In general, allspans is protected by mheap_.lock, which</code></span>
<span class="codeline" id="line-78"><code>	// prevents concurrent access as well as freeing the backing</code></span>
<span class="codeline" id="line-79"><code>	// store. Accesses during STW might not hold the lock, but</code></span>
<span class="codeline" id="line-80"><code>	// must ensure that allocation cannot happen around the</code></span>
<span class="codeline" id="line-81"><code>	// access (since that may free the backing store).</code></span>
<span class="codeline" id="line-82"><code>	allspans []*mspan // all spans out there</code></span>
<span class="codeline" id="line-83"><code></code></span>
<span class="codeline" id="line-84"><code>	_ uint32 // align uint64 fields on 32-bit for atomics</code></span>
<span class="codeline" id="line-85"><code></code></span>
<span class="codeline" id="line-86"><code>	// Proportional sweep</code></span>
<span class="codeline" id="line-87"><code>	//</code></span>
<span class="codeline" id="line-88"><code>	// These parameters represent a linear function from heap_live</code></span>
<span class="codeline" id="line-89"><code>	// to page sweep count. The proportional sweep system works to</code></span>
<span class="codeline" id="line-90"><code>	// stay in the black by keeping the current page sweep count</code></span>
<span class="codeline" id="line-91"><code>	// above this line at the current heap_live.</code></span>
<span class="codeline" id="line-92"><code>	//</code></span>
<span class="codeline" id="line-93"><code>	// The line has slope sweepPagesPerByte and passes through a</code></span>
<span class="codeline" id="line-94"><code>	// basis point at (sweepHeapLiveBasis, pagesSweptBasis). At</code></span>
<span class="codeline" id="line-95"><code>	// any given time, the system is at (memstats.heap_live,</code></span>
<span class="codeline" id="line-96"><code>	// pagesSwept) in this space.</code></span>
<span class="codeline" id="line-97"><code>	//</code></span>
<span class="codeline" id="line-98"><code>	// It's important that the line pass through a point we</code></span>
<span class="codeline" id="line-99"><code>	// control rather than simply starting at a (0,0) origin</code></span>
<span class="codeline" id="line-100"><code>	// because that lets us adjust sweep pacing at any time while</code></span>
<span class="codeline" id="line-101"><code>	// accounting for current progress. If we could only adjust</code></span>
<span class="codeline" id="line-102"><code>	// the slope, it would create a discontinuity in debt if any</code></span>
<span class="codeline" id="line-103"><code>	// progress has already been made.</code></span>
<span class="codeline" id="line-104"><code>	pagesInUse         uint64  // pages of spans in stats mSpanInUse; updated atomically</code></span>
<span class="codeline" id="line-105"><code>	pagesSwept         uint64  // pages swept this cycle; updated atomically</code></span>
<span class="codeline" id="line-106"><code>	pagesSweptBasis    uint64  // pagesSwept to use as the origin of the sweep ratio; updated atomically</code></span>
<span class="codeline" id="line-107"><code>	sweepHeapLiveBasis uint64  // value of heap_live to use as the origin of sweep ratio; written with lock, read without</code></span>
<span class="codeline" id="line-108"><code>	sweepPagesPerByte  float64 // proportional sweep ratio; written with lock, read without</code></span>
<span class="codeline" id="line-109"><code>	// TODO(austin): pagesInUse should be a uintptr, but the 386</code></span>
<span class="codeline" id="line-110"><code>	// compiler can't 8-byte align fields.</code></span>
<span class="codeline" id="line-111"><code></code></span>
<span class="codeline" id="line-112"><code>	// scavengeGoal is the amount of total retained heap memory (measured by</code></span>
<span class="codeline" id="line-113"><code>	// heapRetained) that the runtime will try to maintain by returning memory</code></span>
<span class="codeline" id="line-114"><code>	// to the OS.</code></span>
<span class="codeline" id="line-115"><code>	scavengeGoal uint64</code></span>
<span class="codeline" id="line-116"><code></code></span>
<span class="codeline" id="line-117"><code>	// Page reclaimer state</code></span>
<span class="codeline" id="line-118"><code></code></span>
<span class="codeline" id="line-119"><code>	// reclaimIndex is the page index in allArenas of next page to</code></span>
<span class="codeline" id="line-120"><code>	// reclaim. Specifically, it refers to page (i %</code></span>
<span class="codeline" id="line-121"><code>	// pagesPerArena) of arena allArenas[i / pagesPerArena].</code></span>
<span class="codeline" id="line-122"><code>	//</code></span>
<span class="codeline" id="line-123"><code>	// If this is &gt;= 1&lt;&lt;63, the page reclaimer is done scanning</code></span>
<span class="codeline" id="line-124"><code>	// the page marks.</code></span>
<span class="codeline" id="line-125"><code>	//</code></span>
<span class="codeline" id="line-126"><code>	// This is accessed atomically.</code></span>
<span class="codeline" id="line-127"><code>	reclaimIndex uint64</code></span>
<span class="codeline" id="line-128"><code>	// reclaimCredit is spare credit for extra pages swept. Since</code></span>
<span class="codeline" id="line-129"><code>	// the page reclaimer works in large chunks, it may reclaim</code></span>
<span class="codeline" id="line-130"><code>	// more than requested. Any spare pages released go to this</code></span>
<span class="codeline" id="line-131"><code>	// credit pool.</code></span>
<span class="codeline" id="line-132"><code>	//</code></span>
<span class="codeline" id="line-133"><code>	// This is accessed atomically.</code></span>
<span class="codeline" id="line-134"><code>	reclaimCredit uintptr</code></span>
<span class="codeline" id="line-135"><code></code></span>
<span class="codeline" id="line-136"><code>	// arenas is the heap arena map. It points to the metadata for</code></span>
<span class="codeline" id="line-137"><code>	// the heap for every arena frame of the entire usable virtual</code></span>
<span class="codeline" id="line-138"><code>	// address space.</code></span>
<span class="codeline" id="line-139"><code>	//</code></span>
<span class="codeline" id="line-140"><code>	// Use arenaIndex to compute indexes into this array.</code></span>
<span class="codeline" id="line-141"><code>	//</code></span>
<span class="codeline" id="line-142"><code>	// For regions of the address space that are not backed by the</code></span>
<span class="codeline" id="line-143"><code>	// Go heap, the arena map contains nil.</code></span>
<span class="codeline" id="line-144"><code>	//</code></span>
<span class="codeline" id="line-145"><code>	// Modifications are protected by mheap_.lock. Reads can be</code></span>
<span class="codeline" id="line-146"><code>	// performed without locking; however, a given entry can</code></span>
<span class="codeline" id="line-147"><code>	// transition from nil to non-nil at any time when the lock</code></span>
<span class="codeline" id="line-148"><code>	// isn't held. (Entries never transitions back to nil.)</code></span>
<span class="codeline" id="line-149"><code>	//</code></span>
<span class="codeline" id="line-150"><code>	// In general, this is a two-level mapping consisting of an L1</code></span>
<span class="codeline" id="line-151"><code>	// map and possibly many L2 maps. This saves space when there</code></span>
<span class="codeline" id="line-152"><code>	// are a huge number of arena frames. However, on many</code></span>
<span class="codeline" id="line-153"><code>	// platforms (even 64-bit), arenaL1Bits is 0, making this</code></span>
<span class="codeline" id="line-154"><code>	// effectively a single-level map. In this case, arenas[0]</code></span>
<span class="codeline" id="line-155"><code>	// will never be nil.</code></span>
<span class="codeline" id="line-156"><code>	arenas [1 &lt;&lt; arenaL1Bits]*[1 &lt;&lt; arenaL2Bits]*heapArena</code></span>
<span class="codeline" id="line-157"><code></code></span>
<span class="codeline" id="line-158"><code>	// heapArenaAlloc is pre-reserved space for allocating heapArena</code></span>
<span class="codeline" id="line-159"><code>	// objects. This is only used on 32-bit, where we pre-reserve</code></span>
<span class="codeline" id="line-160"><code>	// this space to avoid interleaving it with the heap itself.</code></span>
<span class="codeline" id="line-161"><code>	heapArenaAlloc linearAlloc</code></span>
<span class="codeline" id="line-162"><code></code></span>
<span class="codeline" id="line-163"><code>	// arenaHints is a list of addresses at which to attempt to</code></span>
<span class="codeline" id="line-164"><code>	// add more heap arenas. This is initially populated with a</code></span>
<span class="codeline" id="line-165"><code>	// set of general hint addresses, and grown with the bounds of</code></span>
<span class="codeline" id="line-166"><code>	// actual heap arena ranges.</code></span>
<span class="codeline" id="line-167"><code>	arenaHints *arenaHint</code></span>
<span class="codeline" id="line-168"><code></code></span>
<span class="codeline" id="line-169"><code>	// arena is a pre-reserved space for allocating heap arenas</code></span>
<span class="codeline" id="line-170"><code>	// (the actual arenas). This is only used on 32-bit.</code></span>
<span class="codeline" id="line-171"><code>	arena linearAlloc</code></span>
<span class="codeline" id="line-172"><code></code></span>
<span class="codeline" id="line-173"><code>	// allArenas is the arenaIndex of every mapped arena. This can</code></span>
<span class="codeline" id="line-174"><code>	// be used to iterate through the address space.</code></span>
<span class="codeline" id="line-175"><code>	//</code></span>
<span class="codeline" id="line-176"><code>	// Access is protected by mheap_.lock. However, since this is</code></span>
<span class="codeline" id="line-177"><code>	// append-only and old backing arrays are never freed, it is</code></span>
<span class="codeline" id="line-178"><code>	// safe to acquire mheap_.lock, copy the slice header, and</code></span>
<span class="codeline" id="line-179"><code>	// then release mheap_.lock.</code></span>
<span class="codeline" id="line-180"><code>	allArenas []arenaIdx</code></span>
<span class="codeline" id="line-181"><code></code></span>
<span class="codeline" id="line-182"><code>	// sweepArenas is a snapshot of allArenas taken at the</code></span>
<span class="codeline" id="line-183"><code>	// beginning of the sweep cycle. This can be read safely by</code></span>
<span class="codeline" id="line-184"><code>	// simply blocking GC (by disabling preemption).</code></span>
<span class="codeline" id="line-185"><code>	sweepArenas []arenaIdx</code></span>
<span class="codeline" id="line-186"><code></code></span>
<span class="codeline" id="line-187"><code>	// markArenas is a snapshot of allArenas taken at the beginning</code></span>
<span class="codeline" id="line-188"><code>	// of the mark cycle. Because allArenas is append-only, neither</code></span>
<span class="codeline" id="line-189"><code>	// this slice nor its contents will change during the mark, so</code></span>
<span class="codeline" id="line-190"><code>	// it can be read safely.</code></span>
<span class="codeline" id="line-191"><code>	markArenas []arenaIdx</code></span>
<span class="codeline" id="line-192"><code></code></span>
<span class="codeline" id="line-193"><code>	// curArena is the arena that the heap is currently growing</code></span>
<span class="codeline" id="line-194"><code>	// into. This should always be physPageSize-aligned.</code></span>
<span class="codeline" id="line-195"><code>	curArena struct {</code></span>
<span class="codeline" id="line-196"><code>		base, end uintptr</code></span>
<span class="codeline" id="line-197"><code>	}</code></span>
<span class="codeline" id="line-198"><code></code></span>
<span class="codeline" id="line-199"><code>	_ uint32 // ensure 64-bit alignment of central</code></span>
<span class="codeline" id="line-200"><code></code></span>
<span class="codeline" id="line-201"><code>	// central free lists for small size classes.</code></span>
<span class="codeline" id="line-202"><code>	// the padding makes sure that the mcentrals are</code></span>
<span class="codeline" id="line-203"><code>	// spaced CacheLinePadSize bytes apart, so that each mcentral.lock</code></span>
<span class="codeline" id="line-204"><code>	// gets its own cache line.</code></span>
<span class="codeline" id="line-205"><code>	// central is indexed by spanClass.</code></span>
<span class="codeline" id="line-206"><code>	central [numSpanClasses]struct {</code></span>
<span class="codeline" id="line-207"><code>		mcentral mcentral</code></span>
<span class="codeline" id="line-208"><code>		pad      [cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize]byte</code></span>
<span class="codeline" id="line-209"><code>	}</code></span>
<span class="codeline" id="line-210"><code></code></span>
<span class="codeline" id="line-211"><code>	spanalloc             fixalloc // allocator for span*</code></span>
<span class="codeline" id="line-212"><code>	cachealloc            fixalloc // allocator for mcache*</code></span>
<span class="codeline" id="line-213"><code>	specialfinalizeralloc fixalloc // allocator for specialfinalizer*</code></span>
<span class="codeline" id="line-214"><code>	specialprofilealloc   fixalloc // allocator for specialprofile*</code></span>
<span class="codeline" id="line-215"><code>	speciallock           mutex    // lock for special record allocators.</code></span>
<span class="codeline" id="line-216"><code>	arenaHintAlloc        fixalloc // allocator for arenaHints</code></span>
<span class="codeline" id="line-217"><code></code></span>
<span class="codeline" id="line-218"><code>	unused *specialfinalizer // never set, just here to force the specialfinalizer type into DWARF</code></span>
<span class="codeline" id="line-219"><code>}</code></span>
<span class="codeline" id="line-220"><code></code></span>
<span class="codeline" id="line-221"><code>var mheap_ mheap</code></span>
<span class="codeline" id="line-222"><code></code></span>
<span class="codeline" id="line-223"><code>// A heapArena stores metadata for a heap arena. heapArenas are stored</code></span>
<span class="codeline" id="line-224"><code>// outside of the Go heap and accessed via the mheap_.arenas index.</code></span>
<span class="codeline" id="line-225"><code>//</code></span>
<span class="codeline" id="line-226"><code>//go:notinheap</code></span>
<span class="codeline" id="line-227"><code>type heapArena struct {</code></span>
<span class="codeline" id="line-228"><code>	// bitmap stores the pointer/scalar bitmap for the words in</code></span>
<span class="codeline" id="line-229"><code>	// this arena. See mbitmap.go for a description. Use the</code></span>
<span class="codeline" id="line-230"><code>	// heapBits type to access this.</code></span>
<span class="codeline" id="line-231"><code>	bitmap [heapArenaBitmapBytes]byte</code></span>
<span class="codeline" id="line-232"><code></code></span>
<span class="codeline" id="line-233"><code>	// spans maps from virtual address page ID within this arena to *mspan.</code></span>
<span class="codeline" id="line-234"><code>	// For allocated spans, their pages map to the span itself.</code></span>
<span class="codeline" id="line-235"><code>	// For free spans, only the lowest and highest pages map to the span itself.</code></span>
<span class="codeline" id="line-236"><code>	// Internal pages map to an arbitrary span.</code></span>
<span class="codeline" id="line-237"><code>	// For pages that have never been allocated, spans entries are nil.</code></span>
<span class="codeline" id="line-238"><code>	//</code></span>
<span class="codeline" id="line-239"><code>	// Modifications are protected by mheap.lock. Reads can be</code></span>
<span class="codeline" id="line-240"><code>	// performed without locking, but ONLY from indexes that are</code></span>
<span class="codeline" id="line-241"><code>	// known to contain in-use or stack spans. This means there</code></span>
<span class="codeline" id="line-242"><code>	// must not be a safe-point between establishing that an</code></span>
<span class="codeline" id="line-243"><code>	// address is live and looking it up in the spans array.</code></span>
<span class="codeline" id="line-244"><code>	spans [pagesPerArena]*mspan</code></span>
<span class="codeline" id="line-245"><code></code></span>
<span class="codeline" id="line-246"><code>	// pageInUse is a bitmap that indicates which spans are in</code></span>
<span class="codeline" id="line-247"><code>	// state mSpanInUse. This bitmap is indexed by page number,</code></span>
<span class="codeline" id="line-248"><code>	// but only the bit corresponding to the first page in each</code></span>
<span class="codeline" id="line-249"><code>	// span is used.</code></span>
<span class="codeline" id="line-250"><code>	//</code></span>
<span class="codeline" id="line-251"><code>	// Reads and writes are atomic.</code></span>
<span class="codeline" id="line-252"><code>	pageInUse [pagesPerArena / 8]uint8</code></span>
<span class="codeline" id="line-253"><code></code></span>
<span class="codeline" id="line-254"><code>	// pageMarks is a bitmap that indicates which spans have any</code></span>
<span class="codeline" id="line-255"><code>	// marked objects on them. Like pageInUse, only the bit</code></span>
<span class="codeline" id="line-256"><code>	// corresponding to the first page in each span is used.</code></span>
<span class="codeline" id="line-257"><code>	//</code></span>
<span class="codeline" id="line-258"><code>	// Writes are done atomically during marking. Reads are</code></span>
<span class="codeline" id="line-259"><code>	// non-atomic and lock-free since they only occur during</code></span>
<span class="codeline" id="line-260"><code>	// sweeping (and hence never race with writes).</code></span>
<span class="codeline" id="line-261"><code>	//</code></span>
<span class="codeline" id="line-262"><code>	// This is used to quickly find whole spans that can be freed.</code></span>
<span class="codeline" id="line-263"><code>	//</code></span>
<span class="codeline" id="line-264"><code>	// TODO(austin): It would be nice if this was uint64 for</code></span>
<span class="codeline" id="line-265"><code>	// faster scanning, but we don't have 64-bit atomic bit</code></span>
<span class="codeline" id="line-266"><code>	// operations.</code></span>
<span class="codeline" id="line-267"><code>	pageMarks [pagesPerArena / 8]uint8</code></span>
<span class="codeline" id="line-268"><code></code></span>
<span class="codeline" id="line-269"><code>	// pageSpecials is a bitmap that indicates which spans have</code></span>
<span class="codeline" id="line-270"><code>	// specials (finalizers or other). Like pageInUse, only the bit</code></span>
<span class="codeline" id="line-271"><code>	// corresponding to the first page in each span is used.</code></span>
<span class="codeline" id="line-272"><code>	//</code></span>
<span class="codeline" id="line-273"><code>	// Writes are done atomically whenever a special is added to</code></span>
<span class="codeline" id="line-274"><code>	// a span and whenever the last special is removed from a span.</code></span>
<span class="codeline" id="line-275"><code>	// Reads are done atomically to find spans containing specials</code></span>
<span class="codeline" id="line-276"><code>	// during marking.</code></span>
<span class="codeline" id="line-277"><code>	pageSpecials [pagesPerArena / 8]uint8</code></span>
<span class="codeline" id="line-278"><code></code></span>
<span class="codeline" id="line-279"><code>	// checkmarks stores the debug.gccheckmark state. It is only</code></span>
<span class="codeline" id="line-280"><code>	// used if debug.gccheckmark &gt; 0.</code></span>
<span class="codeline" id="line-281"><code>	checkmarks *checkmarksMap</code></span>
<span class="codeline" id="line-282"><code></code></span>
<span class="codeline" id="line-283"><code>	// zeroedBase marks the first byte of the first page in this</code></span>
<span class="codeline" id="line-284"><code>	// arena which hasn't been used yet and is therefore already</code></span>
<span class="codeline" id="line-285"><code>	// zero. zeroedBase is relative to the arena base.</code></span>
<span class="codeline" id="line-286"><code>	// Increases monotonically until it hits heapArenaBytes.</code></span>
<span class="codeline" id="line-287"><code>	//</code></span>
<span class="codeline" id="line-288"><code>	// This field is sufficient to determine if an allocation</code></span>
<span class="codeline" id="line-289"><code>	// needs to be zeroed because the page allocator follows an</code></span>
<span class="codeline" id="line-290"><code>	// address-ordered first-fit policy.</code></span>
<span class="codeline" id="line-291"><code>	//</code></span>
<span class="codeline" id="line-292"><code>	// Read atomically and written with an atomic CAS.</code></span>
<span class="codeline" id="line-293"><code>	zeroedBase uintptr</code></span>
<span class="codeline" id="line-294"><code>}</code></span>
<span class="codeline" id="line-295"><code></code></span>
<span class="codeline" id="line-296"><code>// arenaHint is a hint for where to grow the heap arenas. See</code></span>
<span class="codeline" id="line-297"><code>// mheap_.arenaHints.</code></span>
<span class="codeline" id="line-298"><code>//</code></span>
<span class="codeline" id="line-299"><code>//go:notinheap</code></span>
<span class="codeline" id="line-300"><code>type arenaHint struct {</code></span>
<span class="codeline" id="line-301"><code>	addr uintptr</code></span>
<span class="codeline" id="line-302"><code>	down bool</code></span>
<span class="codeline" id="line-303"><code>	next *arenaHint</code></span>
<span class="codeline" id="line-304"><code>}</code></span>
<span class="codeline" id="line-305"><code></code></span>
<span class="codeline" id="line-306"><code>// An mspan is a run of pages.</code></span>
<span class="codeline" id="line-307"><code>//</code></span>
<span class="codeline" id="line-308"><code>// When a mspan is in the heap free treap, state == mSpanFree</code></span>
<span class="codeline" id="line-309"><code>// and heapmap(s-&gt;start) == span, heapmap(s-&gt;start+s-&gt;npages-1) == span.</code></span>
<span class="codeline" id="line-310"><code>// If the mspan is in the heap scav treap, then in addition to the</code></span>
<span class="codeline" id="line-311"><code>// above scavenged == true. scavenged == false in all other cases.</code></span>
<span class="codeline" id="line-312"><code>//</code></span>
<span class="codeline" id="line-313"><code>// When a mspan is allocated, state == mSpanInUse or mSpanManual</code></span>
<span class="codeline" id="line-314"><code>// and heapmap(i) == span for all s-&gt;start &lt;= i &lt; s-&gt;start+s-&gt;npages.</code></span>
<span class="codeline" id="line-315"><code></code></span>
<span class="codeline" id="line-316"><code>// Every mspan is in one doubly-linked list, either in the mheap's</code></span>
<span class="codeline" id="line-317"><code>// busy list or one of the mcentral's span lists.</code></span>
<span class="codeline" id="line-318"><code></code></span>
<span class="codeline" id="line-319"><code>// An mspan representing actual memory has state mSpanInUse,</code></span>
<span class="codeline" id="line-320"><code>// mSpanManual, or mSpanFree. Transitions between these states are</code></span>
<span class="codeline" id="line-321"><code>// constrained as follows:</code></span>
<span class="codeline" id="line-322"><code>//</code></span>
<span class="codeline" id="line-323"><code>// * A span may transition from free to in-use or manual during any GC</code></span>
<span class="codeline" id="line-324"><code>//   phase.</code></span>
<span class="codeline" id="line-325"><code>//</code></span>
<span class="codeline" id="line-326"><code>// * During sweeping (gcphase == _GCoff), a span may transition from</code></span>
<span class="codeline" id="line-327"><code>//   in-use to free (as a result of sweeping) or manual to free (as a</code></span>
<span class="codeline" id="line-328"><code>//   result of stacks being freed).</code></span>
<span class="codeline" id="line-329"><code>//</code></span>
<span class="codeline" id="line-330"><code>// * During GC (gcphase != _GCoff), a span *must not* transition from</code></span>
<span class="codeline" id="line-331"><code>//   manual or in-use to free. Because concurrent GC may read a pointer</code></span>
<span class="codeline" id="line-332"><code>//   and then look up its span, the span state must be monotonic.</code></span>
<span class="codeline" id="line-333"><code>//</code></span>
<span class="codeline" id="line-334"><code>// Setting mspan.state to mSpanInUse or mSpanManual must be done</code></span>
<span class="codeline" id="line-335"><code>// atomically and only after all other span fields are valid.</code></span>
<span class="codeline" id="line-336"><code>// Likewise, if inspecting a span is contingent on it being</code></span>
<span class="codeline" id="line-337"><code>// mSpanInUse, the state should be loaded atomically and checked</code></span>
<span class="codeline" id="line-338"><code>// before depending on other fields. This allows the garbage collector</code></span>
<span class="codeline" id="line-339"><code>// to safely deal with potentially invalid pointers, since resolving</code></span>
<span class="codeline" id="line-340"><code>// such pointers may race with a span being allocated.</code></span>
<span class="codeline" id="line-341"><code>type mSpanState uint8</code></span>
<span class="codeline" id="line-342"><code></code></span>
<span class="codeline" id="line-343"><code>const (</code></span>
<span class="codeline" id="line-344"><code>	mSpanDead   mSpanState = iota</code></span>
<span class="codeline" id="line-345"><code>	mSpanInUse             // allocated for garbage collected heap</code></span>
<span class="codeline" id="line-346"><code>	mSpanManual            // allocated for manual management (e.g., stack allocator)</code></span>
<span class="codeline" id="line-347"><code>)</code></span>
<span class="codeline" id="line-348"><code></code></span>
<span class="codeline" id="line-349"><code>// mSpanStateNames are the names of the span states, indexed by</code></span>
<span class="codeline" id="line-350"><code>// mSpanState.</code></span>
<span class="codeline" id="line-351"><code>var mSpanStateNames = []string{</code></span>
<span class="codeline" id="line-352"><code>	"mSpanDead",</code></span>
<span class="codeline" id="line-353"><code>	"mSpanInUse",</code></span>
<span class="codeline" id="line-354"><code>	"mSpanManual",</code></span>
<span class="codeline" id="line-355"><code>	"mSpanFree",</code></span>
<span class="codeline" id="line-356"><code>}</code></span>
<span class="codeline" id="line-357"><code></code></span>
<span class="codeline" id="line-358"><code>// mSpanStateBox holds an mSpanState and provides atomic operations on</code></span>
<span class="codeline" id="line-359"><code>// it. This is a separate type to disallow accidental comparison or</code></span>
<span class="codeline" id="line-360"><code>// assignment with mSpanState.</code></span>
<span class="codeline" id="line-361"><code>type mSpanStateBox struct {</code></span>
<span class="codeline" id="line-362"><code>	s mSpanState</code></span>
<span class="codeline" id="line-363"><code>}</code></span>
<span class="codeline" id="line-364"><code></code></span>
<span class="codeline" id="line-365"><code>func (b *mSpanStateBox) set(s mSpanState) {</code></span>
<span class="codeline" id="line-366"><code>	atomic.Store8((*uint8)(&amp;b.s), uint8(s))</code></span>
<span class="codeline" id="line-367"><code>}</code></span>
<span class="codeline" id="line-368"><code></code></span>
<span class="codeline" id="line-369"><code>func (b *mSpanStateBox) get() mSpanState {</code></span>
<span class="codeline" id="line-370"><code>	return mSpanState(atomic.Load8((*uint8)(&amp;b.s)))</code></span>
<span class="codeline" id="line-371"><code>}</code></span>
<span class="codeline" id="line-372"><code></code></span>
<span class="codeline" id="line-373"><code>// mSpanList heads a linked list of spans.</code></span>
<span class="codeline" id="line-374"><code>//</code></span>
<span class="codeline" id="line-375"><code>//go:notinheap</code></span>
<span class="codeline" id="line-376"><code>type mSpanList struct {</code></span>
<span class="codeline" id="line-377"><code>	first *mspan // first span in list, or nil if none</code></span>
<span class="codeline" id="line-378"><code>	last  *mspan // last span in list, or nil if none</code></span>
<span class="codeline" id="line-379"><code>}</code></span>
<span class="codeline" id="line-380"><code></code></span>
<span class="codeline" id="line-381"><code>//go:notinheap</code></span>
<span class="codeline" id="line-382"><code>type mspan struct {</code></span>
<span class="codeline" id="line-383"><code>	next *mspan     // next span in list, or nil if none</code></span>
<span class="codeline" id="line-384"><code>	prev *mspan     // previous span in list, or nil if none</code></span>
<span class="codeline" id="line-385"><code>	list *mSpanList // For debugging. TODO: Remove.</code></span>
<span class="codeline" id="line-386"><code></code></span>
<span class="codeline" id="line-387"><code>	startAddr uintptr // address of first byte of span aka s.base()</code></span>
<span class="codeline" id="line-388"><code>	npages    uintptr // number of pages in span</code></span>
<span class="codeline" id="line-389"><code></code></span>
<span class="codeline" id="line-390"><code>	manualFreeList gclinkptr // list of free objects in mSpanManual spans</code></span>
<span class="codeline" id="line-391"><code></code></span>
<span class="codeline" id="line-392"><code>	// freeindex is the slot index between 0 and nelems at which to begin scanning</code></span>
<span class="codeline" id="line-393"><code>	// for the next free object in this span.</code></span>
<span class="codeline" id="line-394"><code>	// Each allocation scans allocBits starting at freeindex until it encounters a 0</code></span>
<span class="codeline" id="line-395"><code>	// indicating a free object. freeindex is then adjusted so that subsequent scans begin</code></span>
<span class="codeline" id="line-396"><code>	// just past the newly discovered free object.</code></span>
<span class="codeline" id="line-397"><code>	//</code></span>
<span class="codeline" id="line-398"><code>	// If freeindex == nelem, this span has no free objects.</code></span>
<span class="codeline" id="line-399"><code>	//</code></span>
<span class="codeline" id="line-400"><code>	// allocBits is a bitmap of objects in this span.</code></span>
<span class="codeline" id="line-401"><code>	// If n &gt;= freeindex and allocBits[n/8] &amp; (1&lt;&lt;(n%8)) is 0</code></span>
<span class="codeline" id="line-402"><code>	// then object n is free;</code></span>
<span class="codeline" id="line-403"><code>	// otherwise, object n is allocated. Bits starting at nelem are</code></span>
<span class="codeline" id="line-404"><code>	// undefined and should never be referenced.</code></span>
<span class="codeline" id="line-405"><code>	//</code></span>
<span class="codeline" id="line-406"><code>	// Object n starts at address n*elemsize + (start &lt;&lt; pageShift).</code></span>
<span class="codeline" id="line-407"><code>	freeindex uintptr</code></span>
<span class="codeline" id="line-408"><code>	// TODO: Look up nelems from sizeclass and remove this field if it</code></span>
<span class="codeline" id="line-409"><code>	// helps performance.</code></span>
<span class="codeline" id="line-410"><code>	nelems uintptr // number of object in the span.</code></span>
<span class="codeline" id="line-411"><code></code></span>
<span class="codeline" id="line-412"><code>	// Cache of the allocBits at freeindex. allocCache is shifted</code></span>
<span class="codeline" id="line-413"><code>	// such that the lowest bit corresponds to the bit freeindex.</code></span>
<span class="codeline" id="line-414"><code>	// allocCache holds the complement of allocBits, thus allowing</code></span>
<span class="codeline" id="line-415"><code>	// ctz (count trailing zero) to use it directly.</code></span>
<span class="codeline" id="line-416"><code>	// allocCache may contain bits beyond s.nelems; the caller must ignore</code></span>
<span class="codeline" id="line-417"><code>	// these.</code></span>
<span class="codeline" id="line-418"><code>	allocCache uint64</code></span>
<span class="codeline" id="line-419"><code></code></span>
<span class="codeline" id="line-420"><code>	// allocBits and gcmarkBits hold pointers to a span's mark and</code></span>
<span class="codeline" id="line-421"><code>	// allocation bits. The pointers are 8 byte aligned.</code></span>
<span class="codeline" id="line-422"><code>	// There are three arenas where this data is held.</code></span>
<span class="codeline" id="line-423"><code>	// free: Dirty arenas that are no longer accessed</code></span>
<span class="codeline" id="line-424"><code>	//       and can be reused.</code></span>
<span class="codeline" id="line-425"><code>	// next: Holds information to be used in the next GC cycle.</code></span>
<span class="codeline" id="line-426"><code>	// current: Information being used during this GC cycle.</code></span>
<span class="codeline" id="line-427"><code>	// previous: Information being used during the last GC cycle.</code></span>
<span class="codeline" id="line-428"><code>	// A new GC cycle starts with the call to finishsweep_m.</code></span>
<span class="codeline" id="line-429"><code>	// finishsweep_m moves the previous arena to the free arena,</code></span>
<span class="codeline" id="line-430"><code>	// the current arena to the previous arena, and</code></span>
<span class="codeline" id="line-431"><code>	// the next arena to the current arena.</code></span>
<span class="codeline" id="line-432"><code>	// The next arena is populated as the spans request</code></span>
<span class="codeline" id="line-433"><code>	// memory to hold gcmarkBits for the next GC cycle as well</code></span>
<span class="codeline" id="line-434"><code>	// as allocBits for newly allocated spans.</code></span>
<span class="codeline" id="line-435"><code>	//</code></span>
<span class="codeline" id="line-436"><code>	// The pointer arithmetic is done "by hand" instead of using</code></span>
<span class="codeline" id="line-437"><code>	// arrays to avoid bounds checks along critical performance</code></span>
<span class="codeline" id="line-438"><code>	// paths.</code></span>
<span class="codeline" id="line-439"><code>	// The sweep will free the old allocBits and set allocBits to the</code></span>
<span class="codeline" id="line-440"><code>	// gcmarkBits. The gcmarkBits are replaced with a fresh zeroed</code></span>
<span class="codeline" id="line-441"><code>	// out memory.</code></span>
<span class="codeline" id="line-442"><code>	allocBits  *gcBits</code></span>
<span class="codeline" id="line-443"><code>	gcmarkBits *gcBits</code></span>
<span class="codeline" id="line-444"><code></code></span>
<span class="codeline" id="line-445"><code>	// sweep generation:</code></span>
<span class="codeline" id="line-446"><code>	// if sweepgen == h-&gt;sweepgen - 2, the span needs sweeping</code></span>
<span class="codeline" id="line-447"><code>	// if sweepgen == h-&gt;sweepgen - 1, the span is currently being swept</code></span>
<span class="codeline" id="line-448"><code>	// if sweepgen == h-&gt;sweepgen, the span is swept and ready to use</code></span>
<span class="codeline" id="line-449"><code>	// if sweepgen == h-&gt;sweepgen + 1, the span was cached before sweep began and is still cached, and needs sweeping</code></span>
<span class="codeline" id="line-450"><code>	// if sweepgen == h-&gt;sweepgen + 3, the span was swept and then cached and is still cached</code></span>
<span class="codeline" id="line-451"><code>	// h-&gt;sweepgen is incremented by 2 after every GC</code></span>
<span class="codeline" id="line-452"><code></code></span>
<span class="codeline" id="line-453"><code>	sweepgen    uint32</code></span>
<span class="codeline" id="line-454"><code>	divMul      uint16        // for divide by elemsize - divMagic.mul</code></span>
<span class="codeline" id="line-455"><code>	baseMask    uint16        // if non-0, elemsize is a power of 2, &amp; this will get object allocation base</code></span>
<span class="codeline" id="line-456"><code>	allocCount  uint16        // number of allocated objects</code></span>
<span class="codeline" id="line-457"><code>	spanclass   spanClass     // size class and noscan (uint8)</code></span>
<span class="codeline" id="line-458"><code>	state       mSpanStateBox // mSpanInUse etc; accessed atomically (get/set methods)</code></span>
<span class="codeline" id="line-459"><code>	needzero    uint8         // needs to be zeroed before allocation</code></span>
<span class="codeline" id="line-460"><code>	divShift    uint8         // for divide by elemsize - divMagic.shift</code></span>
<span class="codeline" id="line-461"><code>	divShift2   uint8         // for divide by elemsize - divMagic.shift2</code></span>
<span class="codeline" id="line-462"><code>	elemsize    uintptr       // computed from sizeclass or from npages</code></span>
<span class="codeline" id="line-463"><code>	limit       uintptr       // end of data in span</code></span>
<span class="codeline" id="line-464"><code>	speciallock mutex         // guards specials list</code></span>
<span class="codeline" id="line-465"><code>	specials    *special      // linked list of special records sorted by offset.</code></span>
<span class="codeline" id="line-466"><code>}</code></span>
<span class="codeline" id="line-467"><code></code></span>
<span class="codeline" id="line-468"><code>func (s *mspan) base() uintptr {</code></span>
<span class="codeline" id="line-469"><code>	return s.startAddr</code></span>
<span class="codeline" id="line-470"><code>}</code></span>
<span class="codeline" id="line-471"><code></code></span>
<span class="codeline" id="line-472"><code>func (s *mspan) layout() (size, n, total uintptr) {</code></span>
<span class="codeline" id="line-473"><code>	total = s.npages &lt;&lt; _PageShift</code></span>
<span class="codeline" id="line-474"><code>	size = s.elemsize</code></span>
<span class="codeline" id="line-475"><code>	if size &gt; 0 {</code></span>
<span class="codeline" id="line-476"><code>		n = total / size</code></span>
<span class="codeline" id="line-477"><code>	}</code></span>
<span class="codeline" id="line-478"><code>	return</code></span>
<span class="codeline" id="line-479"><code>}</code></span>
<span class="codeline" id="line-480"><code></code></span>
<span class="codeline" id="line-481"><code>// recordspan adds a newly allocated span to h.allspans.</code></span>
<span class="codeline" id="line-482"><code>//</code></span>
<span class="codeline" id="line-483"><code>// This only happens the first time a span is allocated from</code></span>
<span class="codeline" id="line-484"><code>// mheap.spanalloc (it is not called when a span is reused).</code></span>
<span class="codeline" id="line-485"><code>//</code></span>
<span class="codeline" id="line-486"><code>// Write barriers are disallowed here because it can be called from</code></span>
<span class="codeline" id="line-487"><code>// gcWork when allocating new workbufs. However, because it's an</code></span>
<span class="codeline" id="line-488"><code>// indirect call from the fixalloc initializer, the compiler can't see</code></span>
<span class="codeline" id="line-489"><code>// this.</code></span>
<span class="codeline" id="line-490"><code>//</code></span>
<span class="codeline" id="line-491"><code>// The heap lock must be held.</code></span>
<span class="codeline" id="line-492"><code>//</code></span>
<span class="codeline" id="line-493"><code>//go:nowritebarrierrec</code></span>
<span class="codeline" id="line-494"><code>func recordspan(vh unsafe.Pointer, p unsafe.Pointer) {</code></span>
<span class="codeline" id="line-495"><code>	h := (*mheap)(vh)</code></span>
<span class="codeline" id="line-496"><code>	s := (*mspan)(p)</code></span>
<span class="codeline" id="line-497"><code></code></span>
<span class="codeline" id="line-498"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-499"><code></code></span>
<span class="codeline" id="line-500"><code>	if len(h.allspans) &gt;= cap(h.allspans) {</code></span>
<span class="codeline" id="line-501"><code>		n := 64 * 1024 / sys.PtrSize</code></span>
<span class="codeline" id="line-502"><code>		if n &lt; cap(h.allspans)*3/2 {</code></span>
<span class="codeline" id="line-503"><code>			n = cap(h.allspans) * 3 / 2</code></span>
<span class="codeline" id="line-504"><code>		}</code></span>
<span class="codeline" id="line-505"><code>		var new []*mspan</code></span>
<span class="codeline" id="line-506"><code>		sp := (*slice)(unsafe.Pointer(&amp;new))</code></span>
<span class="codeline" id="line-507"><code>		sp.array = sysAlloc(uintptr(n)*sys.PtrSize, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-508"><code>		if sp.array == nil {</code></span>
<span class="codeline" id="line-509"><code>			throw("runtime: cannot allocate memory")</code></span>
<span class="codeline" id="line-510"><code>		}</code></span>
<span class="codeline" id="line-511"><code>		sp.len = len(h.allspans)</code></span>
<span class="codeline" id="line-512"><code>		sp.cap = n</code></span>
<span class="codeline" id="line-513"><code>		if len(h.allspans) &gt; 0 {</code></span>
<span class="codeline" id="line-514"><code>			copy(new, h.allspans)</code></span>
<span class="codeline" id="line-515"><code>		}</code></span>
<span class="codeline" id="line-516"><code>		oldAllspans := h.allspans</code></span>
<span class="codeline" id="line-517"><code>		*(*notInHeapSlice)(unsafe.Pointer(&amp;h.allspans)) = *(*notInHeapSlice)(unsafe.Pointer(&amp;new))</code></span>
<span class="codeline" id="line-518"><code>		if len(oldAllspans) != 0 {</code></span>
<span class="codeline" id="line-519"><code>			sysFree(unsafe.Pointer(&amp;oldAllspans[0]), uintptr(cap(oldAllspans))*unsafe.Sizeof(oldAllspans[0]), &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-520"><code>		}</code></span>
<span class="codeline" id="line-521"><code>	}</code></span>
<span class="codeline" id="line-522"><code>	h.allspans = h.allspans[:len(h.allspans)+1]</code></span>
<span class="codeline" id="line-523"><code>	h.allspans[len(h.allspans)-1] = s</code></span>
<span class="codeline" id="line-524"><code>}</code></span>
<span class="codeline" id="line-525"><code></code></span>
<span class="codeline" id="line-526"><code>// A spanClass represents the size class and noscan-ness of a span.</code></span>
<span class="codeline" id="line-527"><code>//</code></span>
<span class="codeline" id="line-528"><code>// Each size class has a noscan spanClass and a scan spanClass. The</code></span>
<span class="codeline" id="line-529"><code>// noscan spanClass contains only noscan objects, which do not contain</code></span>
<span class="codeline" id="line-530"><code>// pointers and thus do not need to be scanned by the garbage</code></span>
<span class="codeline" id="line-531"><code>// collector.</code></span>
<span class="codeline" id="line-532"><code>type spanClass uint8</code></span>
<span class="codeline" id="line-533"><code></code></span>
<span class="codeline" id="line-534"><code>const (</code></span>
<span class="codeline" id="line-535"><code>	numSpanClasses = _NumSizeClasses &lt;&lt; 1</code></span>
<span class="codeline" id="line-536"><code>	tinySpanClass  = spanClass(tinySizeClass&lt;&lt;1 | 1)</code></span>
<span class="codeline" id="line-537"><code>)</code></span>
<span class="codeline" id="line-538"><code></code></span>
<span class="codeline" id="line-539"><code>func makeSpanClass(sizeclass uint8, noscan bool) spanClass {</code></span>
<span class="codeline" id="line-540"><code>	return spanClass(sizeclass&lt;&lt;1) | spanClass(bool2int(noscan))</code></span>
<span class="codeline" id="line-541"><code>}</code></span>
<span class="codeline" id="line-542"><code></code></span>
<span class="codeline" id="line-543"><code>func (sc spanClass) sizeclass() int8 {</code></span>
<span class="codeline" id="line-544"><code>	return int8(sc &gt;&gt; 1)</code></span>
<span class="codeline" id="line-545"><code>}</code></span>
<span class="codeline" id="line-546"><code></code></span>
<span class="codeline" id="line-547"><code>func (sc spanClass) noscan() bool {</code></span>
<span class="codeline" id="line-548"><code>	return sc&amp;1 != 0</code></span>
<span class="codeline" id="line-549"><code>}</code></span>
<span class="codeline" id="line-550"><code></code></span>
<span class="codeline" id="line-551"><code>// arenaIndex returns the index into mheap_.arenas of the arena</code></span>
<span class="codeline" id="line-552"><code>// containing metadata for p. This index combines of an index into the</code></span>
<span class="codeline" id="line-553"><code>// L1 map and an index into the L2 map and should be used as</code></span>
<span class="codeline" id="line-554"><code>// mheap_.arenas[ai.l1()][ai.l2()].</code></span>
<span class="codeline" id="line-555"><code>//</code></span>
<span class="codeline" id="line-556"><code>// If p is outside the range of valid heap addresses, either l1() or</code></span>
<span class="codeline" id="line-557"><code>// l2() will be out of bounds.</code></span>
<span class="codeline" id="line-558"><code>//</code></span>
<span class="codeline" id="line-559"><code>// It is nosplit because it's called by spanOf and several other</code></span>
<span class="codeline" id="line-560"><code>// nosplit functions.</code></span>
<span class="codeline" id="line-561"><code>//</code></span>
<span class="codeline" id="line-562"><code>//go:nosplit</code></span>
<span class="codeline" id="line-563"><code>func arenaIndex(p uintptr) arenaIdx {</code></span>
<span class="codeline" id="line-564"><code>	return arenaIdx((p - arenaBaseOffset) / heapArenaBytes)</code></span>
<span class="codeline" id="line-565"><code>}</code></span>
<span class="codeline" id="line-566"><code></code></span>
<span class="codeline" id="line-567"><code>// arenaBase returns the low address of the region covered by heap</code></span>
<span class="codeline" id="line-568"><code>// arena i.</code></span>
<span class="codeline" id="line-569"><code>func arenaBase(i arenaIdx) uintptr {</code></span>
<span class="codeline" id="line-570"><code>	return uintptr(i)*heapArenaBytes + arenaBaseOffset</code></span>
<span class="codeline" id="line-571"><code>}</code></span>
<span class="codeline" id="line-572"><code></code></span>
<span class="codeline" id="line-573"><code>type arenaIdx uint</code></span>
<span class="codeline" id="line-574"><code></code></span>
<span class="codeline" id="line-575"><code>func (i arenaIdx) l1() uint {</code></span>
<span class="codeline" id="line-576"><code>	if arenaL1Bits == 0 {</code></span>
<span class="codeline" id="line-577"><code>		// Let the compiler optimize this away if there's no</code></span>
<span class="codeline" id="line-578"><code>		// L1 map.</code></span>
<span class="codeline" id="line-579"><code>		return 0</code></span>
<span class="codeline" id="line-580"><code>	} else {</code></span>
<span class="codeline" id="line-581"><code>		return uint(i) &gt;&gt; arenaL1Shift</code></span>
<span class="codeline" id="line-582"><code>	}</code></span>
<span class="codeline" id="line-583"><code>}</code></span>
<span class="codeline" id="line-584"><code></code></span>
<span class="codeline" id="line-585"><code>func (i arenaIdx) l2() uint {</code></span>
<span class="codeline" id="line-586"><code>	if arenaL1Bits == 0 {</code></span>
<span class="codeline" id="line-587"><code>		return uint(i)</code></span>
<span class="codeline" id="line-588"><code>	} else {</code></span>
<span class="codeline" id="line-589"><code>		return uint(i) &amp; (1&lt;&lt;arenaL2Bits - 1)</code></span>
<span class="codeline" id="line-590"><code>	}</code></span>
<span class="codeline" id="line-591"><code>}</code></span>
<span class="codeline" id="line-592"><code></code></span>
<span class="codeline" id="line-593"><code>// inheap reports whether b is a pointer into a (potentially dead) heap object.</code></span>
<span class="codeline" id="line-594"><code>// It returns false for pointers into mSpanManual spans.</code></span>
<span class="codeline" id="line-595"><code>// Non-preemptible because it is used by write barriers.</code></span>
<span class="codeline" id="line-596"><code>//go:nowritebarrier</code></span>
<span class="codeline" id="line-597"><code>//go:nosplit</code></span>
<span class="codeline" id="line-598"><code>func inheap(b uintptr) bool {</code></span>
<span class="codeline" id="line-599"><code>	return spanOfHeap(b) != nil</code></span>
<span class="codeline" id="line-600"><code>}</code></span>
<span class="codeline" id="line-601"><code></code></span>
<span class="codeline" id="line-602"><code>// inHeapOrStack is a variant of inheap that returns true for pointers</code></span>
<span class="codeline" id="line-603"><code>// into any allocated heap span.</code></span>
<span class="codeline" id="line-604"><code>//</code></span>
<span class="codeline" id="line-605"><code>//go:nowritebarrier</code></span>
<span class="codeline" id="line-606"><code>//go:nosplit</code></span>
<span class="codeline" id="line-607"><code>func inHeapOrStack(b uintptr) bool {</code></span>
<span class="codeline" id="line-608"><code>	s := spanOf(b)</code></span>
<span class="codeline" id="line-609"><code>	if s == nil || b &lt; s.base() {</code></span>
<span class="codeline" id="line-610"><code>		return false</code></span>
<span class="codeline" id="line-611"><code>	}</code></span>
<span class="codeline" id="line-612"><code>	switch s.state.get() {</code></span>
<span class="codeline" id="line-613"><code>	case mSpanInUse, mSpanManual:</code></span>
<span class="codeline" id="line-614"><code>		return b &lt; s.limit</code></span>
<span class="codeline" id="line-615"><code>	default:</code></span>
<span class="codeline" id="line-616"><code>		return false</code></span>
<span class="codeline" id="line-617"><code>	}</code></span>
<span class="codeline" id="line-618"><code>}</code></span>
<span class="codeline" id="line-619"><code></code></span>
<span class="codeline" id="line-620"><code>// spanOf returns the span of p. If p does not point into the heap</code></span>
<span class="codeline" id="line-621"><code>// arena or no span has ever contained p, spanOf returns nil.</code></span>
<span class="codeline" id="line-622"><code>//</code></span>
<span class="codeline" id="line-623"><code>// If p does not point to allocated memory, this may return a non-nil</code></span>
<span class="codeline" id="line-624"><code>// span that does *not* contain p. If this is a possibility, the</code></span>
<span class="codeline" id="line-625"><code>// caller should either call spanOfHeap or check the span bounds</code></span>
<span class="codeline" id="line-626"><code>// explicitly.</code></span>
<span class="codeline" id="line-627"><code>//</code></span>
<span class="codeline" id="line-628"><code>// Must be nosplit because it has callers that are nosplit.</code></span>
<span class="codeline" id="line-629"><code>//</code></span>
<span class="codeline" id="line-630"><code>//go:nosplit</code></span>
<span class="codeline" id="line-631"><code>func spanOf(p uintptr) *mspan {</code></span>
<span class="codeline" id="line-632"><code>	// This function looks big, but we use a lot of constant</code></span>
<span class="codeline" id="line-633"><code>	// folding around arenaL1Bits to get it under the inlining</code></span>
<span class="codeline" id="line-634"><code>	// budget. Also, many of the checks here are safety checks</code></span>
<span class="codeline" id="line-635"><code>	// that Go needs to do anyway, so the generated code is quite</code></span>
<span class="codeline" id="line-636"><code>	// short.</code></span>
<span class="codeline" id="line-637"><code>	ri := arenaIndex(p)</code></span>
<span class="codeline" id="line-638"><code>	if arenaL1Bits == 0 {</code></span>
<span class="codeline" id="line-639"><code>		// If there's no L1, then ri.l1() can't be out of bounds but ri.l2() can.</code></span>
<span class="codeline" id="line-640"><code>		if ri.l2() &gt;= uint(len(mheap_.arenas[0])) {</code></span>
<span class="codeline" id="line-641"><code>			return nil</code></span>
<span class="codeline" id="line-642"><code>		}</code></span>
<span class="codeline" id="line-643"><code>	} else {</code></span>
<span class="codeline" id="line-644"><code>		// If there's an L1, then ri.l1() can be out of bounds but ri.l2() can't.</code></span>
<span class="codeline" id="line-645"><code>		if ri.l1() &gt;= uint(len(mheap_.arenas)) {</code></span>
<span class="codeline" id="line-646"><code>			return nil</code></span>
<span class="codeline" id="line-647"><code>		}</code></span>
<span class="codeline" id="line-648"><code>	}</code></span>
<span class="codeline" id="line-649"><code>	l2 := mheap_.arenas[ri.l1()]</code></span>
<span class="codeline" id="line-650"><code>	if arenaL1Bits != 0 &amp;&amp; l2 == nil { // Should never happen if there's no L1.</code></span>
<span class="codeline" id="line-651"><code>		return nil</code></span>
<span class="codeline" id="line-652"><code>	}</code></span>
<span class="codeline" id="line-653"><code>	ha := l2[ri.l2()]</code></span>
<span class="codeline" id="line-654"><code>	if ha == nil {</code></span>
<span class="codeline" id="line-655"><code>		return nil</code></span>
<span class="codeline" id="line-656"><code>	}</code></span>
<span class="codeline" id="line-657"><code>	return ha.spans[(p/pageSize)%pagesPerArena]</code></span>
<span class="codeline" id="line-658"><code>}</code></span>
<span class="codeline" id="line-659"><code></code></span>
<span class="codeline" id="line-660"><code>// spanOfUnchecked is equivalent to spanOf, but the caller must ensure</code></span>
<span class="codeline" id="line-661"><code>// that p points into an allocated heap arena.</code></span>
<span class="codeline" id="line-662"><code>//</code></span>
<span class="codeline" id="line-663"><code>// Must be nosplit because it has callers that are nosplit.</code></span>
<span class="codeline" id="line-664"><code>//</code></span>
<span class="codeline" id="line-665"><code>//go:nosplit</code></span>
<span class="codeline" id="line-666"><code>func spanOfUnchecked(p uintptr) *mspan {</code></span>
<span class="codeline" id="line-667"><code>	ai := arenaIndex(p)</code></span>
<span class="codeline" id="line-668"><code>	return mheap_.arenas[ai.l1()][ai.l2()].spans[(p/pageSize)%pagesPerArena]</code></span>
<span class="codeline" id="line-669"><code>}</code></span>
<span class="codeline" id="line-670"><code></code></span>
<span class="codeline" id="line-671"><code>// spanOfHeap is like spanOf, but returns nil if p does not point to a</code></span>
<span class="codeline" id="line-672"><code>// heap object.</code></span>
<span class="codeline" id="line-673"><code>//</code></span>
<span class="codeline" id="line-674"><code>// Must be nosplit because it has callers that are nosplit.</code></span>
<span class="codeline" id="line-675"><code>//</code></span>
<span class="codeline" id="line-676"><code>//go:nosplit</code></span>
<span class="codeline" id="line-677"><code>func spanOfHeap(p uintptr) *mspan {</code></span>
<span class="codeline" id="line-678"><code>	s := spanOf(p)</code></span>
<span class="codeline" id="line-679"><code>	// s is nil if it's never been allocated. Otherwise, we check</code></span>
<span class="codeline" id="line-680"><code>	// its state first because we don't trust this pointer, so we</code></span>
<span class="codeline" id="line-681"><code>	// have to synchronize with span initialization. Then, it's</code></span>
<span class="codeline" id="line-682"><code>	// still possible we picked up a stale span pointer, so we</code></span>
<span class="codeline" id="line-683"><code>	// have to check the span's bounds.</code></span>
<span class="codeline" id="line-684"><code>	if s == nil || s.state.get() != mSpanInUse || p &lt; s.base() || p &gt;= s.limit {</code></span>
<span class="codeline" id="line-685"><code>		return nil</code></span>
<span class="codeline" id="line-686"><code>	}</code></span>
<span class="codeline" id="line-687"><code>	return s</code></span>
<span class="codeline" id="line-688"><code>}</code></span>
<span class="codeline" id="line-689"><code></code></span>
<span class="codeline" id="line-690"><code>// pageIndexOf returns the arena, page index, and page mask for pointer p.</code></span>
<span class="codeline" id="line-691"><code>// The caller must ensure p is in the heap.</code></span>
<span class="codeline" id="line-692"><code>func pageIndexOf(p uintptr) (arena *heapArena, pageIdx uintptr, pageMask uint8) {</code></span>
<span class="codeline" id="line-693"><code>	ai := arenaIndex(p)</code></span>
<span class="codeline" id="line-694"><code>	arena = mheap_.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-695"><code>	pageIdx = ((p / pageSize) / 8) % uintptr(len(arena.pageInUse))</code></span>
<span class="codeline" id="line-696"><code>	pageMask = byte(1 &lt;&lt; ((p / pageSize) % 8))</code></span>
<span class="codeline" id="line-697"><code>	return</code></span>
<span class="codeline" id="line-698"><code>}</code></span>
<span class="codeline" id="line-699"><code></code></span>
<span class="codeline" id="line-700"><code>// Initialize the heap.</code></span>
<span class="codeline" id="line-701"><code>func (h *mheap) init() {</code></span>
<span class="codeline" id="line-702"><code>	lockInit(&amp;h.lock, lockRankMheap)</code></span>
<span class="codeline" id="line-703"><code>	lockInit(&amp;h.speciallock, lockRankMheapSpecial)</code></span>
<span class="codeline" id="line-704"><code></code></span>
<span class="codeline" id="line-705"><code>	h.spanalloc.init(unsafe.Sizeof(mspan{}), recordspan, unsafe.Pointer(h), &amp;memstats.mspan_sys)</code></span>
<span class="codeline" id="line-706"><code>	h.cachealloc.init(unsafe.Sizeof(mcache{}), nil, nil, &amp;memstats.mcache_sys)</code></span>
<span class="codeline" id="line-707"><code>	h.specialfinalizeralloc.init(unsafe.Sizeof(specialfinalizer{}), nil, nil, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-708"><code>	h.specialprofilealloc.init(unsafe.Sizeof(specialprofile{}), nil, nil, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-709"><code>	h.arenaHintAlloc.init(unsafe.Sizeof(arenaHint{}), nil, nil, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-710"><code></code></span>
<span class="codeline" id="line-711"><code>	// Don't zero mspan allocations. Background sweeping can</code></span>
<span class="codeline" id="line-712"><code>	// inspect a span concurrently with allocating it, so it's</code></span>
<span class="codeline" id="line-713"><code>	// important that the span's sweepgen survive across freeing</code></span>
<span class="codeline" id="line-714"><code>	// and re-allocating a span to prevent background sweeping</code></span>
<span class="codeline" id="line-715"><code>	// from improperly cas'ing it from 0.</code></span>
<span class="codeline" id="line-716"><code>	//</code></span>
<span class="codeline" id="line-717"><code>	// This is safe because mspan contains no heap pointers.</code></span>
<span class="codeline" id="line-718"><code>	h.spanalloc.zero = false</code></span>
<span class="codeline" id="line-719"><code></code></span>
<span class="codeline" id="line-720"><code>	// h-&gt;mapcache needs no init</code></span>
<span class="codeline" id="line-721"><code></code></span>
<span class="codeline" id="line-722"><code>	for i := range h.central {</code></span>
<span class="codeline" id="line-723"><code>		h.central[i].mcentral.init(spanClass(i))</code></span>
<span class="codeline" id="line-724"><code>	}</code></span>
<span class="codeline" id="line-725"><code></code></span>
<span class="codeline" id="line-726"><code>	h.pages.init(&amp;h.lock, &amp;memstats.gcMiscSys)</code></span>
<span class="codeline" id="line-727"><code>}</code></span>
<span class="codeline" id="line-728"><code></code></span>
<span class="codeline" id="line-729"><code>// reclaim sweeps and reclaims at least npage pages into the heap.</code></span>
<span class="codeline" id="line-730"><code>// It is called before allocating npage pages to keep growth in check.</code></span>
<span class="codeline" id="line-731"><code>//</code></span>
<span class="codeline" id="line-732"><code>// reclaim implements the page-reclaimer half of the sweeper.</code></span>
<span class="codeline" id="line-733"><code>//</code></span>
<span class="codeline" id="line-734"><code>// h.lock must NOT be held.</code></span>
<span class="codeline" id="line-735"><code>func (h *mheap) reclaim(npage uintptr) {</code></span>
<span class="codeline" id="line-736"><code>	// TODO(austin): Half of the time spent freeing spans is in</code></span>
<span class="codeline" id="line-737"><code>	// locking/unlocking the heap (even with low contention). We</code></span>
<span class="codeline" id="line-738"><code>	// could make the slow path here several times faster by</code></span>
<span class="codeline" id="line-739"><code>	// batching heap frees.</code></span>
<span class="codeline" id="line-740"><code></code></span>
<span class="codeline" id="line-741"><code>	// Bail early if there's no more reclaim work.</code></span>
<span class="codeline" id="line-742"><code>	if atomic.Load64(&amp;h.reclaimIndex) &gt;= 1&lt;&lt;63 {</code></span>
<span class="codeline" id="line-743"><code>		return</code></span>
<span class="codeline" id="line-744"><code>	}</code></span>
<span class="codeline" id="line-745"><code></code></span>
<span class="codeline" id="line-746"><code>	// Disable preemption so the GC can't start while we're</code></span>
<span class="codeline" id="line-747"><code>	// sweeping, so we can read h.sweepArenas, and so</code></span>
<span class="codeline" id="line-748"><code>	// traceGCSweepStart/Done pair on the P.</code></span>
<span class="codeline" id="line-749"><code>	mp := acquirem()</code></span>
<span class="codeline" id="line-750"><code></code></span>
<span class="codeline" id="line-751"><code>	if trace.enabled {</code></span>
<span class="codeline" id="line-752"><code>		traceGCSweepStart()</code></span>
<span class="codeline" id="line-753"><code>	}</code></span>
<span class="codeline" id="line-754"><code></code></span>
<span class="codeline" id="line-755"><code>	arenas := h.sweepArenas</code></span>
<span class="codeline" id="line-756"><code>	locked := false</code></span>
<span class="codeline" id="line-757"><code>	for npage &gt; 0 {</code></span>
<span class="codeline" id="line-758"><code>		// Pull from accumulated credit first.</code></span>
<span class="codeline" id="line-759"><code>		if credit := atomic.Loaduintptr(&amp;h.reclaimCredit); credit &gt; 0 {</code></span>
<span class="codeline" id="line-760"><code>			take := credit</code></span>
<span class="codeline" id="line-761"><code>			if take &gt; npage {</code></span>
<span class="codeline" id="line-762"><code>				// Take only what we need.</code></span>
<span class="codeline" id="line-763"><code>				take = npage</code></span>
<span class="codeline" id="line-764"><code>			}</code></span>
<span class="codeline" id="line-765"><code>			if atomic.Casuintptr(&amp;h.reclaimCredit, credit, credit-take) {</code></span>
<span class="codeline" id="line-766"><code>				npage -= take</code></span>
<span class="codeline" id="line-767"><code>			}</code></span>
<span class="codeline" id="line-768"><code>			continue</code></span>
<span class="codeline" id="line-769"><code>		}</code></span>
<span class="codeline" id="line-770"><code></code></span>
<span class="codeline" id="line-771"><code>		// Claim a chunk of work.</code></span>
<span class="codeline" id="line-772"><code>		idx := uintptr(atomic.Xadd64(&amp;h.reclaimIndex, pagesPerReclaimerChunk) - pagesPerReclaimerChunk)</code></span>
<span class="codeline" id="line-773"><code>		if idx/pagesPerArena &gt;= uintptr(len(arenas)) {</code></span>
<span class="codeline" id="line-774"><code>			// Page reclaiming is done.</code></span>
<span class="codeline" id="line-775"><code>			atomic.Store64(&amp;h.reclaimIndex, 1&lt;&lt;63)</code></span>
<span class="codeline" id="line-776"><code>			break</code></span>
<span class="codeline" id="line-777"><code>		}</code></span>
<span class="codeline" id="line-778"><code></code></span>
<span class="codeline" id="line-779"><code>		if !locked {</code></span>
<span class="codeline" id="line-780"><code>			// Lock the heap for reclaimChunk.</code></span>
<span class="codeline" id="line-781"><code>			lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-782"><code>			locked = true</code></span>
<span class="codeline" id="line-783"><code>		}</code></span>
<span class="codeline" id="line-784"><code></code></span>
<span class="codeline" id="line-785"><code>		// Scan this chunk.</code></span>
<span class="codeline" id="line-786"><code>		nfound := h.reclaimChunk(arenas, idx, pagesPerReclaimerChunk)</code></span>
<span class="codeline" id="line-787"><code>		if nfound &lt;= npage {</code></span>
<span class="codeline" id="line-788"><code>			npage -= nfound</code></span>
<span class="codeline" id="line-789"><code>		} else {</code></span>
<span class="codeline" id="line-790"><code>			// Put spare pages toward global credit.</code></span>
<span class="codeline" id="line-791"><code>			atomic.Xadduintptr(&amp;h.reclaimCredit, nfound-npage)</code></span>
<span class="codeline" id="line-792"><code>			npage = 0</code></span>
<span class="codeline" id="line-793"><code>		}</code></span>
<span class="codeline" id="line-794"><code>	}</code></span>
<span class="codeline" id="line-795"><code>	if locked {</code></span>
<span class="codeline" id="line-796"><code>		unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-797"><code>	}</code></span>
<span class="codeline" id="line-798"><code></code></span>
<span class="codeline" id="line-799"><code>	if trace.enabled {</code></span>
<span class="codeline" id="line-800"><code>		traceGCSweepDone()</code></span>
<span class="codeline" id="line-801"><code>	}</code></span>
<span class="codeline" id="line-802"><code>	releasem(mp)</code></span>
<span class="codeline" id="line-803"><code>}</code></span>
<span class="codeline" id="line-804"><code></code></span>
<span class="codeline" id="line-805"><code>// reclaimChunk sweeps unmarked spans that start at page indexes [pageIdx, pageIdx+n).</code></span>
<span class="codeline" id="line-806"><code>// It returns the number of pages returned to the heap.</code></span>
<span class="codeline" id="line-807"><code>//</code></span>
<span class="codeline" id="line-808"><code>// h.lock must be held and the caller must be non-preemptible. Note: h.lock may be</code></span>
<span class="codeline" id="line-809"><code>// temporarily unlocked and re-locked in order to do sweeping or if tracing is</code></span>
<span class="codeline" id="line-810"><code>// enabled.</code></span>
<span class="codeline" id="line-811"><code>func (h *mheap) reclaimChunk(arenas []arenaIdx, pageIdx, n uintptr) uintptr {</code></span>
<span class="codeline" id="line-812"><code>	// The heap lock must be held because this accesses the</code></span>
<span class="codeline" id="line-813"><code>	// heapArena.spans arrays using potentially non-live pointers.</code></span>
<span class="codeline" id="line-814"><code>	// In particular, if a span were freed and merged concurrently</code></span>
<span class="codeline" id="line-815"><code>	// with this probing heapArena.spans, it would be possible to</code></span>
<span class="codeline" id="line-816"><code>	// observe arbitrary, stale span pointers.</code></span>
<span class="codeline" id="line-817"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-818"><code></code></span>
<span class="codeline" id="line-819"><code>	n0 := n</code></span>
<span class="codeline" id="line-820"><code>	var nFreed uintptr</code></span>
<span class="codeline" id="line-821"><code>	sg := h.sweepgen</code></span>
<span class="codeline" id="line-822"><code>	for n &gt; 0 {</code></span>
<span class="codeline" id="line-823"><code>		ai := arenas[pageIdx/pagesPerArena]</code></span>
<span class="codeline" id="line-824"><code>		ha := h.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-825"><code></code></span>
<span class="codeline" id="line-826"><code>		// Get a chunk of the bitmap to work on.</code></span>
<span class="codeline" id="line-827"><code>		arenaPage := uint(pageIdx % pagesPerArena)</code></span>
<span class="codeline" id="line-828"><code>		inUse := ha.pageInUse[arenaPage/8:]</code></span>
<span class="codeline" id="line-829"><code>		marked := ha.pageMarks[arenaPage/8:]</code></span>
<span class="codeline" id="line-830"><code>		if uintptr(len(inUse)) &gt; n/8 {</code></span>
<span class="codeline" id="line-831"><code>			inUse = inUse[:n/8]</code></span>
<span class="codeline" id="line-832"><code>			marked = marked[:n/8]</code></span>
<span class="codeline" id="line-833"><code>		}</code></span>
<span class="codeline" id="line-834"><code></code></span>
<span class="codeline" id="line-835"><code>		// Scan this bitmap chunk for spans that are in-use</code></span>
<span class="codeline" id="line-836"><code>		// but have no marked objects on them.</code></span>
<span class="codeline" id="line-837"><code>		for i := range inUse {</code></span>
<span class="codeline" id="line-838"><code>			inUseUnmarked := atomic.Load8(&amp;inUse[i]) &amp;^ marked[i]</code></span>
<span class="codeline" id="line-839"><code>			if inUseUnmarked == 0 {</code></span>
<span class="codeline" id="line-840"><code>				continue</code></span>
<span class="codeline" id="line-841"><code>			}</code></span>
<span class="codeline" id="line-842"><code></code></span>
<span class="codeline" id="line-843"><code>			for j := uint(0); j &lt; 8; j++ {</code></span>
<span class="codeline" id="line-844"><code>				if inUseUnmarked&amp;(1&lt;&lt;j) != 0 {</code></span>
<span class="codeline" id="line-845"><code>					s := ha.spans[arenaPage+uint(i)*8+j]</code></span>
<span class="codeline" id="line-846"><code>					if atomic.Load(&amp;s.sweepgen) == sg-2 &amp;&amp; atomic.Cas(&amp;s.sweepgen, sg-2, sg-1) {</code></span>
<span class="codeline" id="line-847"><code>						npages := s.npages</code></span>
<span class="codeline" id="line-848"><code>						unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-849"><code>						if s.sweep(false) {</code></span>
<span class="codeline" id="line-850"><code>							nFreed += npages</code></span>
<span class="codeline" id="line-851"><code>						}</code></span>
<span class="codeline" id="line-852"><code>						lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-853"><code>						// Reload inUse. It's possible nearby</code></span>
<span class="codeline" id="line-854"><code>						// spans were freed when we dropped the</code></span>
<span class="codeline" id="line-855"><code>						// lock and we don't want to get stale</code></span>
<span class="codeline" id="line-856"><code>						// pointers from the spans array.</code></span>
<span class="codeline" id="line-857"><code>						inUseUnmarked = atomic.Load8(&amp;inUse[i]) &amp;^ marked[i]</code></span>
<span class="codeline" id="line-858"><code>					}</code></span>
<span class="codeline" id="line-859"><code>				}</code></span>
<span class="codeline" id="line-860"><code>			}</code></span>
<span class="codeline" id="line-861"><code>		}</code></span>
<span class="codeline" id="line-862"><code></code></span>
<span class="codeline" id="line-863"><code>		// Advance.</code></span>
<span class="codeline" id="line-864"><code>		pageIdx += uintptr(len(inUse) * 8)</code></span>
<span class="codeline" id="line-865"><code>		n -= uintptr(len(inUse) * 8)</code></span>
<span class="codeline" id="line-866"><code>	}</code></span>
<span class="codeline" id="line-867"><code>	if trace.enabled {</code></span>
<span class="codeline" id="line-868"><code>		unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-869"><code>		// Account for pages scanned but not reclaimed.</code></span>
<span class="codeline" id="line-870"><code>		traceGCSweepSpan((n0 - nFreed) * pageSize)</code></span>
<span class="codeline" id="line-871"><code>		lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-872"><code>	}</code></span>
<span class="codeline" id="line-873"><code></code></span>
<span class="codeline" id="line-874"><code>	assertLockHeld(&amp;h.lock) // Must be locked on return.</code></span>
<span class="codeline" id="line-875"><code>	return nFreed</code></span>
<span class="codeline" id="line-876"><code>}</code></span>
<span class="codeline" id="line-877"><code></code></span>
<span class="codeline" id="line-878"><code>// spanAllocType represents the type of allocation to make, or</code></span>
<span class="codeline" id="line-879"><code>// the type of allocation to be freed.</code></span>
<span class="codeline" id="line-880"><code>type spanAllocType uint8</code></span>
<span class="codeline" id="line-881"><code></code></span>
<span class="codeline" id="line-882"><code>const (</code></span>
<span class="codeline" id="line-883"><code>	spanAllocHeap          spanAllocType = iota // heap span</code></span>
<span class="codeline" id="line-884"><code>	spanAllocStack                              // stack span</code></span>
<span class="codeline" id="line-885"><code>	spanAllocPtrScalarBits                      // unrolled GC prog bitmap span</code></span>
<span class="codeline" id="line-886"><code>	spanAllocWorkBuf                            // work buf span</code></span>
<span class="codeline" id="line-887"><code>)</code></span>
<span class="codeline" id="line-888"><code></code></span>
<span class="codeline" id="line-889"><code>// manual returns true if the span allocation is manually managed.</code></span>
<span class="codeline" id="line-890"><code>func (s spanAllocType) manual() bool {</code></span>
<span class="codeline" id="line-891"><code>	return s != spanAllocHeap</code></span>
<span class="codeline" id="line-892"><code>}</code></span>
<span class="codeline" id="line-893"><code></code></span>
<span class="codeline" id="line-894"><code>// alloc allocates a new span of npage pages from the GC'd heap.</code></span>
<span class="codeline" id="line-895"><code>//</code></span>
<span class="codeline" id="line-896"><code>// spanclass indicates the span's size class and scannability.</code></span>
<span class="codeline" id="line-897"><code>//</code></span>
<span class="codeline" id="line-898"><code>// If needzero is true, the memory for the returned span will be zeroed.</code></span>
<span class="codeline" id="line-899"><code>func (h *mheap) alloc(npages uintptr, spanclass spanClass, needzero bool) *mspan {</code></span>
<span class="codeline" id="line-900"><code>	// Don't do any operations that lock the heap on the G stack.</code></span>
<span class="codeline" id="line-901"><code>	// It might trigger stack growth, and the stack growth code needs</code></span>
<span class="codeline" id="line-902"><code>	// to be able to allocate heap.</code></span>
<span class="codeline" id="line-903"><code>	var s *mspan</code></span>
<span class="codeline" id="line-904"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-905"><code>		// To prevent excessive heap growth, before allocating n pages</code></span>
<span class="codeline" id="line-906"><code>		// we need to sweep and reclaim at least n pages.</code></span>
<span class="codeline" id="line-907"><code>		if h.sweepdone == 0 {</code></span>
<span class="codeline" id="line-908"><code>			h.reclaim(npages)</code></span>
<span class="codeline" id="line-909"><code>		}</code></span>
<span class="codeline" id="line-910"><code>		s = h.allocSpan(npages, spanAllocHeap, spanclass)</code></span>
<span class="codeline" id="line-911"><code>	})</code></span>
<span class="codeline" id="line-912"><code></code></span>
<span class="codeline" id="line-913"><code>	if s != nil {</code></span>
<span class="codeline" id="line-914"><code>		if needzero &amp;&amp; s.needzero != 0 {</code></span>
<span class="codeline" id="line-915"><code>			memclrNoHeapPointers(unsafe.Pointer(s.base()), s.npages&lt;&lt;_PageShift)</code></span>
<span class="codeline" id="line-916"><code>		}</code></span>
<span class="codeline" id="line-917"><code>		s.needzero = 0</code></span>
<span class="codeline" id="line-918"><code>	}</code></span>
<span class="codeline" id="line-919"><code>	return s</code></span>
<span class="codeline" id="line-920"><code>}</code></span>
<span class="codeline" id="line-921"><code></code></span>
<span class="codeline" id="line-922"><code>// allocManual allocates a manually-managed span of npage pages.</code></span>
<span class="codeline" id="line-923"><code>// allocManual returns nil if allocation fails.</code></span>
<span class="codeline" id="line-924"><code>//</code></span>
<span class="codeline" id="line-925"><code>// allocManual adds the bytes used to *stat, which should be a</code></span>
<span class="codeline" id="line-926"><code>// memstats in-use field. Unlike allocations in the GC'd heap, the</code></span>
<span class="codeline" id="line-927"><code>// allocation does *not* count toward heap_inuse or heap_sys.</code></span>
<span class="codeline" id="line-928"><code>//</code></span>
<span class="codeline" id="line-929"><code>// The memory backing the returned span may not be zeroed if</code></span>
<span class="codeline" id="line-930"><code>// span.needzero is set.</code></span>
<span class="codeline" id="line-931"><code>//</code></span>
<span class="codeline" id="line-932"><code>// allocManual must be called on the system stack because it may</code></span>
<span class="codeline" id="line-933"><code>// acquire the heap lock via allocSpan. See mheap for details.</code></span>
<span class="codeline" id="line-934"><code>//</code></span>
<span class="codeline" id="line-935"><code>// If new code is written to call allocManual, do NOT use an</code></span>
<span class="codeline" id="line-936"><code>// existing spanAllocType value and instead declare a new one.</code></span>
<span class="codeline" id="line-937"><code>//</code></span>
<span class="codeline" id="line-938"><code>//go:systemstack</code></span>
<span class="codeline" id="line-939"><code>func (h *mheap) allocManual(npages uintptr, typ spanAllocType) *mspan {</code></span>
<span class="codeline" id="line-940"><code>	if !typ.manual() {</code></span>
<span class="codeline" id="line-941"><code>		throw("manual span allocation called with non-manually-managed type")</code></span>
<span class="codeline" id="line-942"><code>	}</code></span>
<span class="codeline" id="line-943"><code>	return h.allocSpan(npages, typ, 0)</code></span>
<span class="codeline" id="line-944"><code>}</code></span>
<span class="codeline" id="line-945"><code></code></span>
<span class="codeline" id="line-946"><code>// setSpans modifies the span map so [spanOf(base), spanOf(base+npage*pageSize))</code></span>
<span class="codeline" id="line-947"><code>// is s.</code></span>
<span class="codeline" id="line-948"><code>func (h *mheap) setSpans(base, npage uintptr, s *mspan) {</code></span>
<span class="codeline" id="line-949"><code>	p := base / pageSize</code></span>
<span class="codeline" id="line-950"><code>	ai := arenaIndex(base)</code></span>
<span class="codeline" id="line-951"><code>	ha := h.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-952"><code>	for n := uintptr(0); n &lt; npage; n++ {</code></span>
<span class="codeline" id="line-953"><code>		i := (p + n) % pagesPerArena</code></span>
<span class="codeline" id="line-954"><code>		if i == 0 {</code></span>
<span class="codeline" id="line-955"><code>			ai = arenaIndex(base + n*pageSize)</code></span>
<span class="codeline" id="line-956"><code>			ha = h.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-957"><code>		}</code></span>
<span class="codeline" id="line-958"><code>		ha.spans[i] = s</code></span>
<span class="codeline" id="line-959"><code>	}</code></span>
<span class="codeline" id="line-960"><code>}</code></span>
<span class="codeline" id="line-961"><code></code></span>
<span class="codeline" id="line-962"><code>// allocNeedsZero checks if the region of address space [base, base+npage*pageSize),</code></span>
<span class="codeline" id="line-963"><code>// assumed to be allocated, needs to be zeroed, updating heap arena metadata for</code></span>
<span class="codeline" id="line-964"><code>// future allocations.</code></span>
<span class="codeline" id="line-965"><code>//</code></span>
<span class="codeline" id="line-966"><code>// This must be called each time pages are allocated from the heap, even if the page</code></span>
<span class="codeline" id="line-967"><code>// allocator can otherwise prove the memory it's allocating is already zero because</code></span>
<span class="codeline" id="line-968"><code>// they're fresh from the operating system. It updates heapArena metadata that is</code></span>
<span class="codeline" id="line-969"><code>// critical for future page allocations.</code></span>
<span class="codeline" id="line-970"><code>//</code></span>
<span class="codeline" id="line-971"><code>// There are no locking constraints on this method.</code></span>
<span class="codeline" id="line-972"><code>func (h *mheap) allocNeedsZero(base, npage uintptr) (needZero bool) {</code></span>
<span class="codeline" id="line-973"><code>	for npage &gt; 0 {</code></span>
<span class="codeline" id="line-974"><code>		ai := arenaIndex(base)</code></span>
<span class="codeline" id="line-975"><code>		ha := h.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-976"><code></code></span>
<span class="codeline" id="line-977"><code>		zeroedBase := atomic.Loaduintptr(&amp;ha.zeroedBase)</code></span>
<span class="codeline" id="line-978"><code>		arenaBase := base % heapArenaBytes</code></span>
<span class="codeline" id="line-979"><code>		if arenaBase &lt; zeroedBase {</code></span>
<span class="codeline" id="line-980"><code>			// We extended into the non-zeroed part of the</code></span>
<span class="codeline" id="line-981"><code>			// arena, so this region needs to be zeroed before use.</code></span>
<span class="codeline" id="line-982"><code>			//</code></span>
<span class="codeline" id="line-983"><code>			// zeroedBase is monotonically increasing, so if we see this now then</code></span>
<span class="codeline" id="line-984"><code>			// we can be sure we need to zero this memory region.</code></span>
<span class="codeline" id="line-985"><code>			//</code></span>
<span class="codeline" id="line-986"><code>			// We still need to update zeroedBase for this arena, and</code></span>
<span class="codeline" id="line-987"><code>			// potentially more arenas.</code></span>
<span class="codeline" id="line-988"><code>			needZero = true</code></span>
<span class="codeline" id="line-989"><code>		}</code></span>
<span class="codeline" id="line-990"><code>		// We may observe arenaBase &gt; zeroedBase if we're racing with one or more</code></span>
<span class="codeline" id="line-991"><code>		// allocations which are acquiring memory directly before us in the address</code></span>
<span class="codeline" id="line-992"><code>		// space. But, because we know no one else is acquiring *this* memory, it's</code></span>
<span class="codeline" id="line-993"><code>		// still safe to not zero.</code></span>
<span class="codeline" id="line-994"><code></code></span>
<span class="codeline" id="line-995"><code>		// Compute how far into the arena we extend into, capped</code></span>
<span class="codeline" id="line-996"><code>		// at heapArenaBytes.</code></span>
<span class="codeline" id="line-997"><code>		arenaLimit := arenaBase + npage*pageSize</code></span>
<span class="codeline" id="line-998"><code>		if arenaLimit &gt; heapArenaBytes {</code></span>
<span class="codeline" id="line-999"><code>			arenaLimit = heapArenaBytes</code></span>
<span class="codeline" id="line-1000"><code>		}</code></span>
<span class="codeline" id="line-1001"><code>		// Increase ha.zeroedBase so it's &gt;= arenaLimit.</code></span>
<span class="codeline" id="line-1002"><code>		// We may be racing with other updates.</code></span>
<span class="codeline" id="line-1003"><code>		for arenaLimit &gt; zeroedBase {</code></span>
<span class="codeline" id="line-1004"><code>			if atomic.Casuintptr(&amp;ha.zeroedBase, zeroedBase, arenaLimit) {</code></span>
<span class="codeline" id="line-1005"><code>				break</code></span>
<span class="codeline" id="line-1006"><code>			}</code></span>
<span class="codeline" id="line-1007"><code>			zeroedBase = atomic.Loaduintptr(&amp;ha.zeroedBase)</code></span>
<span class="codeline" id="line-1008"><code>			// Sanity check zeroedBase.</code></span>
<span class="codeline" id="line-1009"><code>			if zeroedBase &lt;= arenaLimit &amp;&amp; zeroedBase &gt; arenaBase {</code></span>
<span class="codeline" id="line-1010"><code>				// The zeroedBase moved into the space we were trying to</code></span>
<span class="codeline" id="line-1011"><code>				// claim. That's very bad, and indicates someone allocated</code></span>
<span class="codeline" id="line-1012"><code>				// the same region we did.</code></span>
<span class="codeline" id="line-1013"><code>				throw("potentially overlapping in-use allocations detected")</code></span>
<span class="codeline" id="line-1014"><code>			}</code></span>
<span class="codeline" id="line-1015"><code>		}</code></span>
<span class="codeline" id="line-1016"><code></code></span>
<span class="codeline" id="line-1017"><code>		// Move base forward and subtract from npage to move into</code></span>
<span class="codeline" id="line-1018"><code>		// the next arena, or finish.</code></span>
<span class="codeline" id="line-1019"><code>		base += arenaLimit - arenaBase</code></span>
<span class="codeline" id="line-1020"><code>		npage -= (arenaLimit - arenaBase) / pageSize</code></span>
<span class="codeline" id="line-1021"><code>	}</code></span>
<span class="codeline" id="line-1022"><code>	return</code></span>
<span class="codeline" id="line-1023"><code>}</code></span>
<span class="codeline" id="line-1024"><code></code></span>
<span class="codeline" id="line-1025"><code>// tryAllocMSpan attempts to allocate an mspan object from</code></span>
<span class="codeline" id="line-1026"><code>// the P-local cache, but may fail.</code></span>
<span class="codeline" id="line-1027"><code>//</code></span>
<span class="codeline" id="line-1028"><code>// h.lock need not be held.</code></span>
<span class="codeline" id="line-1029"><code>//</code></span>
<span class="codeline" id="line-1030"><code>// This caller must ensure that its P won't change underneath</code></span>
<span class="codeline" id="line-1031"><code>// it during this function. Currently to ensure that we enforce</code></span>
<span class="codeline" id="line-1032"><code>// that the function is run on the system stack, because that's</code></span>
<span class="codeline" id="line-1033"><code>// the only place it is used now. In the future, this requirement</code></span>
<span class="codeline" id="line-1034"><code>// may be relaxed if its use is necessary elsewhere.</code></span>
<span class="codeline" id="line-1035"><code>//</code></span>
<span class="codeline" id="line-1036"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1037"><code>func (h *mheap) tryAllocMSpan() *mspan {</code></span>
<span class="codeline" id="line-1038"><code>	pp := getg().m.p.ptr()</code></span>
<span class="codeline" id="line-1039"><code>	// If we don't have a p or the cache is empty, we can't do</code></span>
<span class="codeline" id="line-1040"><code>	// anything here.</code></span>
<span class="codeline" id="line-1041"><code>	if pp == nil || pp.mspancache.len == 0 {</code></span>
<span class="codeline" id="line-1042"><code>		return nil</code></span>
<span class="codeline" id="line-1043"><code>	}</code></span>
<span class="codeline" id="line-1044"><code>	// Pull off the last entry in the cache.</code></span>
<span class="codeline" id="line-1045"><code>	s := pp.mspancache.buf[pp.mspancache.len-1]</code></span>
<span class="codeline" id="line-1046"><code>	pp.mspancache.len--</code></span>
<span class="codeline" id="line-1047"><code>	return s</code></span>
<span class="codeline" id="line-1048"><code>}</code></span>
<span class="codeline" id="line-1049"><code></code></span>
<span class="codeline" id="line-1050"><code>// allocMSpanLocked allocates an mspan object.</code></span>
<span class="codeline" id="line-1051"><code>//</code></span>
<span class="codeline" id="line-1052"><code>// h.lock must be held.</code></span>
<span class="codeline" id="line-1053"><code>//</code></span>
<span class="codeline" id="line-1054"><code>// allocMSpanLocked must be called on the system stack because</code></span>
<span class="codeline" id="line-1055"><code>// its caller holds the heap lock. See mheap for details.</code></span>
<span class="codeline" id="line-1056"><code>// Running on the system stack also ensures that we won't</code></span>
<span class="codeline" id="line-1057"><code>// switch Ps during this function. See tryAllocMSpan for details.</code></span>
<span class="codeline" id="line-1058"><code>//</code></span>
<span class="codeline" id="line-1059"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1060"><code>func (h *mheap) allocMSpanLocked() *mspan {</code></span>
<span class="codeline" id="line-1061"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-1062"><code></code></span>
<span class="codeline" id="line-1063"><code>	pp := getg().m.p.ptr()</code></span>
<span class="codeline" id="line-1064"><code>	if pp == nil {</code></span>
<span class="codeline" id="line-1065"><code>		// We don't have a p so just do the normal thing.</code></span>
<span class="codeline" id="line-1066"><code>		return (*mspan)(h.spanalloc.alloc())</code></span>
<span class="codeline" id="line-1067"><code>	}</code></span>
<span class="codeline" id="line-1068"><code>	// Refill the cache if necessary.</code></span>
<span class="codeline" id="line-1069"><code>	if pp.mspancache.len == 0 {</code></span>
<span class="codeline" id="line-1070"><code>		const refillCount = len(pp.mspancache.buf) / 2</code></span>
<span class="codeline" id="line-1071"><code>		for i := 0; i &lt; refillCount; i++ {</code></span>
<span class="codeline" id="line-1072"><code>			pp.mspancache.buf[i] = (*mspan)(h.spanalloc.alloc())</code></span>
<span class="codeline" id="line-1073"><code>		}</code></span>
<span class="codeline" id="line-1074"><code>		pp.mspancache.len = refillCount</code></span>
<span class="codeline" id="line-1075"><code>	}</code></span>
<span class="codeline" id="line-1076"><code>	// Pull off the last entry in the cache.</code></span>
<span class="codeline" id="line-1077"><code>	s := pp.mspancache.buf[pp.mspancache.len-1]</code></span>
<span class="codeline" id="line-1078"><code>	pp.mspancache.len--</code></span>
<span class="codeline" id="line-1079"><code>	return s</code></span>
<span class="codeline" id="line-1080"><code>}</code></span>
<span class="codeline" id="line-1081"><code></code></span>
<span class="codeline" id="line-1082"><code>// freeMSpanLocked free an mspan object.</code></span>
<span class="codeline" id="line-1083"><code>//</code></span>
<span class="codeline" id="line-1084"><code>// h.lock must be held.</code></span>
<span class="codeline" id="line-1085"><code>//</code></span>
<span class="codeline" id="line-1086"><code>// freeMSpanLocked must be called on the system stack because</code></span>
<span class="codeline" id="line-1087"><code>// its caller holds the heap lock. See mheap for details.</code></span>
<span class="codeline" id="line-1088"><code>// Running on the system stack also ensures that we won't</code></span>
<span class="codeline" id="line-1089"><code>// switch Ps during this function. See tryAllocMSpan for details.</code></span>
<span class="codeline" id="line-1090"><code>//</code></span>
<span class="codeline" id="line-1091"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1092"><code>func (h *mheap) freeMSpanLocked(s *mspan) {</code></span>
<span class="codeline" id="line-1093"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-1094"><code></code></span>
<span class="codeline" id="line-1095"><code>	pp := getg().m.p.ptr()</code></span>
<span class="codeline" id="line-1096"><code>	// First try to free the mspan directly to the cache.</code></span>
<span class="codeline" id="line-1097"><code>	if pp != nil &amp;&amp; pp.mspancache.len &lt; len(pp.mspancache.buf) {</code></span>
<span class="codeline" id="line-1098"><code>		pp.mspancache.buf[pp.mspancache.len] = s</code></span>
<span class="codeline" id="line-1099"><code>		pp.mspancache.len++</code></span>
<span class="codeline" id="line-1100"><code>		return</code></span>
<span class="codeline" id="line-1101"><code>	}</code></span>
<span class="codeline" id="line-1102"><code>	// Failing that (or if we don't have a p), just free it to</code></span>
<span class="codeline" id="line-1103"><code>	// the heap.</code></span>
<span class="codeline" id="line-1104"><code>	h.spanalloc.free(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-1105"><code>}</code></span>
<span class="codeline" id="line-1106"><code></code></span>
<span class="codeline" id="line-1107"><code>// allocSpan allocates an mspan which owns npages worth of memory.</code></span>
<span class="codeline" id="line-1108"><code>//</code></span>
<span class="codeline" id="line-1109"><code>// If typ.manual() == false, allocSpan allocates a heap span of class spanclass</code></span>
<span class="codeline" id="line-1110"><code>// and updates heap accounting. If manual == true, allocSpan allocates a</code></span>
<span class="codeline" id="line-1111"><code>// manually-managed span (spanclass is ignored), and the caller is</code></span>
<span class="codeline" id="line-1112"><code>// responsible for any accounting related to its use of the span. Either</code></span>
<span class="codeline" id="line-1113"><code>// way, allocSpan will atomically add the bytes in the newly allocated</code></span>
<span class="codeline" id="line-1114"><code>// span to *sysStat.</code></span>
<span class="codeline" id="line-1115"><code>//</code></span>
<span class="codeline" id="line-1116"><code>// The returned span is fully initialized.</code></span>
<span class="codeline" id="line-1117"><code>//</code></span>
<span class="codeline" id="line-1118"><code>// h.lock must not be held.</code></span>
<span class="codeline" id="line-1119"><code>//</code></span>
<span class="codeline" id="line-1120"><code>// allocSpan must be called on the system stack both because it acquires</code></span>
<span class="codeline" id="line-1121"><code>// the heap lock and because it must block GC transitions.</code></span>
<span class="codeline" id="line-1122"><code>//</code></span>
<span class="codeline" id="line-1123"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1124"><code>func (h *mheap) allocSpan(npages uintptr, typ spanAllocType, spanclass spanClass) (s *mspan) {</code></span>
<span class="codeline" id="line-1125"><code>	// Function-global state.</code></span>
<span class="codeline" id="line-1126"><code>	gp := getg()</code></span>
<span class="codeline" id="line-1127"><code>	base, scav := uintptr(0), uintptr(0)</code></span>
<span class="codeline" id="line-1128"><code></code></span>
<span class="codeline" id="line-1129"><code>	// On some platforms we need to provide physical page aligned stack</code></span>
<span class="codeline" id="line-1130"><code>	// allocations. Where the page size is less than the physical page</code></span>
<span class="codeline" id="line-1131"><code>	// size, we already manage to do this by default.</code></span>
<span class="codeline" id="line-1132"><code>	needPhysPageAlign := physPageAlignedStacks &amp;&amp; typ == spanAllocStack &amp;&amp; pageSize &lt; physPageSize</code></span>
<span class="codeline" id="line-1133"><code></code></span>
<span class="codeline" id="line-1134"><code>	// If the allocation is small enough, try the page cache!</code></span>
<span class="codeline" id="line-1135"><code>	// The page cache does not support aligned allocations, so we cannot use</code></span>
<span class="codeline" id="line-1136"><code>	// it if we need to provide a physical page aligned stack allocation.</code></span>
<span class="codeline" id="line-1137"><code>	pp := gp.m.p.ptr()</code></span>
<span class="codeline" id="line-1138"><code>	if !needPhysPageAlign &amp;&amp; pp != nil &amp;&amp; npages &lt; pageCachePages/4 {</code></span>
<span class="codeline" id="line-1139"><code>		c := &amp;pp.pcache</code></span>
<span class="codeline" id="line-1140"><code></code></span>
<span class="codeline" id="line-1141"><code>		// If the cache is empty, refill it.</code></span>
<span class="codeline" id="line-1142"><code>		if c.empty() {</code></span>
<span class="codeline" id="line-1143"><code>			lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1144"><code>			*c = h.pages.allocToCache()</code></span>
<span class="codeline" id="line-1145"><code>			unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1146"><code>		}</code></span>
<span class="codeline" id="line-1147"><code></code></span>
<span class="codeline" id="line-1148"><code>		// Try to allocate from the cache.</code></span>
<span class="codeline" id="line-1149"><code>		base, scav = c.alloc(npages)</code></span>
<span class="codeline" id="line-1150"><code>		if base != 0 {</code></span>
<span class="codeline" id="line-1151"><code>			s = h.tryAllocMSpan()</code></span>
<span class="codeline" id="line-1152"><code>			if s != nil {</code></span>
<span class="codeline" id="line-1153"><code>				goto HaveSpan</code></span>
<span class="codeline" id="line-1154"><code>			}</code></span>
<span class="codeline" id="line-1155"><code>			// We have a base but no mspan, so we need</code></span>
<span class="codeline" id="line-1156"><code>			// to lock the heap.</code></span>
<span class="codeline" id="line-1157"><code>		}</code></span>
<span class="codeline" id="line-1158"><code>	}</code></span>
<span class="codeline" id="line-1159"><code></code></span>
<span class="codeline" id="line-1160"><code>	// For one reason or another, we couldn't get the</code></span>
<span class="codeline" id="line-1161"><code>	// whole job done without the heap lock.</code></span>
<span class="codeline" id="line-1162"><code>	lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1163"><code></code></span>
<span class="codeline" id="line-1164"><code>	if needPhysPageAlign {</code></span>
<span class="codeline" id="line-1165"><code>		// Overallocate by a physical page to allow for later alignment.</code></span>
<span class="codeline" id="line-1166"><code>		npages += physPageSize / pageSize</code></span>
<span class="codeline" id="line-1167"><code>	}</code></span>
<span class="codeline" id="line-1168"><code></code></span>
<span class="codeline" id="line-1169"><code>	if base == 0 {</code></span>
<span class="codeline" id="line-1170"><code>		// Try to acquire a base address.</code></span>
<span class="codeline" id="line-1171"><code>		base, scav = h.pages.alloc(npages)</code></span>
<span class="codeline" id="line-1172"><code>		if base == 0 {</code></span>
<span class="codeline" id="line-1173"><code>			if !h.grow(npages) {</code></span>
<span class="codeline" id="line-1174"><code>				unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1175"><code>				return nil</code></span>
<span class="codeline" id="line-1176"><code>			}</code></span>
<span class="codeline" id="line-1177"><code>			base, scav = h.pages.alloc(npages)</code></span>
<span class="codeline" id="line-1178"><code>			if base == 0 {</code></span>
<span class="codeline" id="line-1179"><code>				throw("grew heap, but no adequate free space found")</code></span>
<span class="codeline" id="line-1180"><code>			}</code></span>
<span class="codeline" id="line-1181"><code>		}</code></span>
<span class="codeline" id="line-1182"><code>	}</code></span>
<span class="codeline" id="line-1183"><code>	if s == nil {</code></span>
<span class="codeline" id="line-1184"><code>		// We failed to get an mspan earlier, so grab</code></span>
<span class="codeline" id="line-1185"><code>		// one now that we have the heap lock.</code></span>
<span class="codeline" id="line-1186"><code>		s = h.allocMSpanLocked()</code></span>
<span class="codeline" id="line-1187"><code>	}</code></span>
<span class="codeline" id="line-1188"><code></code></span>
<span class="codeline" id="line-1189"><code>	if needPhysPageAlign {</code></span>
<span class="codeline" id="line-1190"><code>		allocBase, allocPages := base, npages</code></span>
<span class="codeline" id="line-1191"><code>		base = alignUp(allocBase, physPageSize)</code></span>
<span class="codeline" id="line-1192"><code>		npages -= physPageSize / pageSize</code></span>
<span class="codeline" id="line-1193"><code></code></span>
<span class="codeline" id="line-1194"><code>		// Return memory around the aligned allocation.</code></span>
<span class="codeline" id="line-1195"><code>		spaceBefore := base - allocBase</code></span>
<span class="codeline" id="line-1196"><code>		if spaceBefore &gt; 0 {</code></span>
<span class="codeline" id="line-1197"><code>			h.pages.free(allocBase, spaceBefore/pageSize)</code></span>
<span class="codeline" id="line-1198"><code>		}</code></span>
<span class="codeline" id="line-1199"><code>		spaceAfter := (allocPages-npages)*pageSize - spaceBefore</code></span>
<span class="codeline" id="line-1200"><code>		if spaceAfter &gt; 0 {</code></span>
<span class="codeline" id="line-1201"><code>			h.pages.free(base+npages*pageSize, spaceAfter/pageSize)</code></span>
<span class="codeline" id="line-1202"><code>		}</code></span>
<span class="codeline" id="line-1203"><code>	}</code></span>
<span class="codeline" id="line-1204"><code></code></span>
<span class="codeline" id="line-1205"><code>	unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1206"><code></code></span>
<span class="codeline" id="line-1207"><code>HaveSpan:</code></span>
<span class="codeline" id="line-1208"><code>	// At this point, both s != nil and base != 0, and the heap</code></span>
<span class="codeline" id="line-1209"><code>	// lock is no longer held. Initialize the span.</code></span>
<span class="codeline" id="line-1210"><code>	s.init(base, npages)</code></span>
<span class="codeline" id="line-1211"><code>	if h.allocNeedsZero(base, npages) {</code></span>
<span class="codeline" id="line-1212"><code>		s.needzero = 1</code></span>
<span class="codeline" id="line-1213"><code>	}</code></span>
<span class="codeline" id="line-1214"><code>	nbytes := npages * pageSize</code></span>
<span class="codeline" id="line-1215"><code>	if typ.manual() {</code></span>
<span class="codeline" id="line-1216"><code>		s.manualFreeList = 0</code></span>
<span class="codeline" id="line-1217"><code>		s.nelems = 0</code></span>
<span class="codeline" id="line-1218"><code>		s.limit = s.base() + s.npages*pageSize</code></span>
<span class="codeline" id="line-1219"><code>		s.state.set(mSpanManual)</code></span>
<span class="codeline" id="line-1220"><code>	} else {</code></span>
<span class="codeline" id="line-1221"><code>		// We must set span properties before the span is published anywhere</code></span>
<span class="codeline" id="line-1222"><code>		// since we're not holding the heap lock.</code></span>
<span class="codeline" id="line-1223"><code>		s.spanclass = spanclass</code></span>
<span class="codeline" id="line-1224"><code>		if sizeclass := spanclass.sizeclass(); sizeclass == 0 {</code></span>
<span class="codeline" id="line-1225"><code>			s.elemsize = nbytes</code></span>
<span class="codeline" id="line-1226"><code>			s.nelems = 1</code></span>
<span class="codeline" id="line-1227"><code></code></span>
<span class="codeline" id="line-1228"><code>			s.divShift = 0</code></span>
<span class="codeline" id="line-1229"><code>			s.divMul = 0</code></span>
<span class="codeline" id="line-1230"><code>			s.divShift2 = 0</code></span>
<span class="codeline" id="line-1231"><code>			s.baseMask = 0</code></span>
<span class="codeline" id="line-1232"><code>		} else {</code></span>
<span class="codeline" id="line-1233"><code>			s.elemsize = uintptr(class_to_size[sizeclass])</code></span>
<span class="codeline" id="line-1234"><code>			s.nelems = nbytes / s.elemsize</code></span>
<span class="codeline" id="line-1235"><code></code></span>
<span class="codeline" id="line-1236"><code>			m := &amp;class_to_divmagic[sizeclass]</code></span>
<span class="codeline" id="line-1237"><code>			s.divShift = m.shift</code></span>
<span class="codeline" id="line-1238"><code>			s.divMul = m.mul</code></span>
<span class="codeline" id="line-1239"><code>			s.divShift2 = m.shift2</code></span>
<span class="codeline" id="line-1240"><code>			s.baseMask = m.baseMask</code></span>
<span class="codeline" id="line-1241"><code>		}</code></span>
<span class="codeline" id="line-1242"><code></code></span>
<span class="codeline" id="line-1243"><code>		// Initialize mark and allocation structures.</code></span>
<span class="codeline" id="line-1244"><code>		s.freeindex = 0</code></span>
<span class="codeline" id="line-1245"><code>		s.allocCache = ^uint64(0) // all 1s indicating all free.</code></span>
<span class="codeline" id="line-1246"><code>		s.gcmarkBits = newMarkBits(s.nelems)</code></span>
<span class="codeline" id="line-1247"><code>		s.allocBits = newAllocBits(s.nelems)</code></span>
<span class="codeline" id="line-1248"><code></code></span>
<span class="codeline" id="line-1249"><code>		// It's safe to access h.sweepgen without the heap lock because it's</code></span>
<span class="codeline" id="line-1250"><code>		// only ever updated with the world stopped and we run on the</code></span>
<span class="codeline" id="line-1251"><code>		// systemstack which blocks a STW transition.</code></span>
<span class="codeline" id="line-1252"><code>		atomic.Store(&amp;s.sweepgen, h.sweepgen)</code></span>
<span class="codeline" id="line-1253"><code></code></span>
<span class="codeline" id="line-1254"><code>		// Now that the span is filled in, set its state. This</code></span>
<span class="codeline" id="line-1255"><code>		// is a publication barrier for the other fields in</code></span>
<span class="codeline" id="line-1256"><code>		// the span. While valid pointers into this span</code></span>
<span class="codeline" id="line-1257"><code>		// should never be visible until the span is returned,</code></span>
<span class="codeline" id="line-1258"><code>		// if the garbage collector finds an invalid pointer,</code></span>
<span class="codeline" id="line-1259"><code>		// access to the span may race with initialization of</code></span>
<span class="codeline" id="line-1260"><code>		// the span. We resolve this race by atomically</code></span>
<span class="codeline" id="line-1261"><code>		// setting the state after the span is fully</code></span>
<span class="codeline" id="line-1262"><code>		// initialized, and atomically checking the state in</code></span>
<span class="codeline" id="line-1263"><code>		// any situation where a pointer is suspect.</code></span>
<span class="codeline" id="line-1264"><code>		s.state.set(mSpanInUse)</code></span>
<span class="codeline" id="line-1265"><code>	}</code></span>
<span class="codeline" id="line-1266"><code></code></span>
<span class="codeline" id="line-1267"><code>	// Commit and account for any scavenged memory that the span now owns.</code></span>
<span class="codeline" id="line-1268"><code>	if scav != 0 {</code></span>
<span class="codeline" id="line-1269"><code>		// sysUsed all the pages that are actually available</code></span>
<span class="codeline" id="line-1270"><code>		// in the span since some of them might be scavenged.</code></span>
<span class="codeline" id="line-1271"><code>		sysUsed(unsafe.Pointer(base), nbytes)</code></span>
<span class="codeline" id="line-1272"><code>		atomic.Xadd64(&amp;memstats.heap_released, -int64(scav))</code></span>
<span class="codeline" id="line-1273"><code>	}</code></span>
<span class="codeline" id="line-1274"><code>	// Update stats.</code></span>
<span class="codeline" id="line-1275"><code>	if typ == spanAllocHeap {</code></span>
<span class="codeline" id="line-1276"><code>		atomic.Xadd64(&amp;memstats.heap_inuse, int64(nbytes))</code></span>
<span class="codeline" id="line-1277"><code>	}</code></span>
<span class="codeline" id="line-1278"><code>	if typ.manual() {</code></span>
<span class="codeline" id="line-1279"><code>		// Manually managed memory doesn't count toward heap_sys.</code></span>
<span class="codeline" id="line-1280"><code>		memstats.heap_sys.add(-int64(nbytes))</code></span>
<span class="codeline" id="line-1281"><code>	}</code></span>
<span class="codeline" id="line-1282"><code>	// Update consistent stats.</code></span>
<span class="codeline" id="line-1283"><code>	stats := memstats.heapStats.acquire()</code></span>
<span class="codeline" id="line-1284"><code>	atomic.Xaddint64(&amp;stats.committed, int64(scav))</code></span>
<span class="codeline" id="line-1285"><code>	atomic.Xaddint64(&amp;stats.released, -int64(scav))</code></span>
<span class="codeline" id="line-1286"><code>	switch typ {</code></span>
<span class="codeline" id="line-1287"><code>	case spanAllocHeap:</code></span>
<span class="codeline" id="line-1288"><code>		atomic.Xaddint64(&amp;stats.inHeap, int64(nbytes))</code></span>
<span class="codeline" id="line-1289"><code>	case spanAllocStack:</code></span>
<span class="codeline" id="line-1290"><code>		atomic.Xaddint64(&amp;stats.inStacks, int64(nbytes))</code></span>
<span class="codeline" id="line-1291"><code>	case spanAllocPtrScalarBits:</code></span>
<span class="codeline" id="line-1292"><code>		atomic.Xaddint64(&amp;stats.inPtrScalarBits, int64(nbytes))</code></span>
<span class="codeline" id="line-1293"><code>	case spanAllocWorkBuf:</code></span>
<span class="codeline" id="line-1294"><code>		atomic.Xaddint64(&amp;stats.inWorkBufs, int64(nbytes))</code></span>
<span class="codeline" id="line-1295"><code>	}</code></span>
<span class="codeline" id="line-1296"><code>	memstats.heapStats.release()</code></span>
<span class="codeline" id="line-1297"><code></code></span>
<span class="codeline" id="line-1298"><code>	// Publish the span in various locations.</code></span>
<span class="codeline" id="line-1299"><code></code></span>
<span class="codeline" id="line-1300"><code>	// This is safe to call without the lock held because the slots</code></span>
<span class="codeline" id="line-1301"><code>	// related to this span will only ever be read or modified by</code></span>
<span class="codeline" id="line-1302"><code>	// this thread until pointers into the span are published (and</code></span>
<span class="codeline" id="line-1303"><code>	// we execute a publication barrier at the end of this function</code></span>
<span class="codeline" id="line-1304"><code>	// before that happens) or pageInUse is updated.</code></span>
<span class="codeline" id="line-1305"><code>	h.setSpans(s.base(), npages, s)</code></span>
<span class="codeline" id="line-1306"><code></code></span>
<span class="codeline" id="line-1307"><code>	if !typ.manual() {</code></span>
<span class="codeline" id="line-1308"><code>		// Mark in-use span in arena page bitmap.</code></span>
<span class="codeline" id="line-1309"><code>		//</code></span>
<span class="codeline" id="line-1310"><code>		// This publishes the span to the page sweeper, so</code></span>
<span class="codeline" id="line-1311"><code>		// it's imperative that the span be completely initialized</code></span>
<span class="codeline" id="line-1312"><code>		// prior to this line.</code></span>
<span class="codeline" id="line-1313"><code>		arena, pageIdx, pageMask := pageIndexOf(s.base())</code></span>
<span class="codeline" id="line-1314"><code>		atomic.Or8(&amp;arena.pageInUse[pageIdx], pageMask)</code></span>
<span class="codeline" id="line-1315"><code></code></span>
<span class="codeline" id="line-1316"><code>		// Update related page sweeper stats.</code></span>
<span class="codeline" id="line-1317"><code>		atomic.Xadd64(&amp;h.pagesInUse, int64(npages))</code></span>
<span class="codeline" id="line-1318"><code>	}</code></span>
<span class="codeline" id="line-1319"><code></code></span>
<span class="codeline" id="line-1320"><code>	// Make sure the newly allocated span will be observed</code></span>
<span class="codeline" id="line-1321"><code>	// by the GC before pointers into the span are published.</code></span>
<span class="codeline" id="line-1322"><code>	publicationBarrier()</code></span>
<span class="codeline" id="line-1323"><code></code></span>
<span class="codeline" id="line-1324"><code>	return s</code></span>
<span class="codeline" id="line-1325"><code>}</code></span>
<span class="codeline" id="line-1326"><code></code></span>
<span class="codeline" id="line-1327"><code>// Try to add at least npage pages of memory to the heap,</code></span>
<span class="codeline" id="line-1328"><code>// returning whether it worked.</code></span>
<span class="codeline" id="line-1329"><code>//</code></span>
<span class="codeline" id="line-1330"><code>// h.lock must be held.</code></span>
<span class="codeline" id="line-1331"><code>func (h *mheap) grow(npage uintptr) bool {</code></span>
<span class="codeline" id="line-1332"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-1333"><code></code></span>
<span class="codeline" id="line-1334"><code>	// We must grow the heap in whole palloc chunks.</code></span>
<span class="codeline" id="line-1335"><code>	ask := alignUp(npage, pallocChunkPages) * pageSize</code></span>
<span class="codeline" id="line-1336"><code></code></span>
<span class="codeline" id="line-1337"><code>	totalGrowth := uintptr(0)</code></span>
<span class="codeline" id="line-1338"><code>	// This may overflow because ask could be very large</code></span>
<span class="codeline" id="line-1339"><code>	// and is otherwise unrelated to h.curArena.base.</code></span>
<span class="codeline" id="line-1340"><code>	end := h.curArena.base + ask</code></span>
<span class="codeline" id="line-1341"><code>	nBase := alignUp(end, physPageSize)</code></span>
<span class="codeline" id="line-1342"><code>	if nBase &gt; h.curArena.end || /* overflow */ end &lt; h.curArena.base {</code></span>
<span class="codeline" id="line-1343"><code>		// Not enough room in the current arena. Allocate more</code></span>
<span class="codeline" id="line-1344"><code>		// arena space. This may not be contiguous with the</code></span>
<span class="codeline" id="line-1345"><code>		// current arena, so we have to request the full ask.</code></span>
<span class="codeline" id="line-1346"><code>		av, asize := h.sysAlloc(ask)</code></span>
<span class="codeline" id="line-1347"><code>		if av == nil {</code></span>
<span class="codeline" id="line-1348"><code>			print("runtime: out of memory: cannot allocate ", ask, "-byte block (", memstats.heap_sys, " in use)\n")</code></span>
<span class="codeline" id="line-1349"><code>			return false</code></span>
<span class="codeline" id="line-1350"><code>		}</code></span>
<span class="codeline" id="line-1351"><code></code></span>
<span class="codeline" id="line-1352"><code>		if uintptr(av) == h.curArena.end {</code></span>
<span class="codeline" id="line-1353"><code>			// The new space is contiguous with the old</code></span>
<span class="codeline" id="line-1354"><code>			// space, so just extend the current space.</code></span>
<span class="codeline" id="line-1355"><code>			h.curArena.end = uintptr(av) + asize</code></span>
<span class="codeline" id="line-1356"><code>		} else {</code></span>
<span class="codeline" id="line-1357"><code>			// The new space is discontiguous. Track what</code></span>
<span class="codeline" id="line-1358"><code>			// remains of the current space and switch to</code></span>
<span class="codeline" id="line-1359"><code>			// the new space. This should be rare.</code></span>
<span class="codeline" id="line-1360"><code>			if size := h.curArena.end - h.curArena.base; size != 0 {</code></span>
<span class="codeline" id="line-1361"><code>				h.pages.grow(h.curArena.base, size)</code></span>
<span class="codeline" id="line-1362"><code>				totalGrowth += size</code></span>
<span class="codeline" id="line-1363"><code>			}</code></span>
<span class="codeline" id="line-1364"><code>			// Switch to the new space.</code></span>
<span class="codeline" id="line-1365"><code>			h.curArena.base = uintptr(av)</code></span>
<span class="codeline" id="line-1366"><code>			h.curArena.end = uintptr(av) + asize</code></span>
<span class="codeline" id="line-1367"><code>		}</code></span>
<span class="codeline" id="line-1368"><code></code></span>
<span class="codeline" id="line-1369"><code>		// The memory just allocated counts as both released</code></span>
<span class="codeline" id="line-1370"><code>		// and idle, even though it's not yet backed by spans.</code></span>
<span class="codeline" id="line-1371"><code>		//</code></span>
<span class="codeline" id="line-1372"><code>		// The allocation is always aligned to the heap arena</code></span>
<span class="codeline" id="line-1373"><code>		// size which is always &gt; physPageSize, so its safe to</code></span>
<span class="codeline" id="line-1374"><code>		// just add directly to heap_released.</code></span>
<span class="codeline" id="line-1375"><code>		atomic.Xadd64(&amp;memstats.heap_released, int64(asize))</code></span>
<span class="codeline" id="line-1376"><code>		stats := memstats.heapStats.acquire()</code></span>
<span class="codeline" id="line-1377"><code>		atomic.Xaddint64(&amp;stats.released, int64(asize))</code></span>
<span class="codeline" id="line-1378"><code>		memstats.heapStats.release()</code></span>
<span class="codeline" id="line-1379"><code></code></span>
<span class="codeline" id="line-1380"><code>		// Recalculate nBase.</code></span>
<span class="codeline" id="line-1381"><code>		// We know this won't overflow, because sysAlloc returned</code></span>
<span class="codeline" id="line-1382"><code>		// a valid region starting at h.curArena.base which is at</code></span>
<span class="codeline" id="line-1383"><code>		// least ask bytes in size.</code></span>
<span class="codeline" id="line-1384"><code>		nBase = alignUp(h.curArena.base+ask, physPageSize)</code></span>
<span class="codeline" id="line-1385"><code>	}</code></span>
<span class="codeline" id="line-1386"><code></code></span>
<span class="codeline" id="line-1387"><code>	// Grow into the current arena.</code></span>
<span class="codeline" id="line-1388"><code>	v := h.curArena.base</code></span>
<span class="codeline" id="line-1389"><code>	h.curArena.base = nBase</code></span>
<span class="codeline" id="line-1390"><code>	h.pages.grow(v, nBase-v)</code></span>
<span class="codeline" id="line-1391"><code>	totalGrowth += nBase - v</code></span>
<span class="codeline" id="line-1392"><code></code></span>
<span class="codeline" id="line-1393"><code>	// We just caused a heap growth, so scavenge down what will soon be used.</code></span>
<span class="codeline" id="line-1394"><code>	// By scavenging inline we deal with the failure to allocate out of</code></span>
<span class="codeline" id="line-1395"><code>	// memory fragments by scavenging the memory fragments that are least</code></span>
<span class="codeline" id="line-1396"><code>	// likely to be re-used.</code></span>
<span class="codeline" id="line-1397"><code>	if retained := heapRetained(); retained+uint64(totalGrowth) &gt; h.scavengeGoal {</code></span>
<span class="codeline" id="line-1398"><code>		todo := totalGrowth</code></span>
<span class="codeline" id="line-1399"><code>		if overage := uintptr(retained + uint64(totalGrowth) - h.scavengeGoal); todo &gt; overage {</code></span>
<span class="codeline" id="line-1400"><code>			todo = overage</code></span>
<span class="codeline" id="line-1401"><code>		}</code></span>
<span class="codeline" id="line-1402"><code>		h.pages.scavenge(todo, false)</code></span>
<span class="codeline" id="line-1403"><code>	}</code></span>
<span class="codeline" id="line-1404"><code>	return true</code></span>
<span class="codeline" id="line-1405"><code>}</code></span>
<span class="codeline" id="line-1406"><code></code></span>
<span class="codeline" id="line-1407"><code>// Free the span back into the heap.</code></span>
<span class="codeline" id="line-1408"><code>func (h *mheap) freeSpan(s *mspan) {</code></span>
<span class="codeline" id="line-1409"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-1410"><code>		lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1411"><code>		if msanenabled {</code></span>
<span class="codeline" id="line-1412"><code>			// Tell msan that this entire span is no longer in use.</code></span>
<span class="codeline" id="line-1413"><code>			base := unsafe.Pointer(s.base())</code></span>
<span class="codeline" id="line-1414"><code>			bytes := s.npages &lt;&lt; _PageShift</code></span>
<span class="codeline" id="line-1415"><code>			msanfree(base, bytes)</code></span>
<span class="codeline" id="line-1416"><code>		}</code></span>
<span class="codeline" id="line-1417"><code>		h.freeSpanLocked(s, spanAllocHeap)</code></span>
<span class="codeline" id="line-1418"><code>		unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1419"><code>	})</code></span>
<span class="codeline" id="line-1420"><code>}</code></span>
<span class="codeline" id="line-1421"><code></code></span>
<span class="codeline" id="line-1422"><code>// freeManual frees a manually-managed span returned by allocManual.</code></span>
<span class="codeline" id="line-1423"><code>// typ must be the same as the spanAllocType passed to the allocManual that</code></span>
<span class="codeline" id="line-1424"><code>// allocated s.</code></span>
<span class="codeline" id="line-1425"><code>//</code></span>
<span class="codeline" id="line-1426"><code>// This must only be called when gcphase == _GCoff. See mSpanState for</code></span>
<span class="codeline" id="line-1427"><code>// an explanation.</code></span>
<span class="codeline" id="line-1428"><code>//</code></span>
<span class="codeline" id="line-1429"><code>// freeManual must be called on the system stack because it acquires</code></span>
<span class="codeline" id="line-1430"><code>// the heap lock. See mheap for details.</code></span>
<span class="codeline" id="line-1431"><code>//</code></span>
<span class="codeline" id="line-1432"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1433"><code>func (h *mheap) freeManual(s *mspan, typ spanAllocType) {</code></span>
<span class="codeline" id="line-1434"><code>	s.needzero = 1</code></span>
<span class="codeline" id="line-1435"><code>	lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1436"><code>	h.freeSpanLocked(s, typ)</code></span>
<span class="codeline" id="line-1437"><code>	unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1438"><code>}</code></span>
<span class="codeline" id="line-1439"><code></code></span>
<span class="codeline" id="line-1440"><code>func (h *mheap) freeSpanLocked(s *mspan, typ spanAllocType) {</code></span>
<span class="codeline" id="line-1441"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-1442"><code></code></span>
<span class="codeline" id="line-1443"><code>	switch s.state.get() {</code></span>
<span class="codeline" id="line-1444"><code>	case mSpanManual:</code></span>
<span class="codeline" id="line-1445"><code>		if s.allocCount != 0 {</code></span>
<span class="codeline" id="line-1446"><code>			throw("mheap.freeSpanLocked - invalid stack free")</code></span>
<span class="codeline" id="line-1447"><code>		}</code></span>
<span class="codeline" id="line-1448"><code>	case mSpanInUse:</code></span>
<span class="codeline" id="line-1449"><code>		if s.allocCount != 0 || s.sweepgen != h.sweepgen {</code></span>
<span class="codeline" id="line-1450"><code>			print("mheap.freeSpanLocked - span ", s, " ptr ", hex(s.base()), " allocCount ", s.allocCount, " sweepgen ", s.sweepgen, "/", h.sweepgen, "\n")</code></span>
<span class="codeline" id="line-1451"><code>			throw("mheap.freeSpanLocked - invalid free")</code></span>
<span class="codeline" id="line-1452"><code>		}</code></span>
<span class="codeline" id="line-1453"><code>		atomic.Xadd64(&amp;h.pagesInUse, -int64(s.npages))</code></span>
<span class="codeline" id="line-1454"><code></code></span>
<span class="codeline" id="line-1455"><code>		// Clear in-use bit in arena page bitmap.</code></span>
<span class="codeline" id="line-1456"><code>		arena, pageIdx, pageMask := pageIndexOf(s.base())</code></span>
<span class="codeline" id="line-1457"><code>		atomic.And8(&amp;arena.pageInUse[pageIdx], ^pageMask)</code></span>
<span class="codeline" id="line-1458"><code>	default:</code></span>
<span class="codeline" id="line-1459"><code>		throw("mheap.freeSpanLocked - invalid span state")</code></span>
<span class="codeline" id="line-1460"><code>	}</code></span>
<span class="codeline" id="line-1461"><code></code></span>
<span class="codeline" id="line-1462"><code>	// Update stats.</code></span>
<span class="codeline" id="line-1463"><code>	//</code></span>
<span class="codeline" id="line-1464"><code>	// Mirrors the code in allocSpan.</code></span>
<span class="codeline" id="line-1465"><code>	nbytes := s.npages * pageSize</code></span>
<span class="codeline" id="line-1466"><code>	if typ == spanAllocHeap {</code></span>
<span class="codeline" id="line-1467"><code>		atomic.Xadd64(&amp;memstats.heap_inuse, -int64(nbytes))</code></span>
<span class="codeline" id="line-1468"><code>	}</code></span>
<span class="codeline" id="line-1469"><code>	if typ.manual() {</code></span>
<span class="codeline" id="line-1470"><code>		// Manually managed memory doesn't count toward heap_sys, so add it back.</code></span>
<span class="codeline" id="line-1471"><code>		memstats.heap_sys.add(int64(nbytes))</code></span>
<span class="codeline" id="line-1472"><code>	}</code></span>
<span class="codeline" id="line-1473"><code>	// Update consistent stats.</code></span>
<span class="codeline" id="line-1474"><code>	stats := memstats.heapStats.acquire()</code></span>
<span class="codeline" id="line-1475"><code>	switch typ {</code></span>
<span class="codeline" id="line-1476"><code>	case spanAllocHeap:</code></span>
<span class="codeline" id="line-1477"><code>		atomic.Xaddint64(&amp;stats.inHeap, -int64(nbytes))</code></span>
<span class="codeline" id="line-1478"><code>	case spanAllocStack:</code></span>
<span class="codeline" id="line-1479"><code>		atomic.Xaddint64(&amp;stats.inStacks, -int64(nbytes))</code></span>
<span class="codeline" id="line-1480"><code>	case spanAllocPtrScalarBits:</code></span>
<span class="codeline" id="line-1481"><code>		atomic.Xaddint64(&amp;stats.inPtrScalarBits, -int64(nbytes))</code></span>
<span class="codeline" id="line-1482"><code>	case spanAllocWorkBuf:</code></span>
<span class="codeline" id="line-1483"><code>		atomic.Xaddint64(&amp;stats.inWorkBufs, -int64(nbytes))</code></span>
<span class="codeline" id="line-1484"><code>	}</code></span>
<span class="codeline" id="line-1485"><code>	memstats.heapStats.release()</code></span>
<span class="codeline" id="line-1486"><code></code></span>
<span class="codeline" id="line-1487"><code>	// Mark the space as free.</code></span>
<span class="codeline" id="line-1488"><code>	h.pages.free(s.base(), s.npages)</code></span>
<span class="codeline" id="line-1489"><code></code></span>
<span class="codeline" id="line-1490"><code>	// Free the span structure. We no longer have a use for it.</code></span>
<span class="codeline" id="line-1491"><code>	s.state.set(mSpanDead)</code></span>
<span class="codeline" id="line-1492"><code>	h.freeMSpanLocked(s)</code></span>
<span class="codeline" id="line-1493"><code>}</code></span>
<span class="codeline" id="line-1494"><code></code></span>
<span class="codeline" id="line-1495"><code>// scavengeAll acquires the heap lock (blocking any additional</code></span>
<span class="codeline" id="line-1496"><code>// manipulation of the page allocator) and iterates over the whole</code></span>
<span class="codeline" id="line-1497"><code>// heap, scavenging every free page available.</code></span>
<span class="codeline" id="line-1498"><code>func (h *mheap) scavengeAll() {</code></span>
<span class="codeline" id="line-1499"><code>	// Disallow malloc or panic while holding the heap lock. We do</code></span>
<span class="codeline" id="line-1500"><code>	// this here because this is a non-mallocgc entry-point to</code></span>
<span class="codeline" id="line-1501"><code>	// the mheap API.</code></span>
<span class="codeline" id="line-1502"><code>	gp := getg()</code></span>
<span class="codeline" id="line-1503"><code>	gp.m.mallocing++</code></span>
<span class="codeline" id="line-1504"><code>	lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1505"><code>	// Start a new scavenge generation so we have a chance to walk</code></span>
<span class="codeline" id="line-1506"><code>	// over the whole heap.</code></span>
<span class="codeline" id="line-1507"><code>	h.pages.scavengeStartGen()</code></span>
<span class="codeline" id="line-1508"><code>	released := h.pages.scavenge(^uintptr(0), false)</code></span>
<span class="codeline" id="line-1509"><code>	gen := h.pages.scav.gen</code></span>
<span class="codeline" id="line-1510"><code>	unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1511"><code>	gp.m.mallocing--</code></span>
<span class="codeline" id="line-1512"><code></code></span>
<span class="codeline" id="line-1513"><code>	if debug.scavtrace &gt; 0 {</code></span>
<span class="codeline" id="line-1514"><code>		printScavTrace(gen, released, true)</code></span>
<span class="codeline" id="line-1515"><code>	}</code></span>
<span class="codeline" id="line-1516"><code>}</code></span>
<span class="codeline" id="line-1517"><code></code></span>
<span class="codeline" id="line-1518"><code>//go:linkname runtime_debug_freeOSMemory runtime/debug.freeOSMemory</code></span>
<span class="codeline" id="line-1519"><code>func runtime_debug_freeOSMemory() {</code></span>
<span class="codeline" id="line-1520"><code>	GC()</code></span>
<span class="codeline" id="line-1521"><code>	systemstack(func() { mheap_.scavengeAll() })</code></span>
<span class="codeline" id="line-1522"><code>}</code></span>
<span class="codeline" id="line-1523"><code></code></span>
<span class="codeline" id="line-1524"><code>// Initialize a new span with the given start and npages.</code></span>
<span class="codeline" id="line-1525"><code>func (span *mspan) init(base uintptr, npages uintptr) {</code></span>
<span class="codeline" id="line-1526"><code>	// span is *not* zeroed.</code></span>
<span class="codeline" id="line-1527"><code>	span.next = nil</code></span>
<span class="codeline" id="line-1528"><code>	span.prev = nil</code></span>
<span class="codeline" id="line-1529"><code>	span.list = nil</code></span>
<span class="codeline" id="line-1530"><code>	span.startAddr = base</code></span>
<span class="codeline" id="line-1531"><code>	span.npages = npages</code></span>
<span class="codeline" id="line-1532"><code>	span.allocCount = 0</code></span>
<span class="codeline" id="line-1533"><code>	span.spanclass = 0</code></span>
<span class="codeline" id="line-1534"><code>	span.elemsize = 0</code></span>
<span class="codeline" id="line-1535"><code>	span.speciallock.key = 0</code></span>
<span class="codeline" id="line-1536"><code>	span.specials = nil</code></span>
<span class="codeline" id="line-1537"><code>	span.needzero = 0</code></span>
<span class="codeline" id="line-1538"><code>	span.freeindex = 0</code></span>
<span class="codeline" id="line-1539"><code>	span.allocBits = nil</code></span>
<span class="codeline" id="line-1540"><code>	span.gcmarkBits = nil</code></span>
<span class="codeline" id="line-1541"><code>	span.state.set(mSpanDead)</code></span>
<span class="codeline" id="line-1542"><code>	lockInit(&amp;span.speciallock, lockRankMspanSpecial)</code></span>
<span class="codeline" id="line-1543"><code>}</code></span>
<span class="codeline" id="line-1544"><code></code></span>
<span class="codeline" id="line-1545"><code>func (span *mspan) inList() bool {</code></span>
<span class="codeline" id="line-1546"><code>	return span.list != nil</code></span>
<span class="codeline" id="line-1547"><code>}</code></span>
<span class="codeline" id="line-1548"><code></code></span>
<span class="codeline" id="line-1549"><code>// Initialize an empty doubly-linked list.</code></span>
<span class="codeline" id="line-1550"><code>func (list *mSpanList) init() {</code></span>
<span class="codeline" id="line-1551"><code>	list.first = nil</code></span>
<span class="codeline" id="line-1552"><code>	list.last = nil</code></span>
<span class="codeline" id="line-1553"><code>}</code></span>
<span class="codeline" id="line-1554"><code></code></span>
<span class="codeline" id="line-1555"><code>func (list *mSpanList) remove(span *mspan) {</code></span>
<span class="codeline" id="line-1556"><code>	if span.list != list {</code></span>
<span class="codeline" id="line-1557"><code>		print("runtime: failed mSpanList.remove span.npages=", span.npages,</code></span>
<span class="codeline" id="line-1558"><code>			" span=", span, " prev=", span.prev, " span.list=", span.list, " list=", list, "\n")</code></span>
<span class="codeline" id="line-1559"><code>		throw("mSpanList.remove")</code></span>
<span class="codeline" id="line-1560"><code>	}</code></span>
<span class="codeline" id="line-1561"><code>	if list.first == span {</code></span>
<span class="codeline" id="line-1562"><code>		list.first = span.next</code></span>
<span class="codeline" id="line-1563"><code>	} else {</code></span>
<span class="codeline" id="line-1564"><code>		span.prev.next = span.next</code></span>
<span class="codeline" id="line-1565"><code>	}</code></span>
<span class="codeline" id="line-1566"><code>	if list.last == span {</code></span>
<span class="codeline" id="line-1567"><code>		list.last = span.prev</code></span>
<span class="codeline" id="line-1568"><code>	} else {</code></span>
<span class="codeline" id="line-1569"><code>		span.next.prev = span.prev</code></span>
<span class="codeline" id="line-1570"><code>	}</code></span>
<span class="codeline" id="line-1571"><code>	span.next = nil</code></span>
<span class="codeline" id="line-1572"><code>	span.prev = nil</code></span>
<span class="codeline" id="line-1573"><code>	span.list = nil</code></span>
<span class="codeline" id="line-1574"><code>}</code></span>
<span class="codeline" id="line-1575"><code></code></span>
<span class="codeline" id="line-1576"><code>func (list *mSpanList) isEmpty() bool {</code></span>
<span class="codeline" id="line-1577"><code>	return list.first == nil</code></span>
<span class="codeline" id="line-1578"><code>}</code></span>
<span class="codeline" id="line-1579"><code></code></span>
<span class="codeline" id="line-1580"><code>func (list *mSpanList) insert(span *mspan) {</code></span>
<span class="codeline" id="line-1581"><code>	if span.next != nil || span.prev != nil || span.list != nil {</code></span>
<span class="codeline" id="line-1582"><code>		println("runtime: failed mSpanList.insert", span, span.next, span.prev, span.list)</code></span>
<span class="codeline" id="line-1583"><code>		throw("mSpanList.insert")</code></span>
<span class="codeline" id="line-1584"><code>	}</code></span>
<span class="codeline" id="line-1585"><code>	span.next = list.first</code></span>
<span class="codeline" id="line-1586"><code>	if list.first != nil {</code></span>
<span class="codeline" id="line-1587"><code>		// The list contains at least one span; link it in.</code></span>
<span class="codeline" id="line-1588"><code>		// The last span in the list doesn't change.</code></span>
<span class="codeline" id="line-1589"><code>		list.first.prev = span</code></span>
<span class="codeline" id="line-1590"><code>	} else {</code></span>
<span class="codeline" id="line-1591"><code>		// The list contains no spans, so this is also the last span.</code></span>
<span class="codeline" id="line-1592"><code>		list.last = span</code></span>
<span class="codeline" id="line-1593"><code>	}</code></span>
<span class="codeline" id="line-1594"><code>	list.first = span</code></span>
<span class="codeline" id="line-1595"><code>	span.list = list</code></span>
<span class="codeline" id="line-1596"><code>}</code></span>
<span class="codeline" id="line-1597"><code></code></span>
<span class="codeline" id="line-1598"><code>func (list *mSpanList) insertBack(span *mspan) {</code></span>
<span class="codeline" id="line-1599"><code>	if span.next != nil || span.prev != nil || span.list != nil {</code></span>
<span class="codeline" id="line-1600"><code>		println("runtime: failed mSpanList.insertBack", span, span.next, span.prev, span.list)</code></span>
<span class="codeline" id="line-1601"><code>		throw("mSpanList.insertBack")</code></span>
<span class="codeline" id="line-1602"><code>	}</code></span>
<span class="codeline" id="line-1603"><code>	span.prev = list.last</code></span>
<span class="codeline" id="line-1604"><code>	if list.last != nil {</code></span>
<span class="codeline" id="line-1605"><code>		// The list contains at least one span.</code></span>
<span class="codeline" id="line-1606"><code>		list.last.next = span</code></span>
<span class="codeline" id="line-1607"><code>	} else {</code></span>
<span class="codeline" id="line-1608"><code>		// The list contains no spans, so this is also the first span.</code></span>
<span class="codeline" id="line-1609"><code>		list.first = span</code></span>
<span class="codeline" id="line-1610"><code>	}</code></span>
<span class="codeline" id="line-1611"><code>	list.last = span</code></span>
<span class="codeline" id="line-1612"><code>	span.list = list</code></span>
<span class="codeline" id="line-1613"><code>}</code></span>
<span class="codeline" id="line-1614"><code></code></span>
<span class="codeline" id="line-1615"><code>// takeAll removes all spans from other and inserts them at the front</code></span>
<span class="codeline" id="line-1616"><code>// of list.</code></span>
<span class="codeline" id="line-1617"><code>func (list *mSpanList) takeAll(other *mSpanList) {</code></span>
<span class="codeline" id="line-1618"><code>	if other.isEmpty() {</code></span>
<span class="codeline" id="line-1619"><code>		return</code></span>
<span class="codeline" id="line-1620"><code>	}</code></span>
<span class="codeline" id="line-1621"><code></code></span>
<span class="codeline" id="line-1622"><code>	// Reparent everything in other to list.</code></span>
<span class="codeline" id="line-1623"><code>	for s := other.first; s != nil; s = s.next {</code></span>
<span class="codeline" id="line-1624"><code>		s.list = list</code></span>
<span class="codeline" id="line-1625"><code>	}</code></span>
<span class="codeline" id="line-1626"><code></code></span>
<span class="codeline" id="line-1627"><code>	// Concatenate the lists.</code></span>
<span class="codeline" id="line-1628"><code>	if list.isEmpty() {</code></span>
<span class="codeline" id="line-1629"><code>		*list = *other</code></span>
<span class="codeline" id="line-1630"><code>	} else {</code></span>
<span class="codeline" id="line-1631"><code>		// Neither list is empty. Put other before list.</code></span>
<span class="codeline" id="line-1632"><code>		other.last.next = list.first</code></span>
<span class="codeline" id="line-1633"><code>		list.first.prev = other.last</code></span>
<span class="codeline" id="line-1634"><code>		list.first = other.first</code></span>
<span class="codeline" id="line-1635"><code>	}</code></span>
<span class="codeline" id="line-1636"><code></code></span>
<span class="codeline" id="line-1637"><code>	other.first, other.last = nil, nil</code></span>
<span class="codeline" id="line-1638"><code>}</code></span>
<span class="codeline" id="line-1639"><code></code></span>
<span class="codeline" id="line-1640"><code>const (</code></span>
<span class="codeline" id="line-1641"><code>	_KindSpecialFinalizer = 1</code></span>
<span class="codeline" id="line-1642"><code>	_KindSpecialProfile   = 2</code></span>
<span class="codeline" id="line-1643"><code>	// Note: The finalizer special must be first because if we're freeing</code></span>
<span class="codeline" id="line-1644"><code>	// an object, a finalizer special will cause the freeing operation</code></span>
<span class="codeline" id="line-1645"><code>	// to abort, and we want to keep the other special records around</code></span>
<span class="codeline" id="line-1646"><code>	// if that happens.</code></span>
<span class="codeline" id="line-1647"><code>)</code></span>
<span class="codeline" id="line-1648"><code></code></span>
<span class="codeline" id="line-1649"><code>//go:notinheap</code></span>
<span class="codeline" id="line-1650"><code>type special struct {</code></span>
<span class="codeline" id="line-1651"><code>	next   *special // linked list in span</code></span>
<span class="codeline" id="line-1652"><code>	offset uint16   // span offset of object</code></span>
<span class="codeline" id="line-1653"><code>	kind   byte     // kind of special</code></span>
<span class="codeline" id="line-1654"><code>}</code></span>
<span class="codeline" id="line-1655"><code></code></span>
<span class="codeline" id="line-1656"><code>// spanHasSpecials marks a span as having specials in the arena bitmap.</code></span>
<span class="codeline" id="line-1657"><code>func spanHasSpecials(s *mspan) {</code></span>
<span class="codeline" id="line-1658"><code>	arenaPage := (s.base() / pageSize) % pagesPerArena</code></span>
<span class="codeline" id="line-1659"><code>	ai := arenaIndex(s.base())</code></span>
<span class="codeline" id="line-1660"><code>	ha := mheap_.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-1661"><code>	atomic.Or8(&amp;ha.pageSpecials[arenaPage/8], uint8(1)&lt;&lt;(arenaPage%8))</code></span>
<span class="codeline" id="line-1662"><code>}</code></span>
<span class="codeline" id="line-1663"><code></code></span>
<span class="codeline" id="line-1664"><code>// spanHasNoSpecials marks a span as having no specials in the arena bitmap.</code></span>
<span class="codeline" id="line-1665"><code>func spanHasNoSpecials(s *mspan) {</code></span>
<span class="codeline" id="line-1666"><code>	arenaPage := (s.base() / pageSize) % pagesPerArena</code></span>
<span class="codeline" id="line-1667"><code>	ai := arenaIndex(s.base())</code></span>
<span class="codeline" id="line-1668"><code>	ha := mheap_.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-1669"><code>	atomic.And8(&amp;ha.pageSpecials[arenaPage/8], ^(uint8(1) &lt;&lt; (arenaPage % 8)))</code></span>
<span class="codeline" id="line-1670"><code>}</code></span>
<span class="codeline" id="line-1671"><code></code></span>
<span class="codeline" id="line-1672"><code>// Adds the special record s to the list of special records for</code></span>
<span class="codeline" id="line-1673"><code>// the object p. All fields of s should be filled in except for</code></span>
<span class="codeline" id="line-1674"><code>// offset &amp; next, which this routine will fill in.</code></span>
<span class="codeline" id="line-1675"><code>// Returns true if the special was successfully added, false otherwise.</code></span>
<span class="codeline" id="line-1676"><code>// (The add will fail only if a record with the same p and s-&gt;kind</code></span>
<span class="codeline" id="line-1677"><code>//  already exists.)</code></span>
<span class="codeline" id="line-1678"><code>func addspecial(p unsafe.Pointer, s *special) bool {</code></span>
<span class="codeline" id="line-1679"><code>	span := spanOfHeap(uintptr(p))</code></span>
<span class="codeline" id="line-1680"><code>	if span == nil {</code></span>
<span class="codeline" id="line-1681"><code>		throw("addspecial on invalid pointer")</code></span>
<span class="codeline" id="line-1682"><code>	}</code></span>
<span class="codeline" id="line-1683"><code></code></span>
<span class="codeline" id="line-1684"><code>	// Ensure that the span is swept.</code></span>
<span class="codeline" id="line-1685"><code>	// Sweeping accesses the specials list w/o locks, so we have</code></span>
<span class="codeline" id="line-1686"><code>	// to synchronize with it. And it's just much safer.</code></span>
<span class="codeline" id="line-1687"><code>	mp := acquirem()</code></span>
<span class="codeline" id="line-1688"><code>	span.ensureSwept()</code></span>
<span class="codeline" id="line-1689"><code></code></span>
<span class="codeline" id="line-1690"><code>	offset := uintptr(p) - span.base()</code></span>
<span class="codeline" id="line-1691"><code>	kind := s.kind</code></span>
<span class="codeline" id="line-1692"><code></code></span>
<span class="codeline" id="line-1693"><code>	lock(&amp;span.speciallock)</code></span>
<span class="codeline" id="line-1694"><code></code></span>
<span class="codeline" id="line-1695"><code>	// Find splice point, check for existing record.</code></span>
<span class="codeline" id="line-1696"><code>	t := &amp;span.specials</code></span>
<span class="codeline" id="line-1697"><code>	for {</code></span>
<span class="codeline" id="line-1698"><code>		x := *t</code></span>
<span class="codeline" id="line-1699"><code>		if x == nil {</code></span>
<span class="codeline" id="line-1700"><code>			break</code></span>
<span class="codeline" id="line-1701"><code>		}</code></span>
<span class="codeline" id="line-1702"><code>		if offset == uintptr(x.offset) &amp;&amp; kind == x.kind {</code></span>
<span class="codeline" id="line-1703"><code>			unlock(&amp;span.speciallock)</code></span>
<span class="codeline" id="line-1704"><code>			releasem(mp)</code></span>
<span class="codeline" id="line-1705"><code>			return false // already exists</code></span>
<span class="codeline" id="line-1706"><code>		}</code></span>
<span class="codeline" id="line-1707"><code>		if offset &lt; uintptr(x.offset) || (offset == uintptr(x.offset) &amp;&amp; kind &lt; x.kind) {</code></span>
<span class="codeline" id="line-1708"><code>			break</code></span>
<span class="codeline" id="line-1709"><code>		}</code></span>
<span class="codeline" id="line-1710"><code>		t = &amp;x.next</code></span>
<span class="codeline" id="line-1711"><code>	}</code></span>
<span class="codeline" id="line-1712"><code></code></span>
<span class="codeline" id="line-1713"><code>	// Splice in record, fill in offset.</code></span>
<span class="codeline" id="line-1714"><code>	s.offset = uint16(offset)</code></span>
<span class="codeline" id="line-1715"><code>	s.next = *t</code></span>
<span class="codeline" id="line-1716"><code>	*t = s</code></span>
<span class="codeline" id="line-1717"><code>	spanHasSpecials(span)</code></span>
<span class="codeline" id="line-1718"><code>	unlock(&amp;span.speciallock)</code></span>
<span class="codeline" id="line-1719"><code>	releasem(mp)</code></span>
<span class="codeline" id="line-1720"><code></code></span>
<span class="codeline" id="line-1721"><code>	return true</code></span>
<span class="codeline" id="line-1722"><code>}</code></span>
<span class="codeline" id="line-1723"><code></code></span>
<span class="codeline" id="line-1724"><code>// Removes the Special record of the given kind for the object p.</code></span>
<span class="codeline" id="line-1725"><code>// Returns the record if the record existed, nil otherwise.</code></span>
<span class="codeline" id="line-1726"><code>// The caller must FixAlloc_Free the result.</code></span>
<span class="codeline" id="line-1727"><code>func removespecial(p unsafe.Pointer, kind uint8) *special {</code></span>
<span class="codeline" id="line-1728"><code>	span := spanOfHeap(uintptr(p))</code></span>
<span class="codeline" id="line-1729"><code>	if span == nil {</code></span>
<span class="codeline" id="line-1730"><code>		throw("removespecial on invalid pointer")</code></span>
<span class="codeline" id="line-1731"><code>	}</code></span>
<span class="codeline" id="line-1732"><code></code></span>
<span class="codeline" id="line-1733"><code>	// Ensure that the span is swept.</code></span>
<span class="codeline" id="line-1734"><code>	// Sweeping accesses the specials list w/o locks, so we have</code></span>
<span class="codeline" id="line-1735"><code>	// to synchronize with it. And it's just much safer.</code></span>
<span class="codeline" id="line-1736"><code>	mp := acquirem()</code></span>
<span class="codeline" id="line-1737"><code>	span.ensureSwept()</code></span>
<span class="codeline" id="line-1738"><code></code></span>
<span class="codeline" id="line-1739"><code>	offset := uintptr(p) - span.base()</code></span>
<span class="codeline" id="line-1740"><code></code></span>
<span class="codeline" id="line-1741"><code>	var result *special</code></span>
<span class="codeline" id="line-1742"><code>	lock(&amp;span.speciallock)</code></span>
<span class="codeline" id="line-1743"><code>	t := &amp;span.specials</code></span>
<span class="codeline" id="line-1744"><code>	for {</code></span>
<span class="codeline" id="line-1745"><code>		s := *t</code></span>
<span class="codeline" id="line-1746"><code>		if s == nil {</code></span>
<span class="codeline" id="line-1747"><code>			break</code></span>
<span class="codeline" id="line-1748"><code>		}</code></span>
<span class="codeline" id="line-1749"><code>		// This function is used for finalizers only, so we don't check for</code></span>
<span class="codeline" id="line-1750"><code>		// "interior" specials (p must be exactly equal to s-&gt;offset).</code></span>
<span class="codeline" id="line-1751"><code>		if offset == uintptr(s.offset) &amp;&amp; kind == s.kind {</code></span>
<span class="codeline" id="line-1752"><code>			*t = s.next</code></span>
<span class="codeline" id="line-1753"><code>			result = s</code></span>
<span class="codeline" id="line-1754"><code>			break</code></span>
<span class="codeline" id="line-1755"><code>		}</code></span>
<span class="codeline" id="line-1756"><code>		t = &amp;s.next</code></span>
<span class="codeline" id="line-1757"><code>	}</code></span>
<span class="codeline" id="line-1758"><code>	if span.specials == nil {</code></span>
<span class="codeline" id="line-1759"><code>		spanHasNoSpecials(span)</code></span>
<span class="codeline" id="line-1760"><code>	}</code></span>
<span class="codeline" id="line-1761"><code>	unlock(&amp;span.speciallock)</code></span>
<span class="codeline" id="line-1762"><code>	releasem(mp)</code></span>
<span class="codeline" id="line-1763"><code>	return result</code></span>
<span class="codeline" id="line-1764"><code>}</code></span>
<span class="codeline" id="line-1765"><code></code></span>
<span class="codeline" id="line-1766"><code>// The described object has a finalizer set for it.</code></span>
<span class="codeline" id="line-1767"><code>//</code></span>
<span class="codeline" id="line-1768"><code>// specialfinalizer is allocated from non-GC'd memory, so any heap</code></span>
<span class="codeline" id="line-1769"><code>// pointers must be specially handled.</code></span>
<span class="codeline" id="line-1770"><code>//</code></span>
<span class="codeline" id="line-1771"><code>//go:notinheap</code></span>
<span class="codeline" id="line-1772"><code>type specialfinalizer struct {</code></span>
<span class="codeline" id="line-1773"><code>	special special</code></span>
<span class="codeline" id="line-1774"><code>	fn      *funcval // May be a heap pointer.</code></span>
<span class="codeline" id="line-1775"><code>	nret    uintptr</code></span>
<span class="codeline" id="line-1776"><code>	fint    *_type   // May be a heap pointer, but always live.</code></span>
<span class="codeline" id="line-1777"><code>	ot      *ptrtype // May be a heap pointer, but always live.</code></span>
<span class="codeline" id="line-1778"><code>}</code></span>
<span class="codeline" id="line-1779"><code></code></span>
<span class="codeline" id="line-1780"><code>// Adds a finalizer to the object p. Returns true if it succeeded.</code></span>
<span class="codeline" id="line-1781"><code>func addfinalizer(p unsafe.Pointer, f *funcval, nret uintptr, fint *_type, ot *ptrtype) bool {</code></span>
<span class="codeline" id="line-1782"><code>	lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1783"><code>	s := (*specialfinalizer)(mheap_.specialfinalizeralloc.alloc())</code></span>
<span class="codeline" id="line-1784"><code>	unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1785"><code>	s.special.kind = _KindSpecialFinalizer</code></span>
<span class="codeline" id="line-1786"><code>	s.fn = f</code></span>
<span class="codeline" id="line-1787"><code>	s.nret = nret</code></span>
<span class="codeline" id="line-1788"><code>	s.fint = fint</code></span>
<span class="codeline" id="line-1789"><code>	s.ot = ot</code></span>
<span class="codeline" id="line-1790"><code>	if addspecial(p, &amp;s.special) {</code></span>
<span class="codeline" id="line-1791"><code>		// This is responsible for maintaining the same</code></span>
<span class="codeline" id="line-1792"><code>		// GC-related invariants as markrootSpans in any</code></span>
<span class="codeline" id="line-1793"><code>		// situation where it's possible that markrootSpans</code></span>
<span class="codeline" id="line-1794"><code>		// has already run but mark termination hasn't yet.</code></span>
<span class="codeline" id="line-1795"><code>		if gcphase != _GCoff {</code></span>
<span class="codeline" id="line-1796"><code>			base, _, _ := findObject(uintptr(p), 0, 0)</code></span>
<span class="codeline" id="line-1797"><code>			mp := acquirem()</code></span>
<span class="codeline" id="line-1798"><code>			gcw := &amp;mp.p.ptr().gcw</code></span>
<span class="codeline" id="line-1799"><code>			// Mark everything reachable from the object</code></span>
<span class="codeline" id="line-1800"><code>			// so it's retained for the finalizer.</code></span>
<span class="codeline" id="line-1801"><code>			scanobject(base, gcw)</code></span>
<span class="codeline" id="line-1802"><code>			// Mark the finalizer itself, since the</code></span>
<span class="codeline" id="line-1803"><code>			// special isn't part of the GC'd heap.</code></span>
<span class="codeline" id="line-1804"><code>			scanblock(uintptr(unsafe.Pointer(&amp;s.fn)), sys.PtrSize, &amp;oneptrmask[0], gcw, nil)</code></span>
<span class="codeline" id="line-1805"><code>			releasem(mp)</code></span>
<span class="codeline" id="line-1806"><code>		}</code></span>
<span class="codeline" id="line-1807"><code>		return true</code></span>
<span class="codeline" id="line-1808"><code>	}</code></span>
<span class="codeline" id="line-1809"><code></code></span>
<span class="codeline" id="line-1810"><code>	// There was an old finalizer</code></span>
<span class="codeline" id="line-1811"><code>	lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1812"><code>	mheap_.specialfinalizeralloc.free(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-1813"><code>	unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1814"><code>	return false</code></span>
<span class="codeline" id="line-1815"><code>}</code></span>
<span class="codeline" id="line-1816"><code></code></span>
<span class="codeline" id="line-1817"><code>// Removes the finalizer (if any) from the object p.</code></span>
<span class="codeline" id="line-1818"><code>func removefinalizer(p unsafe.Pointer) {</code></span>
<span class="codeline" id="line-1819"><code>	s := (*specialfinalizer)(unsafe.Pointer(removespecial(p, _KindSpecialFinalizer)))</code></span>
<span class="codeline" id="line-1820"><code>	if s == nil {</code></span>
<span class="codeline" id="line-1821"><code>		return // there wasn't a finalizer to remove</code></span>
<span class="codeline" id="line-1822"><code>	}</code></span>
<span class="codeline" id="line-1823"><code>	lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1824"><code>	mheap_.specialfinalizeralloc.free(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-1825"><code>	unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1826"><code>}</code></span>
<span class="codeline" id="line-1827"><code></code></span>
<span class="codeline" id="line-1828"><code>// The described object is being heap profiled.</code></span>
<span class="codeline" id="line-1829"><code>//</code></span>
<span class="codeline" id="line-1830"><code>//go:notinheap</code></span>
<span class="codeline" id="line-1831"><code>type specialprofile struct {</code></span>
<span class="codeline" id="line-1832"><code>	special special</code></span>
<span class="codeline" id="line-1833"><code>	b       *bucket</code></span>
<span class="codeline" id="line-1834"><code>}</code></span>
<span class="codeline" id="line-1835"><code></code></span>
<span class="codeline" id="line-1836"><code>// Set the heap profile bucket associated with addr to b.</code></span>
<span class="codeline" id="line-1837"><code>func setprofilebucket(p unsafe.Pointer, b *bucket) {</code></span>
<span class="codeline" id="line-1838"><code>	lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1839"><code>	s := (*specialprofile)(mheap_.specialprofilealloc.alloc())</code></span>
<span class="codeline" id="line-1840"><code>	unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1841"><code>	s.special.kind = _KindSpecialProfile</code></span>
<span class="codeline" id="line-1842"><code>	s.b = b</code></span>
<span class="codeline" id="line-1843"><code>	if !addspecial(p, &amp;s.special) {</code></span>
<span class="codeline" id="line-1844"><code>		throw("setprofilebucket: profile already set")</code></span>
<span class="codeline" id="line-1845"><code>	}</code></span>
<span class="codeline" id="line-1846"><code>}</code></span>
<span class="codeline" id="line-1847"><code></code></span>
<span class="codeline" id="line-1848"><code>// Do whatever cleanup needs to be done to deallocate s. It has</code></span>
<span class="codeline" id="line-1849"><code>// already been unlinked from the mspan specials list.</code></span>
<span class="codeline" id="line-1850"><code>func freespecial(s *special, p unsafe.Pointer, size uintptr) {</code></span>
<span class="codeline" id="line-1851"><code>	switch s.kind {</code></span>
<span class="codeline" id="line-1852"><code>	case _KindSpecialFinalizer:</code></span>
<span class="codeline" id="line-1853"><code>		sf := (*specialfinalizer)(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-1854"><code>		queuefinalizer(p, sf.fn, sf.nret, sf.fint, sf.ot)</code></span>
<span class="codeline" id="line-1855"><code>		lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1856"><code>		mheap_.specialfinalizeralloc.free(unsafe.Pointer(sf))</code></span>
<span class="codeline" id="line-1857"><code>		unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1858"><code>	case _KindSpecialProfile:</code></span>
<span class="codeline" id="line-1859"><code>		sp := (*specialprofile)(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-1860"><code>		mProf_Free(sp.b, size)</code></span>
<span class="codeline" id="line-1861"><code>		lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1862"><code>		mheap_.specialprofilealloc.free(unsafe.Pointer(sp))</code></span>
<span class="codeline" id="line-1863"><code>		unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1864"><code>	default:</code></span>
<span class="codeline" id="line-1865"><code>		throw("bad special kind")</code></span>
<span class="codeline" id="line-1866"><code>		panic("not reached")</code></span>
<span class="codeline" id="line-1867"><code>	}</code></span>
<span class="codeline" id="line-1868"><code>}</code></span>
<span class="codeline" id="line-1869"><code></code></span>
<span class="codeline" id="line-1870"><code>// gcBits is an alloc/mark bitmap. This is always used as *gcBits.</code></span>
<span class="codeline" id="line-1871"><code>//</code></span>
<span class="codeline" id="line-1872"><code>//go:notinheap</code></span>
<span class="codeline" id="line-1873"><code>type gcBits uint8</code></span>
<span class="codeline" id="line-1874"><code></code></span>
<span class="codeline" id="line-1875"><code>// bytep returns a pointer to the n'th byte of b.</code></span>
<span class="codeline" id="line-1876"><code>func (b *gcBits) bytep(n uintptr) *uint8 {</code></span>
<span class="codeline" id="line-1877"><code>	return addb((*uint8)(b), n)</code></span>
<span class="codeline" id="line-1878"><code>}</code></span>
<span class="codeline" id="line-1879"><code></code></span>
<span class="codeline" id="line-1880"><code>// bitp returns a pointer to the byte containing bit n and a mask for</code></span>
<span class="codeline" id="line-1881"><code>// selecting that bit from *bytep.</code></span>
<span class="codeline" id="line-1882"><code>func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8) {</code></span>
<span class="codeline" id="line-1883"><code>	return b.bytep(n / 8), 1 &lt;&lt; (n % 8)</code></span>
<span class="codeline" id="line-1884"><code>}</code></span>
<span class="codeline" id="line-1885"><code></code></span>
<span class="codeline" id="line-1886"><code>const gcBitsChunkBytes = uintptr(64 &lt;&lt; 10)</code></span>
<span class="codeline" id="line-1887"><code>const gcBitsHeaderBytes = unsafe.Sizeof(gcBitsHeader{})</code></span>
<span class="codeline" id="line-1888"><code></code></span>
<span class="codeline" id="line-1889"><code>type gcBitsHeader struct {</code></span>
<span class="codeline" id="line-1890"><code>	free uintptr // free is the index into bits of the next free byte.</code></span>
<span class="codeline" id="line-1891"><code>	next uintptr // *gcBits triggers recursive type bug. (issue 14620)</code></span>
<span class="codeline" id="line-1892"><code>}</code></span>
<span class="codeline" id="line-1893"><code></code></span>
<span class="codeline" id="line-1894"><code>//go:notinheap</code></span>
<span class="codeline" id="line-1895"><code>type gcBitsArena struct {</code></span>
<span class="codeline" id="line-1896"><code>	// gcBitsHeader // side step recursive type bug (issue 14620) by including fields by hand.</code></span>
<span class="codeline" id="line-1897"><code>	free uintptr // free is the index into bits of the next free byte; read/write atomically</code></span>
<span class="codeline" id="line-1898"><code>	next *gcBitsArena</code></span>
<span class="codeline" id="line-1899"><code>	bits [gcBitsChunkBytes - gcBitsHeaderBytes]gcBits</code></span>
<span class="codeline" id="line-1900"><code>}</code></span>
<span class="codeline" id="line-1901"><code></code></span>
<span class="codeline" id="line-1902"><code>var gcBitsArenas struct {</code></span>
<span class="codeline" id="line-1903"><code>	lock     mutex</code></span>
<span class="codeline" id="line-1904"><code>	free     *gcBitsArena</code></span>
<span class="codeline" id="line-1905"><code>	next     *gcBitsArena // Read atomically. Write atomically under lock.</code></span>
<span class="codeline" id="line-1906"><code>	current  *gcBitsArena</code></span>
<span class="codeline" id="line-1907"><code>	previous *gcBitsArena</code></span>
<span class="codeline" id="line-1908"><code>}</code></span>
<span class="codeline" id="line-1909"><code></code></span>
<span class="codeline" id="line-1910"><code>// tryAlloc allocates from b or returns nil if b does not have enough room.</code></span>
<span class="codeline" id="line-1911"><code>// This is safe to call concurrently.</code></span>
<span class="codeline" id="line-1912"><code>func (b *gcBitsArena) tryAlloc(bytes uintptr) *gcBits {</code></span>
<span class="codeline" id="line-1913"><code>	if b == nil || atomic.Loaduintptr(&amp;b.free)+bytes &gt; uintptr(len(b.bits)) {</code></span>
<span class="codeline" id="line-1914"><code>		return nil</code></span>
<span class="codeline" id="line-1915"><code>	}</code></span>
<span class="codeline" id="line-1916"><code>	// Try to allocate from this block.</code></span>
<span class="codeline" id="line-1917"><code>	end := atomic.Xadduintptr(&amp;b.free, bytes)</code></span>
<span class="codeline" id="line-1918"><code>	if end &gt; uintptr(len(b.bits)) {</code></span>
<span class="codeline" id="line-1919"><code>		return nil</code></span>
<span class="codeline" id="line-1920"><code>	}</code></span>
<span class="codeline" id="line-1921"><code>	// There was enough room.</code></span>
<span class="codeline" id="line-1922"><code>	start := end - bytes</code></span>
<span class="codeline" id="line-1923"><code>	return &amp;b.bits[start]</code></span>
<span class="codeline" id="line-1924"><code>}</code></span>
<span class="codeline" id="line-1925"><code></code></span>
<span class="codeline" id="line-1926"><code>// newMarkBits returns a pointer to 8 byte aligned bytes</code></span>
<span class="codeline" id="line-1927"><code>// to be used for a span's mark bits.</code></span>
<span class="codeline" id="line-1928"><code>func newMarkBits(nelems uintptr) *gcBits {</code></span>
<span class="codeline" id="line-1929"><code>	blocksNeeded := uintptr((nelems + 63) / 64)</code></span>
<span class="codeline" id="line-1930"><code>	bytesNeeded := blocksNeeded * 8</code></span>
<span class="codeline" id="line-1931"><code></code></span>
<span class="codeline" id="line-1932"><code>	// Try directly allocating from the current head arena.</code></span>
<span class="codeline" id="line-1933"><code>	head := (*gcBitsArena)(atomic.Loadp(unsafe.Pointer(&amp;gcBitsArenas.next)))</code></span>
<span class="codeline" id="line-1934"><code>	if p := head.tryAlloc(bytesNeeded); p != nil {</code></span>
<span class="codeline" id="line-1935"><code>		return p</code></span>
<span class="codeline" id="line-1936"><code>	}</code></span>
<span class="codeline" id="line-1937"><code></code></span>
<span class="codeline" id="line-1938"><code>	// There's not enough room in the head arena. We may need to</code></span>
<span class="codeline" id="line-1939"><code>	// allocate a new arena.</code></span>
<span class="codeline" id="line-1940"><code>	lock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-1941"><code>	// Try the head arena again, since it may have changed. Now</code></span>
<span class="codeline" id="line-1942"><code>	// that we hold the lock, the list head can't change, but its</code></span>
<span class="codeline" id="line-1943"><code>	// free position still can.</code></span>
<span class="codeline" id="line-1944"><code>	if p := gcBitsArenas.next.tryAlloc(bytesNeeded); p != nil {</code></span>
<span class="codeline" id="line-1945"><code>		unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-1946"><code>		return p</code></span>
<span class="codeline" id="line-1947"><code>	}</code></span>
<span class="codeline" id="line-1948"><code></code></span>
<span class="codeline" id="line-1949"><code>	// Allocate a new arena. This may temporarily drop the lock.</code></span>
<span class="codeline" id="line-1950"><code>	fresh := newArenaMayUnlock()</code></span>
<span class="codeline" id="line-1951"><code>	// If newArenaMayUnlock dropped the lock, another thread may</code></span>
<span class="codeline" id="line-1952"><code>	// have put a fresh arena on the "next" list. Try allocating</code></span>
<span class="codeline" id="line-1953"><code>	// from next again.</code></span>
<span class="codeline" id="line-1954"><code>	if p := gcBitsArenas.next.tryAlloc(bytesNeeded); p != nil {</code></span>
<span class="codeline" id="line-1955"><code>		// Put fresh back on the free list.</code></span>
<span class="codeline" id="line-1956"><code>		// TODO: Mark it "already zeroed"</code></span>
<span class="codeline" id="line-1957"><code>		fresh.next = gcBitsArenas.free</code></span>
<span class="codeline" id="line-1958"><code>		gcBitsArenas.free = fresh</code></span>
<span class="codeline" id="line-1959"><code>		unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-1960"><code>		return p</code></span>
<span class="codeline" id="line-1961"><code>	}</code></span>
<span class="codeline" id="line-1962"><code></code></span>
<span class="codeline" id="line-1963"><code>	// Allocate from the fresh arena. We haven't linked it in yet, so</code></span>
<span class="codeline" id="line-1964"><code>	// this cannot race and is guaranteed to succeed.</code></span>
<span class="codeline" id="line-1965"><code>	p := fresh.tryAlloc(bytesNeeded)</code></span>
<span class="codeline" id="line-1966"><code>	if p == nil {</code></span>
<span class="codeline" id="line-1967"><code>		throw("markBits overflow")</code></span>
<span class="codeline" id="line-1968"><code>	}</code></span>
<span class="codeline" id="line-1969"><code></code></span>
<span class="codeline" id="line-1970"><code>	// Add the fresh arena to the "next" list.</code></span>
<span class="codeline" id="line-1971"><code>	fresh.next = gcBitsArenas.next</code></span>
<span class="codeline" id="line-1972"><code>	atomic.StorepNoWB(unsafe.Pointer(&amp;gcBitsArenas.next), unsafe.Pointer(fresh))</code></span>
<span class="codeline" id="line-1973"><code></code></span>
<span class="codeline" id="line-1974"><code>	unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-1975"><code>	return p</code></span>
<span class="codeline" id="line-1976"><code>}</code></span>
<span class="codeline" id="line-1977"><code></code></span>
<span class="codeline" id="line-1978"><code>// newAllocBits returns a pointer to 8 byte aligned bytes</code></span>
<span class="codeline" id="line-1979"><code>// to be used for this span's alloc bits.</code></span>
<span class="codeline" id="line-1980"><code>// newAllocBits is used to provide newly initialized spans</code></span>
<span class="codeline" id="line-1981"><code>// allocation bits. For spans not being initialized the</code></span>
<span class="codeline" id="line-1982"><code>// mark bits are repurposed as allocation bits when</code></span>
<span class="codeline" id="line-1983"><code>// the span is swept.</code></span>
<span class="codeline" id="line-1984"><code>func newAllocBits(nelems uintptr) *gcBits {</code></span>
<span class="codeline" id="line-1985"><code>	return newMarkBits(nelems)</code></span>
<span class="codeline" id="line-1986"><code>}</code></span>
<span class="codeline" id="line-1987"><code></code></span>
<span class="codeline" id="line-1988"><code>// nextMarkBitArenaEpoch establishes a new epoch for the arenas</code></span>
<span class="codeline" id="line-1989"><code>// holding the mark bits. The arenas are named relative to the</code></span>
<span class="codeline" id="line-1990"><code>// current GC cycle which is demarcated by the call to finishweep_m.</code></span>
<span class="codeline" id="line-1991"><code>//</code></span>
<span class="codeline" id="line-1992"><code>// All current spans have been swept.</code></span>
<span class="codeline" id="line-1993"><code>// During that sweep each span allocated room for its gcmarkBits in</code></span>
<span class="codeline" id="line-1994"><code>// gcBitsArenas.next block. gcBitsArenas.next becomes the gcBitsArenas.current</code></span>
<span class="codeline" id="line-1995"><code>// where the GC will mark objects and after each span is swept these bits</code></span>
<span class="codeline" id="line-1996"><code>// will be used to allocate objects.</code></span>
<span class="codeline" id="line-1997"><code>// gcBitsArenas.current becomes gcBitsArenas.previous where the span's</code></span>
<span class="codeline" id="line-1998"><code>// gcAllocBits live until all the spans have been swept during this GC cycle.</code></span>
<span class="codeline" id="line-1999"><code>// The span's sweep extinguishes all the references to gcBitsArenas.previous</code></span>
<span class="codeline" id="line-2000"><code>// by pointing gcAllocBits into the gcBitsArenas.current.</code></span>
<span class="codeline" id="line-2001"><code>// The gcBitsArenas.previous is released to the gcBitsArenas.free list.</code></span>
<span class="codeline" id="line-2002"><code>func nextMarkBitArenaEpoch() {</code></span>
<span class="codeline" id="line-2003"><code>	lock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2004"><code>	if gcBitsArenas.previous != nil {</code></span>
<span class="codeline" id="line-2005"><code>		if gcBitsArenas.free == nil {</code></span>
<span class="codeline" id="line-2006"><code>			gcBitsArenas.free = gcBitsArenas.previous</code></span>
<span class="codeline" id="line-2007"><code>		} else {</code></span>
<span class="codeline" id="line-2008"><code>			// Find end of previous arenas.</code></span>
<span class="codeline" id="line-2009"><code>			last := gcBitsArenas.previous</code></span>
<span class="codeline" id="line-2010"><code>			for last = gcBitsArenas.previous; last.next != nil; last = last.next {</code></span>
<span class="codeline" id="line-2011"><code>			}</code></span>
<span class="codeline" id="line-2012"><code>			last.next = gcBitsArenas.free</code></span>
<span class="codeline" id="line-2013"><code>			gcBitsArenas.free = gcBitsArenas.previous</code></span>
<span class="codeline" id="line-2014"><code>		}</code></span>
<span class="codeline" id="line-2015"><code>	}</code></span>
<span class="codeline" id="line-2016"><code>	gcBitsArenas.previous = gcBitsArenas.current</code></span>
<span class="codeline" id="line-2017"><code>	gcBitsArenas.current = gcBitsArenas.next</code></span>
<span class="codeline" id="line-2018"><code>	atomic.StorepNoWB(unsafe.Pointer(&amp;gcBitsArenas.next), nil) // newMarkBits calls newArena when needed</code></span>
<span class="codeline" id="line-2019"><code>	unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2020"><code>}</code></span>
<span class="codeline" id="line-2021"><code></code></span>
<span class="codeline" id="line-2022"><code>// newArenaMayUnlock allocates and zeroes a gcBits arena.</code></span>
<span class="codeline" id="line-2023"><code>// The caller must hold gcBitsArena.lock. This may temporarily release it.</code></span>
<span class="codeline" id="line-2024"><code>func newArenaMayUnlock() *gcBitsArena {</code></span>
<span class="codeline" id="line-2025"><code>	var result *gcBitsArena</code></span>
<span class="codeline" id="line-2026"><code>	if gcBitsArenas.free == nil {</code></span>
<span class="codeline" id="line-2027"><code>		unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2028"><code>		result = (*gcBitsArena)(sysAlloc(gcBitsChunkBytes, &amp;memstats.gcMiscSys))</code></span>
<span class="codeline" id="line-2029"><code>		if result == nil {</code></span>
<span class="codeline" id="line-2030"><code>			throw("runtime: cannot allocate memory")</code></span>
<span class="codeline" id="line-2031"><code>		}</code></span>
<span class="codeline" id="line-2032"><code>		lock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2033"><code>	} else {</code></span>
<span class="codeline" id="line-2034"><code>		result = gcBitsArenas.free</code></span>
<span class="codeline" id="line-2035"><code>		gcBitsArenas.free = gcBitsArenas.free.next</code></span>
<span class="codeline" id="line-2036"><code>		memclrNoHeapPointers(unsafe.Pointer(result), gcBitsChunkBytes)</code></span>
<span class="codeline" id="line-2037"><code>	}</code></span>
<span class="codeline" id="line-2038"><code>	result.next = nil</code></span>
<span class="codeline" id="line-2039"><code>	// If result.bits is not 8 byte aligned adjust index so</code></span>
<span class="codeline" id="line-2040"><code>	// that &amp;result.bits[result.free] is 8 byte aligned.</code></span>
<span class="codeline" id="line-2041"><code>	if uintptr(unsafe.Offsetof(gcBitsArena{}.bits))&amp;7 == 0 {</code></span>
<span class="codeline" id="line-2042"><code>		result.free = 0</code></span>
<span class="codeline" id="line-2043"><code>	} else {</code></span>
<span class="codeline" id="line-2044"><code>		result.free = 8 - (uintptr(unsafe.Pointer(&amp;result.bits[0])) &amp; 7)</code></span>
<span class="codeline" id="line-2045"><code>	}</code></span>
<span class="codeline" id="line-2046"><code>	return result</code></span>
<span class="codeline" id="line-2047"><code>}</code></span>
</pre><pre id="footer">
<table><tr><td><img src="../../png/go101-twitter.png"></td>
<td>The pages are generated with <a href="https://go101.org/article/tool-golds.html"><b>Golds</b></a> <i>v0.3.2</i>. (GOOS=linux GOARCH=amd64)
<b>Golds</b> is a <a href="https://go101.org">Go 101</a> project developed by <a href="https://tapirgames.com">Tapir Liu</a>.
PR and bug reports are welcome and can be submitted to <a href="https://github.com/go101/golds">the issue list</a>.
Please follow <a href="https://twitter.com/go100and1">@Go100and1</a> (reachable from the left QR code) to get the latest news of <b>Golds</b>.</td></tr></table></pre>
<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Source: mgcscavenge.go in package runtime</title>
<link href="../../css/light-v0.3.2.css" rel="stylesheet">
<script src="../../jvs/golds-v0.3.2.js"></script>
<body onload="onPageLoad()"><div>

<pre id="header"><code><span class="title">Source File</span>
	mgcscavenge.go

<span class="title">Belonging Package</span>
	<a href="../../pkg/runtime.html">runtime</a>
</code></pre>

<pre class="line-numbers">
<span class="codeline" id="line-1"><code>// Copyright 2019 The Go Authors. All rights reserved.</code></span>
<span class="codeline" id="line-2"><code>// Use of this source code is governed by a BSD-style</code></span>
<span class="codeline" id="line-3"><code>// license that can be found in the LICENSE file.</code></span>
<span class="codeline" id="line-4"><code></code></span>
<span class="codeline" id="line-5"><code>// Scavenging free pages.</code></span>
<span class="codeline" id="line-6"><code>//</code></span>
<span class="codeline" id="line-7"><code>// This file implements scavenging (the release of physical pages backing mapped</code></span>
<span class="codeline" id="line-8"><code>// memory) of free and unused pages in the heap as a way to deal with page-level</code></span>
<span class="codeline" id="line-9"><code>// fragmentation and reduce the RSS of Go applications.</code></span>
<span class="codeline" id="line-10"><code>//</code></span>
<span class="codeline" id="line-11"><code>// Scavenging in Go happens on two fronts: there's the background</code></span>
<span class="codeline" id="line-12"><code>// (asynchronous) scavenger and the heap-growth (synchronous) scavenger.</code></span>
<span class="codeline" id="line-13"><code>//</code></span>
<span class="codeline" id="line-14"><code>// The former happens on a goroutine much like the background sweeper which is</code></span>
<span class="codeline" id="line-15"><code>// soft-capped at using scavengePercent of the mutator's time, based on</code></span>
<span class="codeline" id="line-16"><code>// order-of-magnitude estimates of the costs of scavenging. The background</code></span>
<span class="codeline" id="line-17"><code>// scavenger's primary goal is to bring the estimated heap RSS of the</code></span>
<span class="codeline" id="line-18"><code>// application down to a goal.</code></span>
<span class="codeline" id="line-19"><code>//</code></span>
<span class="codeline" id="line-20"><code>// That goal is defined as:</code></span>
<span class="codeline" id="line-21"><code>//   (retainExtraPercent+100) / 100 * (next_gc / last_next_gc) * last_heap_inuse</code></span>
<span class="codeline" id="line-22"><code>//</code></span>
<span class="codeline" id="line-23"><code>// Essentially, we wish to have the application's RSS track the heap goal, but</code></span>
<span class="codeline" id="line-24"><code>// the heap goal is defined in terms of bytes of objects, rather than pages like</code></span>
<span class="codeline" id="line-25"><code>// RSS. As a result, we need to take into account for fragmentation internal to</code></span>
<span class="codeline" id="line-26"><code>// spans. next_gc / last_next_gc defines the ratio between the current heap goal</code></span>
<span class="codeline" id="line-27"><code>// and the last heap goal, which tells us by how much the heap is growing and</code></span>
<span class="codeline" id="line-28"><code>// shrinking. We estimate what the heap will grow to in terms of pages by taking</code></span>
<span class="codeline" id="line-29"><code>// this ratio and multiplying it by heap_inuse at the end of the last GC, which</code></span>
<span class="codeline" id="line-30"><code>// allows us to account for this additional fragmentation. Note that this</code></span>
<span class="codeline" id="line-31"><code>// procedure makes the assumption that the degree of fragmentation won't change</code></span>
<span class="codeline" id="line-32"><code>// dramatically over the next GC cycle. Overestimating the amount of</code></span>
<span class="codeline" id="line-33"><code>// fragmentation simply results in higher memory use, which will be accounted</code></span>
<span class="codeline" id="line-34"><code>// for by the next pacing up date. Underestimating the fragmentation however</code></span>
<span class="codeline" id="line-35"><code>// could lead to performance degradation. Handling this case is not within the</code></span>
<span class="codeline" id="line-36"><code>// scope of the scavenger. Situations where the amount of fragmentation balloons</code></span>
<span class="codeline" id="line-37"><code>// over the course of a single GC cycle should be considered pathologies,</code></span>
<span class="codeline" id="line-38"><code>// flagged as bugs, and fixed appropriately.</code></span>
<span class="codeline" id="line-39"><code>//</code></span>
<span class="codeline" id="line-40"><code>// An additional factor of retainExtraPercent is added as a buffer to help ensure</code></span>
<span class="codeline" id="line-41"><code>// that there's more unscavenged memory to allocate out of, since each allocation</code></span>
<span class="codeline" id="line-42"><code>// out of scavenged memory incurs a potentially expensive page fault.</code></span>
<span class="codeline" id="line-43"><code>//</code></span>
<span class="codeline" id="line-44"><code>// The goal is updated after each GC and the scavenger's pacing parameters</code></span>
<span class="codeline" id="line-45"><code>// (which live in mheap_) are updated to match. The pacing parameters work much</code></span>
<span class="codeline" id="line-46"><code>// like the background sweeping parameters. The parameters define a line whose</code></span>
<span class="codeline" id="line-47"><code>// horizontal axis is time and vertical axis is estimated heap RSS, and the</code></span>
<span class="codeline" id="line-48"><code>// scavenger attempts to stay below that line at all times.</code></span>
<span class="codeline" id="line-49"><code>//</code></span>
<span class="codeline" id="line-50"><code>// The synchronous heap-growth scavenging happens whenever the heap grows in</code></span>
<span class="codeline" id="line-51"><code>// size, for some definition of heap-growth. The intuition behind this is that</code></span>
<span class="codeline" id="line-52"><code>// the application had to grow the heap because existing fragments were</code></span>
<span class="codeline" id="line-53"><code>// not sufficiently large to satisfy a page-level memory allocation, so we</code></span>
<span class="codeline" id="line-54"><code>// scavenge those fragments eagerly to offset the growth in RSS that results.</code></span>
<span class="codeline" id="line-55"><code></code></span>
<span class="codeline" id="line-56"><code>package runtime</code></span>
<span class="codeline" id="line-57"><code></code></span>
<span class="codeline" id="line-58"><code>import (</code></span>
<span class="codeline" id="line-59"><code>	"runtime/internal/atomic"</code></span>
<span class="codeline" id="line-60"><code>	"runtime/internal/sys"</code></span>
<span class="codeline" id="line-61"><code>	"unsafe"</code></span>
<span class="codeline" id="line-62"><code>)</code></span>
<span class="codeline" id="line-63"><code></code></span>
<span class="codeline" id="line-64"><code>const (</code></span>
<span class="codeline" id="line-65"><code>	// The background scavenger is paced according to these parameters.</code></span>
<span class="codeline" id="line-66"><code>	//</code></span>
<span class="codeline" id="line-67"><code>	// scavengePercent represents the portion of mutator time we're willing</code></span>
<span class="codeline" id="line-68"><code>	// to spend on scavenging in percent.</code></span>
<span class="codeline" id="line-69"><code>	scavengePercent = 1 // 1%</code></span>
<span class="codeline" id="line-70"><code></code></span>
<span class="codeline" id="line-71"><code>	// retainExtraPercent represents the amount of memory over the heap goal</code></span>
<span class="codeline" id="line-72"><code>	// that the scavenger should keep as a buffer space for the allocator.</code></span>
<span class="codeline" id="line-73"><code>	//</code></span>
<span class="codeline" id="line-74"><code>	// The purpose of maintaining this overhead is to have a greater pool of</code></span>
<span class="codeline" id="line-75"><code>	// unscavenged memory available for allocation (since using scavenged memory</code></span>
<span class="codeline" id="line-76"><code>	// incurs an additional cost), to account for heap fragmentation and</code></span>
<span class="codeline" id="line-77"><code>	// the ever-changing layout of the heap.</code></span>
<span class="codeline" id="line-78"><code>	retainExtraPercent = 10</code></span>
<span class="codeline" id="line-79"><code></code></span>
<span class="codeline" id="line-80"><code>	// maxPagesPerPhysPage is the maximum number of supported runtime pages per</code></span>
<span class="codeline" id="line-81"><code>	// physical page, based on maxPhysPageSize.</code></span>
<span class="codeline" id="line-82"><code>	maxPagesPerPhysPage = maxPhysPageSize / pageSize</code></span>
<span class="codeline" id="line-83"><code></code></span>
<span class="codeline" id="line-84"><code>	// scavengeCostRatio is the approximate ratio between the costs of using previously</code></span>
<span class="codeline" id="line-85"><code>	// scavenged memory and scavenging memory.</code></span>
<span class="codeline" id="line-86"><code>	//</code></span>
<span class="codeline" id="line-87"><code>	// For most systems the cost of scavenging greatly outweighs the costs</code></span>
<span class="codeline" id="line-88"><code>	// associated with using scavenged memory, making this constant 0. On other systems</code></span>
<span class="codeline" id="line-89"><code>	// (especially ones where "sysUsed" is not just a no-op) this cost is non-trivial.</code></span>
<span class="codeline" id="line-90"><code>	//</code></span>
<span class="codeline" id="line-91"><code>	// This ratio is used as part of multiplicative factor to help the scavenger account</code></span>
<span class="codeline" id="line-92"><code>	// for the additional costs of using scavenged memory in its pacing.</code></span>
<span class="codeline" id="line-93"><code>	scavengeCostRatio = 0.7 * (sys.GoosDarwin + sys.GoosIos)</code></span>
<span class="codeline" id="line-94"><code></code></span>
<span class="codeline" id="line-95"><code>	// scavengeReservationShards determines the amount of memory the scavenger</code></span>
<span class="codeline" id="line-96"><code>	// should reserve for scavenging at a time. Specifically, the amount of</code></span>
<span class="codeline" id="line-97"><code>	// memory reserved is (heap size in bytes) / scavengeReservationShards.</code></span>
<span class="codeline" id="line-98"><code>	scavengeReservationShards = 64</code></span>
<span class="codeline" id="line-99"><code>)</code></span>
<span class="codeline" id="line-100"><code></code></span>
<span class="codeline" id="line-101"><code>// heapRetained returns an estimate of the current heap RSS.</code></span>
<span class="codeline" id="line-102"><code>func heapRetained() uint64 {</code></span>
<span class="codeline" id="line-103"><code>	return memstats.heap_sys.load() - atomic.Load64(&amp;memstats.heap_released)</code></span>
<span class="codeline" id="line-104"><code>}</code></span>
<span class="codeline" id="line-105"><code></code></span>
<span class="codeline" id="line-106"><code>// gcPaceScavenger updates the scavenger's pacing, particularly</code></span>
<span class="codeline" id="line-107"><code>// its rate and RSS goal.</code></span>
<span class="codeline" id="line-108"><code>//</code></span>
<span class="codeline" id="line-109"><code>// The RSS goal is based on the current heap goal with a small overhead</code></span>
<span class="codeline" id="line-110"><code>// to accommodate non-determinism in the allocator.</code></span>
<span class="codeline" id="line-111"><code>//</code></span>
<span class="codeline" id="line-112"><code>// The pacing is based on scavengePageRate, which applies to both regular and</code></span>
<span class="codeline" id="line-113"><code>// huge pages. See that constant for more information.</code></span>
<span class="codeline" id="line-114"><code>//</code></span>
<span class="codeline" id="line-115"><code>// mheap_.lock must be held or the world must be stopped.</code></span>
<span class="codeline" id="line-116"><code>func gcPaceScavenger() {</code></span>
<span class="codeline" id="line-117"><code>	// If we're called before the first GC completed, disable scavenging.</code></span>
<span class="codeline" id="line-118"><code>	// We never scavenge before the 2nd GC cycle anyway (we don't have enough</code></span>
<span class="codeline" id="line-119"><code>	// information about the heap yet) so this is fine, and avoids a fault</code></span>
<span class="codeline" id="line-120"><code>	// or garbage data later.</code></span>
<span class="codeline" id="line-121"><code>	if memstats.last_next_gc == 0 {</code></span>
<span class="codeline" id="line-122"><code>		mheap_.scavengeGoal = ^uint64(0)</code></span>
<span class="codeline" id="line-123"><code>		return</code></span>
<span class="codeline" id="line-124"><code>	}</code></span>
<span class="codeline" id="line-125"><code>	// Compute our scavenging goal.</code></span>
<span class="codeline" id="line-126"><code>	goalRatio := float64(atomic.Load64(&amp;memstats.next_gc)) / float64(memstats.last_next_gc)</code></span>
<span class="codeline" id="line-127"><code>	retainedGoal := uint64(float64(memstats.last_heap_inuse) * goalRatio)</code></span>
<span class="codeline" id="line-128"><code>	// Add retainExtraPercent overhead to retainedGoal. This calculation</code></span>
<span class="codeline" id="line-129"><code>	// looks strange but the purpose is to arrive at an integer division</code></span>
<span class="codeline" id="line-130"><code>	// (e.g. if retainExtraPercent = 12.5, then we get a divisor of 8)</code></span>
<span class="codeline" id="line-131"><code>	// that also avoids the overflow from a multiplication.</code></span>
<span class="codeline" id="line-132"><code>	retainedGoal += retainedGoal / (1.0 / (retainExtraPercent / 100.0))</code></span>
<span class="codeline" id="line-133"><code>	// Align it to a physical page boundary to make the following calculations</code></span>
<span class="codeline" id="line-134"><code>	// a bit more exact.</code></span>
<span class="codeline" id="line-135"><code>	retainedGoal = (retainedGoal + uint64(physPageSize) - 1) &amp;^ (uint64(physPageSize) - 1)</code></span>
<span class="codeline" id="line-136"><code></code></span>
<span class="codeline" id="line-137"><code>	// Represents where we are now in the heap's contribution to RSS in bytes.</code></span>
<span class="codeline" id="line-138"><code>	//</code></span>
<span class="codeline" id="line-139"><code>	// Guaranteed to always be a multiple of physPageSize on systems where</code></span>
<span class="codeline" id="line-140"><code>	// physPageSize &lt;= pageSize since we map heap_sys at a rate larger than</code></span>
<span class="codeline" id="line-141"><code>	// any physPageSize and released memory in multiples of the physPageSize.</code></span>
<span class="codeline" id="line-142"><code>	//</code></span>
<span class="codeline" id="line-143"><code>	// However, certain functions recategorize heap_sys as other stats (e.g.</code></span>
<span class="codeline" id="line-144"><code>	// stack_sys) and this happens in multiples of pageSize, so on systems</code></span>
<span class="codeline" id="line-145"><code>	// where physPageSize &gt; pageSize the calculations below will not be exact.</code></span>
<span class="codeline" id="line-146"><code>	// Generally this is OK since we'll be off by at most one regular</code></span>
<span class="codeline" id="line-147"><code>	// physical page.</code></span>
<span class="codeline" id="line-148"><code>	retainedNow := heapRetained()</code></span>
<span class="codeline" id="line-149"><code></code></span>
<span class="codeline" id="line-150"><code>	// If we're already below our goal, or within one page of our goal, then disable</code></span>
<span class="codeline" id="line-151"><code>	// the background scavenger. We disable the background scavenger if there's</code></span>
<span class="codeline" id="line-152"><code>	// less than one physical page of work to do because it's not worth it.</code></span>
<span class="codeline" id="line-153"><code>	if retainedNow &lt;= retainedGoal || retainedNow-retainedGoal &lt; uint64(physPageSize) {</code></span>
<span class="codeline" id="line-154"><code>		mheap_.scavengeGoal = ^uint64(0)</code></span>
<span class="codeline" id="line-155"><code>		return</code></span>
<span class="codeline" id="line-156"><code>	}</code></span>
<span class="codeline" id="line-157"><code>	mheap_.scavengeGoal = retainedGoal</code></span>
<span class="codeline" id="line-158"><code>}</code></span>
<span class="codeline" id="line-159"><code></code></span>
<span class="codeline" id="line-160"><code>// Sleep/wait state of the background scavenger.</code></span>
<span class="codeline" id="line-161"><code>var scavenge struct {</code></span>
<span class="codeline" id="line-162"><code>	lock       mutex</code></span>
<span class="codeline" id="line-163"><code>	g          *g</code></span>
<span class="codeline" id="line-164"><code>	parked     bool</code></span>
<span class="codeline" id="line-165"><code>	timer      *timer</code></span>
<span class="codeline" id="line-166"><code>	sysmonWake uint32 // Set atomically.</code></span>
<span class="codeline" id="line-167"><code>}</code></span>
<span class="codeline" id="line-168"><code></code></span>
<span class="codeline" id="line-169"><code>// readyForScavenger signals sysmon to wake the scavenger because</code></span>
<span class="codeline" id="line-170"><code>// there may be new work to do.</code></span>
<span class="codeline" id="line-171"><code>//</code></span>
<span class="codeline" id="line-172"><code>// There may be a significant delay between when this function runs</code></span>
<span class="codeline" id="line-173"><code>// and when the scavenger is kicked awake, but it may be safely invoked</code></span>
<span class="codeline" id="line-174"><code>// in contexts where wakeScavenger is unsafe to call directly.</code></span>
<span class="codeline" id="line-175"><code>func readyForScavenger() {</code></span>
<span class="codeline" id="line-176"><code>	atomic.Store(&amp;scavenge.sysmonWake, 1)</code></span>
<span class="codeline" id="line-177"><code>}</code></span>
<span class="codeline" id="line-178"><code></code></span>
<span class="codeline" id="line-179"><code>// wakeScavenger immediately unparks the scavenger if necessary.</code></span>
<span class="codeline" id="line-180"><code>//</code></span>
<span class="codeline" id="line-181"><code>// May run without a P, but it may allocate, so it must not be called</code></span>
<span class="codeline" id="line-182"><code>// on any allocation path.</code></span>
<span class="codeline" id="line-183"><code>//</code></span>
<span class="codeline" id="line-184"><code>// mheap_.lock, scavenge.lock, and sched.lock must not be held.</code></span>
<span class="codeline" id="line-185"><code>func wakeScavenger() {</code></span>
<span class="codeline" id="line-186"><code>	lock(&amp;scavenge.lock)</code></span>
<span class="codeline" id="line-187"><code>	if scavenge.parked {</code></span>
<span class="codeline" id="line-188"><code>		// Notify sysmon that it shouldn't bother waking up the scavenger.</code></span>
<span class="codeline" id="line-189"><code>		atomic.Store(&amp;scavenge.sysmonWake, 0)</code></span>
<span class="codeline" id="line-190"><code></code></span>
<span class="codeline" id="line-191"><code>		// Try to stop the timer but we don't really care if we succeed.</code></span>
<span class="codeline" id="line-192"><code>		// It's possible that either a timer was never started, or that</code></span>
<span class="codeline" id="line-193"><code>		// we're racing with it.</code></span>
<span class="codeline" id="line-194"><code>		// In the case that we're racing with there's the low chance that</code></span>
<span class="codeline" id="line-195"><code>		// we experience a spurious wake-up of the scavenger, but that's</code></span>
<span class="codeline" id="line-196"><code>		// totally safe.</code></span>
<span class="codeline" id="line-197"><code>		stopTimer(scavenge.timer)</code></span>
<span class="codeline" id="line-198"><code></code></span>
<span class="codeline" id="line-199"><code>		// Unpark the goroutine and tell it that there may have been a pacing</code></span>
<span class="codeline" id="line-200"><code>		// change. Note that we skip the scheduler's runnext slot because we</code></span>
<span class="codeline" id="line-201"><code>		// want to avoid having the scavenger interfere with the fair</code></span>
<span class="codeline" id="line-202"><code>		// scheduling of user goroutines. In effect, this schedules the</code></span>
<span class="codeline" id="line-203"><code>		// scavenger at a "lower priority" but that's OK because it'll</code></span>
<span class="codeline" id="line-204"><code>		// catch up on the work it missed when it does get scheduled.</code></span>
<span class="codeline" id="line-205"><code>		scavenge.parked = false</code></span>
<span class="codeline" id="line-206"><code></code></span>
<span class="codeline" id="line-207"><code>		// Ready the goroutine by injecting it. We use injectglist instead</code></span>
<span class="codeline" id="line-208"><code>		// of ready or goready in order to allow us to run this function</code></span>
<span class="codeline" id="line-209"><code>		// without a P. injectglist also avoids placing the goroutine in</code></span>
<span class="codeline" id="line-210"><code>		// the current P's runnext slot, which is desireable to prevent</code></span>
<span class="codeline" id="line-211"><code>		// the scavenger from interfering with user goroutine scheduling</code></span>
<span class="codeline" id="line-212"><code>		// too much.</code></span>
<span class="codeline" id="line-213"><code>		var list gList</code></span>
<span class="codeline" id="line-214"><code>		list.push(scavenge.g)</code></span>
<span class="codeline" id="line-215"><code>		injectglist(&amp;list)</code></span>
<span class="codeline" id="line-216"><code>	}</code></span>
<span class="codeline" id="line-217"><code>	unlock(&amp;scavenge.lock)</code></span>
<span class="codeline" id="line-218"><code>}</code></span>
<span class="codeline" id="line-219"><code></code></span>
<span class="codeline" id="line-220"><code>// scavengeSleep attempts to put the scavenger to sleep for ns.</code></span>
<span class="codeline" id="line-221"><code>//</code></span>
<span class="codeline" id="line-222"><code>// Note that this function should only be called by the scavenger.</code></span>
<span class="codeline" id="line-223"><code>//</code></span>
<span class="codeline" id="line-224"><code>// The scavenger may be woken up earlier by a pacing change, and it may not go</code></span>
<span class="codeline" id="line-225"><code>// to sleep at all if there's a pending pacing change.</code></span>
<span class="codeline" id="line-226"><code>//</code></span>
<span class="codeline" id="line-227"><code>// Returns the amount of time actually slept.</code></span>
<span class="codeline" id="line-228"><code>func scavengeSleep(ns int64) int64 {</code></span>
<span class="codeline" id="line-229"><code>	lock(&amp;scavenge.lock)</code></span>
<span class="codeline" id="line-230"><code></code></span>
<span class="codeline" id="line-231"><code>	// Set the timer.</code></span>
<span class="codeline" id="line-232"><code>	//</code></span>
<span class="codeline" id="line-233"><code>	// This must happen here instead of inside gopark</code></span>
<span class="codeline" id="line-234"><code>	// because we can't close over any variables without</code></span>
<span class="codeline" id="line-235"><code>	// failing escape analysis.</code></span>
<span class="codeline" id="line-236"><code>	start := nanotime()</code></span>
<span class="codeline" id="line-237"><code>	resetTimer(scavenge.timer, start+ns)</code></span>
<span class="codeline" id="line-238"><code></code></span>
<span class="codeline" id="line-239"><code>	// Mark ourself as asleep and go to sleep.</code></span>
<span class="codeline" id="line-240"><code>	scavenge.parked = true</code></span>
<span class="codeline" id="line-241"><code>	goparkunlock(&amp;scavenge.lock, waitReasonSleep, traceEvGoSleep, 2)</code></span>
<span class="codeline" id="line-242"><code></code></span>
<span class="codeline" id="line-243"><code>	// Return how long we actually slept for.</code></span>
<span class="codeline" id="line-244"><code>	return nanotime() - start</code></span>
<span class="codeline" id="line-245"><code>}</code></span>
<span class="codeline" id="line-246"><code></code></span>
<span class="codeline" id="line-247"><code>// Background scavenger.</code></span>
<span class="codeline" id="line-248"><code>//</code></span>
<span class="codeline" id="line-249"><code>// The background scavenger maintains the RSS of the application below</code></span>
<span class="codeline" id="line-250"><code>// the line described by the proportional scavenging statistics in</code></span>
<span class="codeline" id="line-251"><code>// the mheap struct.</code></span>
<span class="codeline" id="line-252"><code>func bgscavenge(c chan int) {</code></span>
<span class="codeline" id="line-253"><code>	scavenge.g = getg()</code></span>
<span class="codeline" id="line-254"><code></code></span>
<span class="codeline" id="line-255"><code>	lockInit(&amp;scavenge.lock, lockRankScavenge)</code></span>
<span class="codeline" id="line-256"><code>	lock(&amp;scavenge.lock)</code></span>
<span class="codeline" id="line-257"><code>	scavenge.parked = true</code></span>
<span class="codeline" id="line-258"><code></code></span>
<span class="codeline" id="line-259"><code>	scavenge.timer = new(timer)</code></span>
<span class="codeline" id="line-260"><code>	scavenge.timer.f = func(_ interface{}, _ uintptr) {</code></span>
<span class="codeline" id="line-261"><code>		wakeScavenger()</code></span>
<span class="codeline" id="line-262"><code>	}</code></span>
<span class="codeline" id="line-263"><code></code></span>
<span class="codeline" id="line-264"><code>	c &lt;- 1</code></span>
<span class="codeline" id="line-265"><code>	goparkunlock(&amp;scavenge.lock, waitReasonGCScavengeWait, traceEvGoBlock, 1)</code></span>
<span class="codeline" id="line-266"><code></code></span>
<span class="codeline" id="line-267"><code>	// Exponentially-weighted moving average of the fraction of time this</code></span>
<span class="codeline" id="line-268"><code>	// goroutine spends scavenging (that is, percent of a single CPU).</code></span>
<span class="codeline" id="line-269"><code>	// It represents a measure of scheduling overheads which might extend</code></span>
<span class="codeline" id="line-270"><code>	// the sleep or the critical time beyond what's expected. Assume no</code></span>
<span class="codeline" id="line-271"><code>	// overhead to begin with.</code></span>
<span class="codeline" id="line-272"><code>	//</code></span>
<span class="codeline" id="line-273"><code>	// TODO(mknyszek): Consider making this based on total CPU time of the</code></span>
<span class="codeline" id="line-274"><code>	// application (i.e. scavengePercent * GOMAXPROCS). This isn't really</code></span>
<span class="codeline" id="line-275"><code>	// feasible now because the scavenger acquires the heap lock over the</code></span>
<span class="codeline" id="line-276"><code>	// scavenging operation, which means scavenging effectively blocks</code></span>
<span class="codeline" id="line-277"><code>	// allocators and isn't scalable. However, given a scalable allocator,</code></span>
<span class="codeline" id="line-278"><code>	// it makes sense to also make the scavenger scale with it; if you're</code></span>
<span class="codeline" id="line-279"><code>	// allocating more frequently, then presumably you're also generating</code></span>
<span class="codeline" id="line-280"><code>	// more work for the scavenger.</code></span>
<span class="codeline" id="line-281"><code>	const idealFraction = scavengePercent / 100.0</code></span>
<span class="codeline" id="line-282"><code>	scavengeEWMA := float64(idealFraction)</code></span>
<span class="codeline" id="line-283"><code></code></span>
<span class="codeline" id="line-284"><code>	for {</code></span>
<span class="codeline" id="line-285"><code>		released := uintptr(0)</code></span>
<span class="codeline" id="line-286"><code></code></span>
<span class="codeline" id="line-287"><code>		// Time in scavenging critical section.</code></span>
<span class="codeline" id="line-288"><code>		crit := float64(0)</code></span>
<span class="codeline" id="line-289"><code></code></span>
<span class="codeline" id="line-290"><code>		// Run on the system stack since we grab the heap lock,</code></span>
<span class="codeline" id="line-291"><code>		// and a stack growth with the heap lock means a deadlock.</code></span>
<span class="codeline" id="line-292"><code>		systemstack(func() {</code></span>
<span class="codeline" id="line-293"><code>			lock(&amp;mheap_.lock)</code></span>
<span class="codeline" id="line-294"><code></code></span>
<span class="codeline" id="line-295"><code>			// If background scavenging is disabled or if there's no work to do just park.</code></span>
<span class="codeline" id="line-296"><code>			retained, goal := heapRetained(), mheap_.scavengeGoal</code></span>
<span class="codeline" id="line-297"><code>			if retained &lt;= goal {</code></span>
<span class="codeline" id="line-298"><code>				unlock(&amp;mheap_.lock)</code></span>
<span class="codeline" id="line-299"><code>				return</code></span>
<span class="codeline" id="line-300"><code>			}</code></span>
<span class="codeline" id="line-301"><code></code></span>
<span class="codeline" id="line-302"><code>			// Scavenge one page, and measure the amount of time spent scavenging.</code></span>
<span class="codeline" id="line-303"><code>			start := nanotime()</code></span>
<span class="codeline" id="line-304"><code>			released = mheap_.pages.scavenge(physPageSize, true)</code></span>
<span class="codeline" id="line-305"><code>			mheap_.pages.scav.released += released</code></span>
<span class="codeline" id="line-306"><code>			crit = float64(nanotime() - start)</code></span>
<span class="codeline" id="line-307"><code></code></span>
<span class="codeline" id="line-308"><code>			unlock(&amp;mheap_.lock)</code></span>
<span class="codeline" id="line-309"><code>		})</code></span>
<span class="codeline" id="line-310"><code></code></span>
<span class="codeline" id="line-311"><code>		if released == 0 {</code></span>
<span class="codeline" id="line-312"><code>			lock(&amp;scavenge.lock)</code></span>
<span class="codeline" id="line-313"><code>			scavenge.parked = true</code></span>
<span class="codeline" id="line-314"><code>			goparkunlock(&amp;scavenge.lock, waitReasonGCScavengeWait, traceEvGoBlock, 1)</code></span>
<span class="codeline" id="line-315"><code>			continue</code></span>
<span class="codeline" id="line-316"><code>		}</code></span>
<span class="codeline" id="line-317"><code></code></span>
<span class="codeline" id="line-318"><code>		if released &lt; physPageSize {</code></span>
<span class="codeline" id="line-319"><code>			// If this happens, it means that we may have attempted to release part</code></span>
<span class="codeline" id="line-320"><code>			// of a physical page, but the likely effect of that is that it released</code></span>
<span class="codeline" id="line-321"><code>			// the whole physical page, some of which may have still been in-use.</code></span>
<span class="codeline" id="line-322"><code>			// This could lead to memory corruption. Throw.</code></span>
<span class="codeline" id="line-323"><code>			throw("released less than one physical page of memory")</code></span>
<span class="codeline" id="line-324"><code>		}</code></span>
<span class="codeline" id="line-325"><code></code></span>
<span class="codeline" id="line-326"><code>		// On some platforms we may see crit as zero if the time it takes to scavenge</code></span>
<span class="codeline" id="line-327"><code>		// memory is less than the minimum granularity of its clock (e.g. Windows).</code></span>
<span class="codeline" id="line-328"><code>		// In this case, just assume scavenging takes 10 µs per regular physical page</code></span>
<span class="codeline" id="line-329"><code>		// (determined empirically), and conservatively ignore the impact of huge pages</code></span>
<span class="codeline" id="line-330"><code>		// on timing.</code></span>
<span class="codeline" id="line-331"><code>		//</code></span>
<span class="codeline" id="line-332"><code>		// We shouldn't ever see a crit value less than zero unless there's a bug of</code></span>
<span class="codeline" id="line-333"><code>		// some kind, either on our side or in the platform we're running on, but be</code></span>
<span class="codeline" id="line-334"><code>		// defensive in that case as well.</code></span>
<span class="codeline" id="line-335"><code>		const approxCritNSPerPhysicalPage = 10e3</code></span>
<span class="codeline" id="line-336"><code>		if crit &lt;= 0 {</code></span>
<span class="codeline" id="line-337"><code>			crit = approxCritNSPerPhysicalPage * float64(released/physPageSize)</code></span>
<span class="codeline" id="line-338"><code>		}</code></span>
<span class="codeline" id="line-339"><code></code></span>
<span class="codeline" id="line-340"><code>		// Multiply the critical time by 1 + the ratio of the costs of using</code></span>
<span class="codeline" id="line-341"><code>		// scavenged memory vs. scavenging memory. This forces us to pay down</code></span>
<span class="codeline" id="line-342"><code>		// the cost of reusing this memory eagerly by sleeping for a longer period</code></span>
<span class="codeline" id="line-343"><code>		// of time and scavenging less frequently. More concretely, we avoid situations</code></span>
<span class="codeline" id="line-344"><code>		// where we end up scavenging so often that we hurt allocation performance</code></span>
<span class="codeline" id="line-345"><code>		// because of the additional overheads of using scavenged memory.</code></span>
<span class="codeline" id="line-346"><code>		crit *= 1 + scavengeCostRatio</code></span>
<span class="codeline" id="line-347"><code></code></span>
<span class="codeline" id="line-348"><code>		// If we spent more than 10 ms (for example, if the OS scheduled us away, or someone</code></span>
<span class="codeline" id="line-349"><code>		// put their machine to sleep) in the critical section, bound the time we use to</code></span>
<span class="codeline" id="line-350"><code>		// calculate at 10 ms to avoid letting the sleep time get arbitrarily high.</code></span>
<span class="codeline" id="line-351"><code>		const maxCrit = 10e6</code></span>
<span class="codeline" id="line-352"><code>		if crit &gt; maxCrit {</code></span>
<span class="codeline" id="line-353"><code>			crit = maxCrit</code></span>
<span class="codeline" id="line-354"><code>		}</code></span>
<span class="codeline" id="line-355"><code></code></span>
<span class="codeline" id="line-356"><code>		// Compute the amount of time to sleep, assuming we want to use at most</code></span>
<span class="codeline" id="line-357"><code>		// scavengePercent of CPU time. Take into account scheduling overheads</code></span>
<span class="codeline" id="line-358"><code>		// that may extend the length of our sleep by multiplying by how far</code></span>
<span class="codeline" id="line-359"><code>		// off we are from the ideal ratio. For example, if we're sleeping too</code></span>
<span class="codeline" id="line-360"><code>		// much, then scavengeEMWA &lt; idealFraction, so we'll adjust the sleep time</code></span>
<span class="codeline" id="line-361"><code>		// down.</code></span>
<span class="codeline" id="line-362"><code>		adjust := scavengeEWMA / idealFraction</code></span>
<span class="codeline" id="line-363"><code>		sleepTime := int64(adjust * crit / (scavengePercent / 100.0))</code></span>
<span class="codeline" id="line-364"><code></code></span>
<span class="codeline" id="line-365"><code>		// Go to sleep.</code></span>
<span class="codeline" id="line-366"><code>		slept := scavengeSleep(sleepTime)</code></span>
<span class="codeline" id="line-367"><code></code></span>
<span class="codeline" id="line-368"><code>		// Compute the new ratio.</code></span>
<span class="codeline" id="line-369"><code>		fraction := crit / (crit + float64(slept))</code></span>
<span class="codeline" id="line-370"><code></code></span>
<span class="codeline" id="line-371"><code>		// Set a lower bound on the fraction.</code></span>
<span class="codeline" id="line-372"><code>		// Due to OS-related anomalies we may "sleep" for an inordinate amount</code></span>
<span class="codeline" id="line-373"><code>		// of time. Let's avoid letting the ratio get out of hand by bounding</code></span>
<span class="codeline" id="line-374"><code>		// the sleep time we use in our EWMA.</code></span>
<span class="codeline" id="line-375"><code>		const minFraction = 1 / 1000</code></span>
<span class="codeline" id="line-376"><code>		if fraction &lt; minFraction {</code></span>
<span class="codeline" id="line-377"><code>			fraction = minFraction</code></span>
<span class="codeline" id="line-378"><code>		}</code></span>
<span class="codeline" id="line-379"><code></code></span>
<span class="codeline" id="line-380"><code>		// Update scavengeEWMA by merging in the new crit/slept ratio.</code></span>
<span class="codeline" id="line-381"><code>		const alpha = 0.5</code></span>
<span class="codeline" id="line-382"><code>		scavengeEWMA = alpha*fraction + (1-alpha)*scavengeEWMA</code></span>
<span class="codeline" id="line-383"><code>	}</code></span>
<span class="codeline" id="line-384"><code>}</code></span>
<span class="codeline" id="line-385"><code></code></span>
<span class="codeline" id="line-386"><code>// scavenge scavenges nbytes worth of free pages, starting with the</code></span>
<span class="codeline" id="line-387"><code>// highest address first. Successive calls continue from where it left</code></span>
<span class="codeline" id="line-388"><code>// off until the heap is exhausted. Call scavengeStartGen to bring it</code></span>
<span class="codeline" id="line-389"><code>// back to the top of the heap.</code></span>
<span class="codeline" id="line-390"><code>//</code></span>
<span class="codeline" id="line-391"><code>// Returns the amount of memory scavenged in bytes.</code></span>
<span class="codeline" id="line-392"><code>//</code></span>
<span class="codeline" id="line-393"><code>// p.mheapLock must be held, but may be temporarily released if</code></span>
<span class="codeline" id="line-394"><code>// mayUnlock == true.</code></span>
<span class="codeline" id="line-395"><code>//</code></span>
<span class="codeline" id="line-396"><code>// Must run on the system stack because p.mheapLock must be held.</code></span>
<span class="codeline" id="line-397"><code>//</code></span>
<span class="codeline" id="line-398"><code>//go:systemstack</code></span>
<span class="codeline" id="line-399"><code>func (p *pageAlloc) scavenge(nbytes uintptr, mayUnlock bool) uintptr {</code></span>
<span class="codeline" id="line-400"><code>	assertLockHeld(p.mheapLock)</code></span>
<span class="codeline" id="line-401"><code></code></span>
<span class="codeline" id="line-402"><code>	var (</code></span>
<span class="codeline" id="line-403"><code>		addrs addrRange</code></span>
<span class="codeline" id="line-404"><code>		gen   uint32</code></span>
<span class="codeline" id="line-405"><code>	)</code></span>
<span class="codeline" id="line-406"><code>	released := uintptr(0)</code></span>
<span class="codeline" id="line-407"><code>	for released &lt; nbytes {</code></span>
<span class="codeline" id="line-408"><code>		if addrs.size() == 0 {</code></span>
<span class="codeline" id="line-409"><code>			if addrs, gen = p.scavengeReserve(); addrs.size() == 0 {</code></span>
<span class="codeline" id="line-410"><code>				break</code></span>
<span class="codeline" id="line-411"><code>			}</code></span>
<span class="codeline" id="line-412"><code>		}</code></span>
<span class="codeline" id="line-413"><code>		r, a := p.scavengeOne(addrs, nbytes-released, mayUnlock)</code></span>
<span class="codeline" id="line-414"><code>		released += r</code></span>
<span class="codeline" id="line-415"><code>		addrs = a</code></span>
<span class="codeline" id="line-416"><code>	}</code></span>
<span class="codeline" id="line-417"><code>	// Only unreserve the space which hasn't been scavenged or searched</code></span>
<span class="codeline" id="line-418"><code>	// to ensure we always make progress.</code></span>
<span class="codeline" id="line-419"><code>	p.scavengeUnreserve(addrs, gen)</code></span>
<span class="codeline" id="line-420"><code>	return released</code></span>
<span class="codeline" id="line-421"><code>}</code></span>
<span class="codeline" id="line-422"><code></code></span>
<span class="codeline" id="line-423"><code>// printScavTrace prints a scavenge trace line to standard error.</code></span>
<span class="codeline" id="line-424"><code>//</code></span>
<span class="codeline" id="line-425"><code>// released should be the amount of memory released since the last time this</code></span>
<span class="codeline" id="line-426"><code>// was called, and forced indicates whether the scavenge was forced by the</code></span>
<span class="codeline" id="line-427"><code>// application.</code></span>
<span class="codeline" id="line-428"><code>func printScavTrace(gen uint32, released uintptr, forced bool) {</code></span>
<span class="codeline" id="line-429"><code>	printlock()</code></span>
<span class="codeline" id="line-430"><code>	print("scav ", gen, " ",</code></span>
<span class="codeline" id="line-431"><code>		released&gt;&gt;10, " KiB work, ",</code></span>
<span class="codeline" id="line-432"><code>		atomic.Load64(&amp;memstats.heap_released)&gt;&gt;10, " KiB total, ",</code></span>
<span class="codeline" id="line-433"><code>		(atomic.Load64(&amp;memstats.heap_inuse)*100)/heapRetained(), "% util",</code></span>
<span class="codeline" id="line-434"><code>	)</code></span>
<span class="codeline" id="line-435"><code>	if forced {</code></span>
<span class="codeline" id="line-436"><code>		print(" (forced)")</code></span>
<span class="codeline" id="line-437"><code>	}</code></span>
<span class="codeline" id="line-438"><code>	println()</code></span>
<span class="codeline" id="line-439"><code>	printunlock()</code></span>
<span class="codeline" id="line-440"><code>}</code></span>
<span class="codeline" id="line-441"><code></code></span>
<span class="codeline" id="line-442"><code>// scavengeStartGen starts a new scavenge generation, resetting</code></span>
<span class="codeline" id="line-443"><code>// the scavenger's search space to the full in-use address space.</code></span>
<span class="codeline" id="line-444"><code>//</code></span>
<span class="codeline" id="line-445"><code>// p.mheapLock must be held.</code></span>
<span class="codeline" id="line-446"><code>//</code></span>
<span class="codeline" id="line-447"><code>// Must run on the system stack because p.mheapLock must be held.</code></span>
<span class="codeline" id="line-448"><code>//</code></span>
<span class="codeline" id="line-449"><code>//go:systemstack</code></span>
<span class="codeline" id="line-450"><code>func (p *pageAlloc) scavengeStartGen() {</code></span>
<span class="codeline" id="line-451"><code>	assertLockHeld(p.mheapLock)</code></span>
<span class="codeline" id="line-452"><code></code></span>
<span class="codeline" id="line-453"><code>	if debug.scavtrace &gt; 0 {</code></span>
<span class="codeline" id="line-454"><code>		printScavTrace(p.scav.gen, p.scav.released, false)</code></span>
<span class="codeline" id="line-455"><code>	}</code></span>
<span class="codeline" id="line-456"><code>	p.inUse.cloneInto(&amp;p.scav.inUse)</code></span>
<span class="codeline" id="line-457"><code></code></span>
<span class="codeline" id="line-458"><code>	// Pick the new starting address for the scavenger cycle.</code></span>
<span class="codeline" id="line-459"><code>	var startAddr offAddr</code></span>
<span class="codeline" id="line-460"><code>	if p.scav.scavLWM.lessThan(p.scav.freeHWM) {</code></span>
<span class="codeline" id="line-461"><code>		// The "free" high watermark exceeds the "scavenged" low watermark,</code></span>
<span class="codeline" id="line-462"><code>		// so there are free scavengable pages in parts of the address space</code></span>
<span class="codeline" id="line-463"><code>		// that the scavenger already searched, the high watermark being the</code></span>
<span class="codeline" id="line-464"><code>		// highest one. Pick that as our new starting point to ensure we</code></span>
<span class="codeline" id="line-465"><code>		// see those pages.</code></span>
<span class="codeline" id="line-466"><code>		startAddr = p.scav.freeHWM</code></span>
<span class="codeline" id="line-467"><code>	} else {</code></span>
<span class="codeline" id="line-468"><code>		// The "free" high watermark does not exceed the "scavenged" low</code></span>
<span class="codeline" id="line-469"><code>		// watermark. This means the allocator didn't free any memory in</code></span>
<span class="codeline" id="line-470"><code>		// the range we scavenged last cycle, so we might as well continue</code></span>
<span class="codeline" id="line-471"><code>		// scavenging from where we were.</code></span>
<span class="codeline" id="line-472"><code>		startAddr = p.scav.scavLWM</code></span>
<span class="codeline" id="line-473"><code>	}</code></span>
<span class="codeline" id="line-474"><code>	p.scav.inUse.removeGreaterEqual(startAddr.addr())</code></span>
<span class="codeline" id="line-475"><code></code></span>
<span class="codeline" id="line-476"><code>	// reservationBytes may be zero if p.inUse.totalBytes is small, or if</code></span>
<span class="codeline" id="line-477"><code>	// scavengeReservationShards is large. This case is fine as the scavenger</code></span>
<span class="codeline" id="line-478"><code>	// will simply be turned off, but it does mean that scavengeReservationShards,</code></span>
<span class="codeline" id="line-479"><code>	// in concert with pallocChunkBytes, dictates the minimum heap size at which</code></span>
<span class="codeline" id="line-480"><code>	// the scavenger triggers. In practice this minimum is generally less than an</code></span>
<span class="codeline" id="line-481"><code>	// arena in size, so virtually every heap has the scavenger on.</code></span>
<span class="codeline" id="line-482"><code>	p.scav.reservationBytes = alignUp(p.inUse.totalBytes, pallocChunkBytes) / scavengeReservationShards</code></span>
<span class="codeline" id="line-483"><code>	p.scav.gen++</code></span>
<span class="codeline" id="line-484"><code>	p.scav.released = 0</code></span>
<span class="codeline" id="line-485"><code>	p.scav.freeHWM = minOffAddr</code></span>
<span class="codeline" id="line-486"><code>	p.scav.scavLWM = maxOffAddr</code></span>
<span class="codeline" id="line-487"><code>}</code></span>
<span class="codeline" id="line-488"><code></code></span>
<span class="codeline" id="line-489"><code>// scavengeReserve reserves a contiguous range of the address space</code></span>
<span class="codeline" id="line-490"><code>// for scavenging. The maximum amount of space it reserves is proportional</code></span>
<span class="codeline" id="line-491"><code>// to the size of the heap. The ranges are reserved from the high addresses</code></span>
<span class="codeline" id="line-492"><code>// first.</code></span>
<span class="codeline" id="line-493"><code>//</code></span>
<span class="codeline" id="line-494"><code>// Returns the reserved range and the scavenge generation number for it.</code></span>
<span class="codeline" id="line-495"><code>//</code></span>
<span class="codeline" id="line-496"><code>// p.mheapLock must be held.</code></span>
<span class="codeline" id="line-497"><code>//</code></span>
<span class="codeline" id="line-498"><code>// Must run on the system stack because p.mheapLock must be held.</code></span>
<span class="codeline" id="line-499"><code>//</code></span>
<span class="codeline" id="line-500"><code>//go:systemstack</code></span>
<span class="codeline" id="line-501"><code>func (p *pageAlloc) scavengeReserve() (addrRange, uint32) {</code></span>
<span class="codeline" id="line-502"><code>	assertLockHeld(p.mheapLock)</code></span>
<span class="codeline" id="line-503"><code></code></span>
<span class="codeline" id="line-504"><code>	// Start by reserving the minimum.</code></span>
<span class="codeline" id="line-505"><code>	r := p.scav.inUse.removeLast(p.scav.reservationBytes)</code></span>
<span class="codeline" id="line-506"><code></code></span>
<span class="codeline" id="line-507"><code>	// Return early if the size is zero; we don't want to use</code></span>
<span class="codeline" id="line-508"><code>	// the bogus address below.</code></span>
<span class="codeline" id="line-509"><code>	if r.size() == 0 {</code></span>
<span class="codeline" id="line-510"><code>		return r, p.scav.gen</code></span>
<span class="codeline" id="line-511"><code>	}</code></span>
<span class="codeline" id="line-512"><code></code></span>
<span class="codeline" id="line-513"><code>	// The scavenger requires that base be aligned to a</code></span>
<span class="codeline" id="line-514"><code>	// palloc chunk because that's the unit of operation for</code></span>
<span class="codeline" id="line-515"><code>	// the scavenger, so align down, potentially extending</code></span>
<span class="codeline" id="line-516"><code>	// the range.</code></span>
<span class="codeline" id="line-517"><code>	newBase := alignDown(r.base.addr(), pallocChunkBytes)</code></span>
<span class="codeline" id="line-518"><code></code></span>
<span class="codeline" id="line-519"><code>	// Remove from inUse however much extra we just pulled out.</code></span>
<span class="codeline" id="line-520"><code>	p.scav.inUse.removeGreaterEqual(newBase)</code></span>
<span class="codeline" id="line-521"><code>	r.base = offAddr{newBase}</code></span>
<span class="codeline" id="line-522"><code>	return r, p.scav.gen</code></span>
<span class="codeline" id="line-523"><code>}</code></span>
<span class="codeline" id="line-524"><code></code></span>
<span class="codeline" id="line-525"><code>// scavengeUnreserve returns an unscavenged portion of a range that was</code></span>
<span class="codeline" id="line-526"><code>// previously reserved with scavengeReserve.</code></span>
<span class="codeline" id="line-527"><code>//</code></span>
<span class="codeline" id="line-528"><code>// p.mheapLock must be held.</code></span>
<span class="codeline" id="line-529"><code>//</code></span>
<span class="codeline" id="line-530"><code>// Must run on the system stack because p.mheapLock must be held.</code></span>
<span class="codeline" id="line-531"><code>//</code></span>
<span class="codeline" id="line-532"><code>//go:systemstack</code></span>
<span class="codeline" id="line-533"><code>func (p *pageAlloc) scavengeUnreserve(r addrRange, gen uint32) {</code></span>
<span class="codeline" id="line-534"><code>	assertLockHeld(p.mheapLock)</code></span>
<span class="codeline" id="line-535"><code></code></span>
<span class="codeline" id="line-536"><code>	if r.size() == 0 || gen != p.scav.gen {</code></span>
<span class="codeline" id="line-537"><code>		return</code></span>
<span class="codeline" id="line-538"><code>	}</code></span>
<span class="codeline" id="line-539"><code>	if r.base.addr()%pallocChunkBytes != 0 {</code></span>
<span class="codeline" id="line-540"><code>		throw("unreserving unaligned region")</code></span>
<span class="codeline" id="line-541"><code>	}</code></span>
<span class="codeline" id="line-542"><code>	p.scav.inUse.add(r)</code></span>
<span class="codeline" id="line-543"><code>}</code></span>
<span class="codeline" id="line-544"><code></code></span>
<span class="codeline" id="line-545"><code>// scavengeOne walks over address range work until it finds</code></span>
<span class="codeline" id="line-546"><code>// a contiguous run of pages to scavenge. It will try to scavenge</code></span>
<span class="codeline" id="line-547"><code>// at most max bytes at once, but may scavenge more to avoid</code></span>
<span class="codeline" id="line-548"><code>// breaking huge pages. Once it scavenges some memory it returns</code></span>
<span class="codeline" id="line-549"><code>// how much it scavenged in bytes.</code></span>
<span class="codeline" id="line-550"><code>//</code></span>
<span class="codeline" id="line-551"><code>// Returns the number of bytes scavenged and the part of work</code></span>
<span class="codeline" id="line-552"><code>// which was not yet searched.</code></span>
<span class="codeline" id="line-553"><code>//</code></span>
<span class="codeline" id="line-554"><code>// work's base address must be aligned to pallocChunkBytes.</code></span>
<span class="codeline" id="line-555"><code>//</code></span>
<span class="codeline" id="line-556"><code>// p.mheapLock must be held, but may be temporarily released if</code></span>
<span class="codeline" id="line-557"><code>// mayUnlock == true.</code></span>
<span class="codeline" id="line-558"><code>//</code></span>
<span class="codeline" id="line-559"><code>// Must run on the system stack because p.mheapLock must be held.</code></span>
<span class="codeline" id="line-560"><code>//</code></span>
<span class="codeline" id="line-561"><code>//go:systemstack</code></span>
<span class="codeline" id="line-562"><code>func (p *pageAlloc) scavengeOne(work addrRange, max uintptr, mayUnlock bool) (uintptr, addrRange) {</code></span>
<span class="codeline" id="line-563"><code>	assertLockHeld(p.mheapLock)</code></span>
<span class="codeline" id="line-564"><code></code></span>
<span class="codeline" id="line-565"><code>	// Defensively check if we've received an empty address range.</code></span>
<span class="codeline" id="line-566"><code>	// If so, just return.</code></span>
<span class="codeline" id="line-567"><code>	if work.size() == 0 {</code></span>
<span class="codeline" id="line-568"><code>		// Nothing to do.</code></span>
<span class="codeline" id="line-569"><code>		return 0, work</code></span>
<span class="codeline" id="line-570"><code>	}</code></span>
<span class="codeline" id="line-571"><code>	// Check the prerequisites of work.</code></span>
<span class="codeline" id="line-572"><code>	if work.base.addr()%pallocChunkBytes != 0 {</code></span>
<span class="codeline" id="line-573"><code>		throw("scavengeOne called with unaligned work region")</code></span>
<span class="codeline" id="line-574"><code>	}</code></span>
<span class="codeline" id="line-575"><code>	// Calculate the maximum number of pages to scavenge.</code></span>
<span class="codeline" id="line-576"><code>	//</code></span>
<span class="codeline" id="line-577"><code>	// This should be alignUp(max, pageSize) / pageSize but max can and will</code></span>
<span class="codeline" id="line-578"><code>	// be ^uintptr(0), so we need to be very careful not to overflow here.</code></span>
<span class="codeline" id="line-579"><code>	// Rather than use alignUp, calculate the number of pages rounded down</code></span>
<span class="codeline" id="line-580"><code>	// first, then add back one if necessary.</code></span>
<span class="codeline" id="line-581"><code>	maxPages := max / pageSize</code></span>
<span class="codeline" id="line-582"><code>	if max%pageSize != 0 {</code></span>
<span class="codeline" id="line-583"><code>		maxPages++</code></span>
<span class="codeline" id="line-584"><code>	}</code></span>
<span class="codeline" id="line-585"><code></code></span>
<span class="codeline" id="line-586"><code>	// Calculate the minimum number of pages we can scavenge.</code></span>
<span class="codeline" id="line-587"><code>	//</code></span>
<span class="codeline" id="line-588"><code>	// Because we can only scavenge whole physical pages, we must</code></span>
<span class="codeline" id="line-589"><code>	// ensure that we scavenge at least minPages each time, aligned</code></span>
<span class="codeline" id="line-590"><code>	// to minPages*pageSize.</code></span>
<span class="codeline" id="line-591"><code>	minPages := physPageSize / pageSize</code></span>
<span class="codeline" id="line-592"><code>	if minPages &lt; 1 {</code></span>
<span class="codeline" id="line-593"><code>		minPages = 1</code></span>
<span class="codeline" id="line-594"><code>	}</code></span>
<span class="codeline" id="line-595"><code></code></span>
<span class="codeline" id="line-596"><code>	// Helpers for locking and unlocking only if mayUnlock == true.</code></span>
<span class="codeline" id="line-597"><code>	lockHeap := func() {</code></span>
<span class="codeline" id="line-598"><code>		if mayUnlock {</code></span>
<span class="codeline" id="line-599"><code>			lock(p.mheapLock)</code></span>
<span class="codeline" id="line-600"><code>		}</code></span>
<span class="codeline" id="line-601"><code>	}</code></span>
<span class="codeline" id="line-602"><code>	unlockHeap := func() {</code></span>
<span class="codeline" id="line-603"><code>		if mayUnlock {</code></span>
<span class="codeline" id="line-604"><code>			unlock(p.mheapLock)</code></span>
<span class="codeline" id="line-605"><code>		}</code></span>
<span class="codeline" id="line-606"><code>	}</code></span>
<span class="codeline" id="line-607"><code></code></span>
<span class="codeline" id="line-608"><code>	// Fast path: check the chunk containing the top-most address in work,</code></span>
<span class="codeline" id="line-609"><code>	// starting at that address's page index in the chunk.</code></span>
<span class="codeline" id="line-610"><code>	//</code></span>
<span class="codeline" id="line-611"><code>	// Note that work.end() is exclusive, so get the chunk we care about</code></span>
<span class="codeline" id="line-612"><code>	// by subtracting 1.</code></span>
<span class="codeline" id="line-613"><code>	maxAddr := work.limit.addr() - 1</code></span>
<span class="codeline" id="line-614"><code>	maxChunk := chunkIndex(maxAddr)</code></span>
<span class="codeline" id="line-615"><code>	if p.summary[len(p.summary)-1][maxChunk].max() &gt;= uint(minPages) {</code></span>
<span class="codeline" id="line-616"><code>		// We only bother looking for a candidate if there at least</code></span>
<span class="codeline" id="line-617"><code>		// minPages free pages at all.</code></span>
<span class="codeline" id="line-618"><code>		base, npages := p.chunkOf(maxChunk).findScavengeCandidate(chunkPageIndex(maxAddr), minPages, maxPages)</code></span>
<span class="codeline" id="line-619"><code></code></span>
<span class="codeline" id="line-620"><code>		// If we found something, scavenge it and return!</code></span>
<span class="codeline" id="line-621"><code>		if npages != 0 {</code></span>
<span class="codeline" id="line-622"><code>			work.limit = offAddr{p.scavengeRangeLocked(maxChunk, base, npages)}</code></span>
<span class="codeline" id="line-623"><code></code></span>
<span class="codeline" id="line-624"><code>			assertLockHeld(p.mheapLock) // Must be locked on return.</code></span>
<span class="codeline" id="line-625"><code>			return uintptr(npages) * pageSize, work</code></span>
<span class="codeline" id="line-626"><code>		}</code></span>
<span class="codeline" id="line-627"><code>	}</code></span>
<span class="codeline" id="line-628"><code>	// Update the limit to reflect the fact that we checked maxChunk already.</code></span>
<span class="codeline" id="line-629"><code>	work.limit = offAddr{chunkBase(maxChunk)}</code></span>
<span class="codeline" id="line-630"><code></code></span>
<span class="codeline" id="line-631"><code>	// findCandidate finds the next scavenge candidate in work optimistically.</code></span>
<span class="codeline" id="line-632"><code>	//</code></span>
<span class="codeline" id="line-633"><code>	// Returns the candidate chunk index and true on success, and false on failure.</code></span>
<span class="codeline" id="line-634"><code>	//</code></span>
<span class="codeline" id="line-635"><code>	// The heap need not be locked.</code></span>
<span class="codeline" id="line-636"><code>	findCandidate := func(work addrRange) (chunkIdx, bool) {</code></span>
<span class="codeline" id="line-637"><code>		// Iterate over this work's chunks.</code></span>
<span class="codeline" id="line-638"><code>		for i := chunkIndex(work.limit.addr() - 1); i &gt;= chunkIndex(work.base.addr()); i-- {</code></span>
<span class="codeline" id="line-639"><code>			// If this chunk is totally in-use or has no unscavenged pages, don't bother</code></span>
<span class="codeline" id="line-640"><code>			// doing a more sophisticated check.</code></span>
<span class="codeline" id="line-641"><code>			//</code></span>
<span class="codeline" id="line-642"><code>			// Note we're accessing the summary and the chunks without a lock, but</code></span>
<span class="codeline" id="line-643"><code>			// that's fine. We're being optimistic anyway.</code></span>
<span class="codeline" id="line-644"><code></code></span>
<span class="codeline" id="line-645"><code>			// Check quickly if there are enough free pages at all.</code></span>
<span class="codeline" id="line-646"><code>			if p.summary[len(p.summary)-1][i].max() &lt; uint(minPages) {</code></span>
<span class="codeline" id="line-647"><code>				continue</code></span>
<span class="codeline" id="line-648"><code>			}</code></span>
<span class="codeline" id="line-649"><code></code></span>
<span class="codeline" id="line-650"><code>			// Run over the chunk looking harder for a candidate. Again, we could</code></span>
<span class="codeline" id="line-651"><code>			// race with a lot of different pieces of code, but we're just being</code></span>
<span class="codeline" id="line-652"><code>			// optimistic. Make sure we load the l2 pointer atomically though, to</code></span>
<span class="codeline" id="line-653"><code>			// avoid races with heap growth. It may or may not be possible to also</code></span>
<span class="codeline" id="line-654"><code>			// see a nil pointer in this case if we do race with heap growth, but</code></span>
<span class="codeline" id="line-655"><code>			// just defensively ignore the nils. This operation is optimistic anyway.</code></span>
<span class="codeline" id="line-656"><code>			l2 := (*[1 &lt;&lt; pallocChunksL2Bits]pallocData)(atomic.Loadp(unsafe.Pointer(&amp;p.chunks[i.l1()])))</code></span>
<span class="codeline" id="line-657"><code>			if l2 != nil &amp;&amp; l2[i.l2()].hasScavengeCandidate(minPages) {</code></span>
<span class="codeline" id="line-658"><code>				return i, true</code></span>
<span class="codeline" id="line-659"><code>			}</code></span>
<span class="codeline" id="line-660"><code>		}</code></span>
<span class="codeline" id="line-661"><code>		return 0, false</code></span>
<span class="codeline" id="line-662"><code>	}</code></span>
<span class="codeline" id="line-663"><code></code></span>
<span class="codeline" id="line-664"><code>	// Slow path: iterate optimistically over the in-use address space</code></span>
<span class="codeline" id="line-665"><code>	// looking for any free and unscavenged page. If we think we see something,</code></span>
<span class="codeline" id="line-666"><code>	// lock and verify it!</code></span>
<span class="codeline" id="line-667"><code>	for work.size() != 0 {</code></span>
<span class="codeline" id="line-668"><code>		unlockHeap()</code></span>
<span class="codeline" id="line-669"><code></code></span>
<span class="codeline" id="line-670"><code>		// Search for the candidate.</code></span>
<span class="codeline" id="line-671"><code>		candidateChunkIdx, ok := findCandidate(work)</code></span>
<span class="codeline" id="line-672"><code></code></span>
<span class="codeline" id="line-673"><code>		// Lock the heap. We need to do this now if we found a candidate or not.</code></span>
<span class="codeline" id="line-674"><code>		// If we did, we'll verify it. If not, we need to lock before returning</code></span>
<span class="codeline" id="line-675"><code>		// anyway.</code></span>
<span class="codeline" id="line-676"><code>		lockHeap()</code></span>
<span class="codeline" id="line-677"><code></code></span>
<span class="codeline" id="line-678"><code>		if !ok {</code></span>
<span class="codeline" id="line-679"><code>			// We didn't find a candidate, so we're done.</code></span>
<span class="codeline" id="line-680"><code>			work.limit = work.base</code></span>
<span class="codeline" id="line-681"><code>			break</code></span>
<span class="codeline" id="line-682"><code>		}</code></span>
<span class="codeline" id="line-683"><code></code></span>
<span class="codeline" id="line-684"><code>		// Find, verify, and scavenge if we can.</code></span>
<span class="codeline" id="line-685"><code>		chunk := p.chunkOf(candidateChunkIdx)</code></span>
<span class="codeline" id="line-686"><code>		base, npages := chunk.findScavengeCandidate(pallocChunkPages-1, minPages, maxPages)</code></span>
<span class="codeline" id="line-687"><code>		if npages &gt; 0 {</code></span>
<span class="codeline" id="line-688"><code>			work.limit = offAddr{p.scavengeRangeLocked(candidateChunkIdx, base, npages)}</code></span>
<span class="codeline" id="line-689"><code></code></span>
<span class="codeline" id="line-690"><code>			assertLockHeld(p.mheapLock) // Must be locked on return.</code></span>
<span class="codeline" id="line-691"><code>			return uintptr(npages) * pageSize, work</code></span>
<span class="codeline" id="line-692"><code>		}</code></span>
<span class="codeline" id="line-693"><code></code></span>
<span class="codeline" id="line-694"><code>		// We were fooled, so let's continue from where we left off.</code></span>
<span class="codeline" id="line-695"><code>		work.limit = offAddr{chunkBase(candidateChunkIdx)}</code></span>
<span class="codeline" id="line-696"><code>	}</code></span>
<span class="codeline" id="line-697"><code></code></span>
<span class="codeline" id="line-698"><code>	assertLockHeld(p.mheapLock) // Must be locked on return.</code></span>
<span class="codeline" id="line-699"><code>	return 0, work</code></span>
<span class="codeline" id="line-700"><code>}</code></span>
<span class="codeline" id="line-701"><code></code></span>
<span class="codeline" id="line-702"><code>// scavengeRangeLocked scavenges the given region of memory.</code></span>
<span class="codeline" id="line-703"><code>// The region of memory is described by its chunk index (ci),</code></span>
<span class="codeline" id="line-704"><code>// the starting page index of the region relative to that</code></span>
<span class="codeline" id="line-705"><code>// chunk (base), and the length of the region in pages (npages).</code></span>
<span class="codeline" id="line-706"><code>//</code></span>
<span class="codeline" id="line-707"><code>// Returns the base address of the scavenged region.</code></span>
<span class="codeline" id="line-708"><code>//</code></span>
<span class="codeline" id="line-709"><code>// p.mheapLock must be held.</code></span>
<span class="codeline" id="line-710"><code>func (p *pageAlloc) scavengeRangeLocked(ci chunkIdx, base, npages uint) uintptr {</code></span>
<span class="codeline" id="line-711"><code>	assertLockHeld(p.mheapLock)</code></span>
<span class="codeline" id="line-712"><code></code></span>
<span class="codeline" id="line-713"><code>	p.chunkOf(ci).scavenged.setRange(base, npages)</code></span>
<span class="codeline" id="line-714"><code></code></span>
<span class="codeline" id="line-715"><code>	// Compute the full address for the start of the range.</code></span>
<span class="codeline" id="line-716"><code>	addr := chunkBase(ci) + uintptr(base)*pageSize</code></span>
<span class="codeline" id="line-717"><code></code></span>
<span class="codeline" id="line-718"><code>	// Update the scavenge low watermark.</code></span>
<span class="codeline" id="line-719"><code>	if oAddr := (offAddr{addr}); oAddr.lessThan(p.scav.scavLWM) {</code></span>
<span class="codeline" id="line-720"><code>		p.scav.scavLWM = oAddr</code></span>
<span class="codeline" id="line-721"><code>	}</code></span>
<span class="codeline" id="line-722"><code></code></span>
<span class="codeline" id="line-723"><code>	// Only perform the actual scavenging if we're not in a test.</code></span>
<span class="codeline" id="line-724"><code>	// It's dangerous to do so otherwise.</code></span>
<span class="codeline" id="line-725"><code>	if p.test {</code></span>
<span class="codeline" id="line-726"><code>		return addr</code></span>
<span class="codeline" id="line-727"><code>	}</code></span>
<span class="codeline" id="line-728"><code>	sysUnused(unsafe.Pointer(addr), uintptr(npages)*pageSize)</code></span>
<span class="codeline" id="line-729"><code></code></span>
<span class="codeline" id="line-730"><code>	// Update global accounting only when not in test, otherwise</code></span>
<span class="codeline" id="line-731"><code>	// the runtime's accounting will be wrong.</code></span>
<span class="codeline" id="line-732"><code>	nbytes := int64(npages) * pageSize</code></span>
<span class="codeline" id="line-733"><code>	atomic.Xadd64(&amp;memstats.heap_released, nbytes)</code></span>
<span class="codeline" id="line-734"><code></code></span>
<span class="codeline" id="line-735"><code>	// Update consistent accounting too.</code></span>
<span class="codeline" id="line-736"><code>	stats := memstats.heapStats.acquire()</code></span>
<span class="codeline" id="line-737"><code>	atomic.Xaddint64(&amp;stats.committed, -nbytes)</code></span>
<span class="codeline" id="line-738"><code>	atomic.Xaddint64(&amp;stats.released, nbytes)</code></span>
<span class="codeline" id="line-739"><code>	memstats.heapStats.release()</code></span>
<span class="codeline" id="line-740"><code></code></span>
<span class="codeline" id="line-741"><code>	return addr</code></span>
<span class="codeline" id="line-742"><code>}</code></span>
<span class="codeline" id="line-743"><code></code></span>
<span class="codeline" id="line-744"><code>// fillAligned returns x but with all zeroes in m-aligned</code></span>
<span class="codeline" id="line-745"><code>// groups of m bits set to 1 if any bit in the group is non-zero.</code></span>
<span class="codeline" id="line-746"><code>//</code></span>
<span class="codeline" id="line-747"><code>// For example, fillAligned(0x0100a3, 8) == 0xff00ff.</code></span>
<span class="codeline" id="line-748"><code>//</code></span>
<span class="codeline" id="line-749"><code>// Note that if m == 1, this is a no-op.</code></span>
<span class="codeline" id="line-750"><code>//</code></span>
<span class="codeline" id="line-751"><code>// m must be a power of 2 &lt;= maxPagesPerPhysPage.</code></span>
<span class="codeline" id="line-752"><code>func fillAligned(x uint64, m uint) uint64 {</code></span>
<span class="codeline" id="line-753"><code>	apply := func(x uint64, c uint64) uint64 {</code></span>
<span class="codeline" id="line-754"><code>		// The technique used it here is derived from</code></span>
<span class="codeline" id="line-755"><code>		// https://graphics.stanford.edu/~seander/bithacks.html#ZeroInWord</code></span>
<span class="codeline" id="line-756"><code>		// and extended for more than just bytes (like nibbles</code></span>
<span class="codeline" id="line-757"><code>		// and uint16s) by using an appropriate constant.</code></span>
<span class="codeline" id="line-758"><code>		//</code></span>
<span class="codeline" id="line-759"><code>		// To summarize the technique, quoting from that page:</code></span>
<span class="codeline" id="line-760"><code>		// "[It] works by first zeroing the high bits of the [8]</code></span>
<span class="codeline" id="line-761"><code>		// bytes in the word. Subsequently, it adds a number that</code></span>
<span class="codeline" id="line-762"><code>		// will result in an overflow to the high bit of a byte if</code></span>
<span class="codeline" id="line-763"><code>		// any of the low bits were initially set. Next the high</code></span>
<span class="codeline" id="line-764"><code>		// bits of the original word are ORed with these values;</code></span>
<span class="codeline" id="line-765"><code>		// thus, the high bit of a byte is set iff any bit in the</code></span>
<span class="codeline" id="line-766"><code>		// byte was set. Finally, we determine if any of these high</code></span>
<span class="codeline" id="line-767"><code>		// bits are zero by ORing with ones everywhere except the</code></span>
<span class="codeline" id="line-768"><code>		// high bits and inverting the result."</code></span>
<span class="codeline" id="line-769"><code>		return ^((((x &amp; c) + c) | x) | c)</code></span>
<span class="codeline" id="line-770"><code>	}</code></span>
<span class="codeline" id="line-771"><code>	// Transform x to contain a 1 bit at the top of each m-aligned</code></span>
<span class="codeline" id="line-772"><code>	// group of m zero bits.</code></span>
<span class="codeline" id="line-773"><code>	switch m {</code></span>
<span class="codeline" id="line-774"><code>	case 1:</code></span>
<span class="codeline" id="line-775"><code>		return x</code></span>
<span class="codeline" id="line-776"><code>	case 2:</code></span>
<span class="codeline" id="line-777"><code>		x = apply(x, 0x5555555555555555)</code></span>
<span class="codeline" id="line-778"><code>	case 4:</code></span>
<span class="codeline" id="line-779"><code>		x = apply(x, 0x7777777777777777)</code></span>
<span class="codeline" id="line-780"><code>	case 8:</code></span>
<span class="codeline" id="line-781"><code>		x = apply(x, 0x7f7f7f7f7f7f7f7f)</code></span>
<span class="codeline" id="line-782"><code>	case 16:</code></span>
<span class="codeline" id="line-783"><code>		x = apply(x, 0x7fff7fff7fff7fff)</code></span>
<span class="codeline" id="line-784"><code>	case 32:</code></span>
<span class="codeline" id="line-785"><code>		x = apply(x, 0x7fffffff7fffffff)</code></span>
<span class="codeline" id="line-786"><code>	case 64: // == maxPagesPerPhysPage</code></span>
<span class="codeline" id="line-787"><code>		x = apply(x, 0x7fffffffffffffff)</code></span>
<span class="codeline" id="line-788"><code>	default:</code></span>
<span class="codeline" id="line-789"><code>		throw("bad m value")</code></span>
<span class="codeline" id="line-790"><code>	}</code></span>
<span class="codeline" id="line-791"><code>	// Now, the top bit of each m-aligned group in x is set</code></span>
<span class="codeline" id="line-792"><code>	// that group was all zero in the original x.</code></span>
<span class="codeline" id="line-793"><code></code></span>
<span class="codeline" id="line-794"><code>	// From each group of m bits subtract 1.</code></span>
<span class="codeline" id="line-795"><code>	// Because we know only the top bits of each</code></span>
<span class="codeline" id="line-796"><code>	// m-aligned group are set, we know this will</code></span>
<span class="codeline" id="line-797"><code>	// set each group to have all the bits set except</code></span>
<span class="codeline" id="line-798"><code>	// the top bit, so just OR with the original</code></span>
<span class="codeline" id="line-799"><code>	// result to set all the bits.</code></span>
<span class="codeline" id="line-800"><code>	return ^((x - (x &gt;&gt; (m - 1))) | x)</code></span>
<span class="codeline" id="line-801"><code>}</code></span>
<span class="codeline" id="line-802"><code></code></span>
<span class="codeline" id="line-803"><code>// hasScavengeCandidate returns true if there's any min-page-aligned groups of</code></span>
<span class="codeline" id="line-804"><code>// min pages of free-and-unscavenged memory in the region represented by this</code></span>
<span class="codeline" id="line-805"><code>// pallocData.</code></span>
<span class="codeline" id="line-806"><code>//</code></span>
<span class="codeline" id="line-807"><code>// min must be a non-zero power of 2 &lt;= maxPagesPerPhysPage.</code></span>
<span class="codeline" id="line-808"><code>func (m *pallocData) hasScavengeCandidate(min uintptr) bool {</code></span>
<span class="codeline" id="line-809"><code>	if min&amp;(min-1) != 0 || min == 0 {</code></span>
<span class="codeline" id="line-810"><code>		print("runtime: min = ", min, "\n")</code></span>
<span class="codeline" id="line-811"><code>		throw("min must be a non-zero power of 2")</code></span>
<span class="codeline" id="line-812"><code>	} else if min &gt; maxPagesPerPhysPage {</code></span>
<span class="codeline" id="line-813"><code>		print("runtime: min = ", min, "\n")</code></span>
<span class="codeline" id="line-814"><code>		throw("min too large")</code></span>
<span class="codeline" id="line-815"><code>	}</code></span>
<span class="codeline" id="line-816"><code></code></span>
<span class="codeline" id="line-817"><code>	// The goal of this search is to see if the chunk contains any free and unscavenged memory.</code></span>
<span class="codeline" id="line-818"><code>	for i := len(m.scavenged) - 1; i &gt;= 0; i-- {</code></span>
<span class="codeline" id="line-819"><code>		// 1s are scavenged OR non-free =&gt; 0s are unscavenged AND free</code></span>
<span class="codeline" id="line-820"><code>		//</code></span>
<span class="codeline" id="line-821"><code>		// TODO(mknyszek): Consider splitting up fillAligned into two</code></span>
<span class="codeline" id="line-822"><code>		// functions, since here we technically could get by with just</code></span>
<span class="codeline" id="line-823"><code>		// the first half of its computation. It'll save a few instructions</code></span>
<span class="codeline" id="line-824"><code>		// but adds some additional code complexity.</code></span>
<span class="codeline" id="line-825"><code>		x := fillAligned(m.scavenged[i]|m.pallocBits[i], uint(min))</code></span>
<span class="codeline" id="line-826"><code></code></span>
<span class="codeline" id="line-827"><code>		// Quickly skip over chunks of non-free or scavenged pages.</code></span>
<span class="codeline" id="line-828"><code>		if x != ^uint64(0) {</code></span>
<span class="codeline" id="line-829"><code>			return true</code></span>
<span class="codeline" id="line-830"><code>		}</code></span>
<span class="codeline" id="line-831"><code>	}</code></span>
<span class="codeline" id="line-832"><code>	return false</code></span>
<span class="codeline" id="line-833"><code>}</code></span>
<span class="codeline" id="line-834"><code></code></span>
<span class="codeline" id="line-835"><code>// findScavengeCandidate returns a start index and a size for this pallocData</code></span>
<span class="codeline" id="line-836"><code>// segment which represents a contiguous region of free and unscavenged memory.</code></span>
<span class="codeline" id="line-837"><code>//</code></span>
<span class="codeline" id="line-838"><code>// searchIdx indicates the page index within this chunk to start the search, but</code></span>
<span class="codeline" id="line-839"><code>// note that findScavengeCandidate searches backwards through the pallocData. As a</code></span>
<span class="codeline" id="line-840"><code>// a result, it will return the highest scavenge candidate in address order.</code></span>
<span class="codeline" id="line-841"><code>//</code></span>
<span class="codeline" id="line-842"><code>// min indicates a hard minimum size and alignment for runs of pages. That is,</code></span>
<span class="codeline" id="line-843"><code>// findScavengeCandidate will not return a region smaller than min pages in size,</code></span>
<span class="codeline" id="line-844"><code>// or that is min pages or greater in size but not aligned to min. min must be</code></span>
<span class="codeline" id="line-845"><code>// a non-zero power of 2 &lt;= maxPagesPerPhysPage.</code></span>
<span class="codeline" id="line-846"><code>//</code></span>
<span class="codeline" id="line-847"><code>// max is a hint for how big of a region is desired. If max &gt;= pallocChunkPages, then</code></span>
<span class="codeline" id="line-848"><code>// findScavengeCandidate effectively returns entire free and unscavenged regions.</code></span>
<span class="codeline" id="line-849"><code>// If max &lt; pallocChunkPages, it may truncate the returned region such that size is</code></span>
<span class="codeline" id="line-850"><code>// max. However, findScavengeCandidate may still return a larger region if, for</code></span>
<span class="codeline" id="line-851"><code>// example, it chooses to preserve huge pages, or if max is not aligned to min (it</code></span>
<span class="codeline" id="line-852"><code>// will round up). That is, even if max is small, the returned size is not guaranteed</code></span>
<span class="codeline" id="line-853"><code>// to be equal to max. max is allowed to be less than min, in which case it is as if</code></span>
<span class="codeline" id="line-854"><code>// max == min.</code></span>
<span class="codeline" id="line-855"><code>func (m *pallocData) findScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint) {</code></span>
<span class="codeline" id="line-856"><code>	if min&amp;(min-1) != 0 || min == 0 {</code></span>
<span class="codeline" id="line-857"><code>		print("runtime: min = ", min, "\n")</code></span>
<span class="codeline" id="line-858"><code>		throw("min must be a non-zero power of 2")</code></span>
<span class="codeline" id="line-859"><code>	} else if min &gt; maxPagesPerPhysPage {</code></span>
<span class="codeline" id="line-860"><code>		print("runtime: min = ", min, "\n")</code></span>
<span class="codeline" id="line-861"><code>		throw("min too large")</code></span>
<span class="codeline" id="line-862"><code>	}</code></span>
<span class="codeline" id="line-863"><code>	// max may not be min-aligned, so we might accidentally truncate to</code></span>
<span class="codeline" id="line-864"><code>	// a max value which causes us to return a non-min-aligned value.</code></span>
<span class="codeline" id="line-865"><code>	// To prevent this, align max up to a multiple of min (which is always</code></span>
<span class="codeline" id="line-866"><code>	// a power of 2). This also prevents max from ever being less than</code></span>
<span class="codeline" id="line-867"><code>	// min, unless it's zero, so handle that explicitly.</code></span>
<span class="codeline" id="line-868"><code>	if max == 0 {</code></span>
<span class="codeline" id="line-869"><code>		max = min</code></span>
<span class="codeline" id="line-870"><code>	} else {</code></span>
<span class="codeline" id="line-871"><code>		max = alignUp(max, min)</code></span>
<span class="codeline" id="line-872"><code>	}</code></span>
<span class="codeline" id="line-873"><code></code></span>
<span class="codeline" id="line-874"><code>	i := int(searchIdx / 64)</code></span>
<span class="codeline" id="line-875"><code>	// Start by quickly skipping over blocks of non-free or scavenged pages.</code></span>
<span class="codeline" id="line-876"><code>	for ; i &gt;= 0; i-- {</code></span>
<span class="codeline" id="line-877"><code>		// 1s are scavenged OR non-free =&gt; 0s are unscavenged AND free</code></span>
<span class="codeline" id="line-878"><code>		x := fillAligned(m.scavenged[i]|m.pallocBits[i], uint(min))</code></span>
<span class="codeline" id="line-879"><code>		if x != ^uint64(0) {</code></span>
<span class="codeline" id="line-880"><code>			break</code></span>
<span class="codeline" id="line-881"><code>		}</code></span>
<span class="codeline" id="line-882"><code>	}</code></span>
<span class="codeline" id="line-883"><code>	if i &lt; 0 {</code></span>
<span class="codeline" id="line-884"><code>		// Failed to find any free/unscavenged pages.</code></span>
<span class="codeline" id="line-885"><code>		return 0, 0</code></span>
<span class="codeline" id="line-886"><code>	}</code></span>
<span class="codeline" id="line-887"><code>	// We have something in the 64-bit chunk at i, but it could</code></span>
<span class="codeline" id="line-888"><code>	// extend further. Loop until we find the extent of it.</code></span>
<span class="codeline" id="line-889"><code></code></span>
<span class="codeline" id="line-890"><code>	// 1s are scavenged OR non-free =&gt; 0s are unscavenged AND free</code></span>
<span class="codeline" id="line-891"><code>	x := fillAligned(m.scavenged[i]|m.pallocBits[i], uint(min))</code></span>
<span class="codeline" id="line-892"><code>	z1 := uint(sys.LeadingZeros64(^x))</code></span>
<span class="codeline" id="line-893"><code>	run, end := uint(0), uint(i)*64+(64-z1)</code></span>
<span class="codeline" id="line-894"><code>	if x&lt;&lt;z1 != 0 {</code></span>
<span class="codeline" id="line-895"><code>		// After shifting out z1 bits, we still have 1s,</code></span>
<span class="codeline" id="line-896"><code>		// so the run ends inside this word.</code></span>
<span class="codeline" id="line-897"><code>		run = uint(sys.LeadingZeros64(x &lt;&lt; z1))</code></span>
<span class="codeline" id="line-898"><code>	} else {</code></span>
<span class="codeline" id="line-899"><code>		// After shifting out z1 bits, we have no more 1s.</code></span>
<span class="codeline" id="line-900"><code>		// This means the run extends to the bottom of the</code></span>
<span class="codeline" id="line-901"><code>		// word so it may extend into further words.</code></span>
<span class="codeline" id="line-902"><code>		run = 64 - z1</code></span>
<span class="codeline" id="line-903"><code>		for j := i - 1; j &gt;= 0; j-- {</code></span>
<span class="codeline" id="line-904"><code>			x := fillAligned(m.scavenged[j]|m.pallocBits[j], uint(min))</code></span>
<span class="codeline" id="line-905"><code>			run += uint(sys.LeadingZeros64(x))</code></span>
<span class="codeline" id="line-906"><code>			if x != 0 {</code></span>
<span class="codeline" id="line-907"><code>				// The run stopped in this word.</code></span>
<span class="codeline" id="line-908"><code>				break</code></span>
<span class="codeline" id="line-909"><code>			}</code></span>
<span class="codeline" id="line-910"><code>		}</code></span>
<span class="codeline" id="line-911"><code>	}</code></span>
<span class="codeline" id="line-912"><code></code></span>
<span class="codeline" id="line-913"><code>	// Split the run we found if it's larger than max but hold on to</code></span>
<span class="codeline" id="line-914"><code>	// our original length, since we may need it later.</code></span>
<span class="codeline" id="line-915"><code>	size := run</code></span>
<span class="codeline" id="line-916"><code>	if size &gt; uint(max) {</code></span>
<span class="codeline" id="line-917"><code>		size = uint(max)</code></span>
<span class="codeline" id="line-918"><code>	}</code></span>
<span class="codeline" id="line-919"><code>	start := end - size</code></span>
<span class="codeline" id="line-920"><code></code></span>
<span class="codeline" id="line-921"><code>	// Each huge page is guaranteed to fit in a single palloc chunk.</code></span>
<span class="codeline" id="line-922"><code>	//</code></span>
<span class="codeline" id="line-923"><code>	// TODO(mknyszek): Support larger huge page sizes.</code></span>
<span class="codeline" id="line-924"><code>	// TODO(mknyszek): Consider taking pages-per-huge-page as a parameter</code></span>
<span class="codeline" id="line-925"><code>	// so we can write tests for this.</code></span>
<span class="codeline" id="line-926"><code>	if physHugePageSize &gt; pageSize &amp;&amp; physHugePageSize &gt; physPageSize {</code></span>
<span class="codeline" id="line-927"><code>		// We have huge pages, so let's ensure we don't break one by scavenging</code></span>
<span class="codeline" id="line-928"><code>		// over a huge page boundary. If the range [start, start+size) overlaps with</code></span>
<span class="codeline" id="line-929"><code>		// a free-and-unscavenged huge page, we want to grow the region we scavenge</code></span>
<span class="codeline" id="line-930"><code>		// to include that huge page.</code></span>
<span class="codeline" id="line-931"><code></code></span>
<span class="codeline" id="line-932"><code>		// Compute the huge page boundary above our candidate.</code></span>
<span class="codeline" id="line-933"><code>		pagesPerHugePage := uintptr(physHugePageSize / pageSize)</code></span>
<span class="codeline" id="line-934"><code>		hugePageAbove := uint(alignUp(uintptr(start), pagesPerHugePage))</code></span>
<span class="codeline" id="line-935"><code></code></span>
<span class="codeline" id="line-936"><code>		// If that boundary is within our current candidate, then we may be breaking</code></span>
<span class="codeline" id="line-937"><code>		// a huge page.</code></span>
<span class="codeline" id="line-938"><code>		if hugePageAbove &lt;= end {</code></span>
<span class="codeline" id="line-939"><code>			// Compute the huge page boundary below our candidate.</code></span>
<span class="codeline" id="line-940"><code>			hugePageBelow := uint(alignDown(uintptr(start), pagesPerHugePage))</code></span>
<span class="codeline" id="line-941"><code></code></span>
<span class="codeline" id="line-942"><code>			if hugePageBelow &gt;= end-run {</code></span>
<span class="codeline" id="line-943"><code>				// We're in danger of breaking apart a huge page since start+size crosses</code></span>
<span class="codeline" id="line-944"><code>				// a huge page boundary and rounding down start to the nearest huge</code></span>
<span class="codeline" id="line-945"><code>				// page boundary is included in the full run we found. Include the entire</code></span>
<span class="codeline" id="line-946"><code>				// huge page in the bound by rounding down to the huge page size.</code></span>
<span class="codeline" id="line-947"><code>				size = size + (start - hugePageBelow)</code></span>
<span class="codeline" id="line-948"><code>				start = hugePageBelow</code></span>
<span class="codeline" id="line-949"><code>			}</code></span>
<span class="codeline" id="line-950"><code>		}</code></span>
<span class="codeline" id="line-951"><code>	}</code></span>
<span class="codeline" id="line-952"><code>	return start, size</code></span>
<span class="codeline" id="line-953"><code>}</code></span>
</pre><pre id="footer">
<table><tr><td><img src="../../png/go101-twitter.png"></td>
<td>The pages are generated with <a href="https://go101.org/article/tool-golds.html"><b>Golds</b></a> <i>v0.3.2</i>. (GOOS=linux GOARCH=amd64)
<b>Golds</b> is a <a href="https://go101.org">Go 101</a> project developed by <a href="https://tapirgames.com">Tapir Liu</a>.
PR and bug reports are welcome and can be submitted to <a href="https://github.com/go101/golds">the issue list</a>.
Please follow <a href="https://twitter.com/go100and1">@Go100and1</a> (reachable from the left QR code) to get the latest news of <b>Golds</b>.</td></tr></table></pre>